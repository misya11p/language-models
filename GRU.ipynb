{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n",
    "\n",
    "*Gated Recurent Unit*\n",
    "\n",
    "ゲート付き回帰型ユニット\n",
    "\n",
    "RNN層にゲートと呼ばれる機構を追加して長期的な文脈が保持できるようになったもの。  \n",
    "RNN同様、「GRU」が層を表している場合とそれが組み込まれたNNを表している場合がある。前章ではそれらの表記を分けたが、本章では分けないので、察して読んで。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext import transforms\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchvision.transforms import Compose\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ゲート\n",
    "\n",
    "あるデータをどれくらい通すかを示したもの。具体的には対称のデータと同じサイズのベクトル。0-1の値をとる。  \n",
    "NNで実装してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力されたデータを線形変化し、sigmoid関数に入力するだけ。このモデルにあるデータを入力したときの出力値が、そのデータのゲートとなる。  \n",
    "ゲートを元のデータに掛けることで、元のデータの一部を**通した**ということになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([-0.7854,  0.4304,  0.9104])\n",
      "gate: tensor([0.5302, 0.4206, 0.6209], grad_fn=<SigmoidBackward0>)\n",
      "output: tensor([-0.4165,  0.1810,  0.5653], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_size = 3\n",
    "gate = Gate(input_size)\n",
    "\n",
    "x = torch.randn(input_size)\n",
    "y = x * gate(x)\n",
    "print('input:', x)\n",
    "print('gate:', gate(x))\n",
    "print('output:', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## GRUの構造\n",
    "\n",
    "GRUの構造を見ていこう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦RNNの復習をしよう。\n",
    "\n",
    "\n",
    "RNNはある時間$t$の入力$x_t$に対して以下のような演算で出力値$h_t$を決定する。\n",
    "\n",
    "$$\n",
    "h_t = \\mathrm{tanh}(W_x x_t + b_x + W_h h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "この$x_t$と$h_{t-1}$の全結合の部分は$\\mathrm{fc}(x,h)$で表すことにしよう。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_t &= \\mathrm{tanh}(\\mathrm{fc}(x_t,h_{t-1})) \\\\\n",
    "\\mathrm{fc}(x,h) &= W_x x + b_x + W_h h + b_h\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "んで、$\\mathrm{fc}(x,h)$の実装もしておこう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc_input = nn.Linear(input_size, hidden_size)\n",
    "        self.fc_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        return self.fc_input(x) + self.fc_hidden(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、GRUの構造を見ていこう。  \n",
    "GRUは以下のような演算で出力値$h_t$を決定する。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_t &= (1 - z_t) \\odot \\tilde{h}_t + z_t \\odot h_{t-1} \\\\\n",
    "\\tilde{h}_t &= \\mathrm{tanh}(\\mathrm{fc}_{\\tilde h}(x_t,h_{t-1})) \\\\\n",
    "z_t &= \\sigma(\\mathrm{fc}_{z}(x_t,h_{t-1})) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\sigma(x)$はsigmoid関数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNでは新たなデータ$\\tilde h_t$がそのまま出力されていた。  \n",
    "GRUでは、新たなデータ$\\tilde h_t$を古いデータ$h_{t-1}$に足して出力する。そして、その際の比率をゲート$z_t$で決める。この$z_t$は$h_{t-1}$をどれだけ通すかを表す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc = FullyConnected(input_size, hidden_size)\n",
    "        self.gate = nn.Sequential(\n",
    "            FullyConnected(input_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        h_new = F.tanh(self.fc(x, h))\n",
    "        z = self.gate(x)\n",
    "        h = (1 - z) * h_new + z * h\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、GRUではゲートを用いて新たなデータをどれだけ取り入れるべきか、そして古いデータをどれだけ捨てるか考えることが出来る。  \n",
    "この枠組みの下で学習を行うことで、長期的に保持すべきデータをしっかりと保持できるようになることが期待される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに、上記のモデルは一般的なGRUを私が簡略化したもの。  \n",
    "一般的なGRUは、上記のモデルにゲートを一つ追加した以下のモデルである。\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_t &= (1 - z_t) \\odot \\tilde{h}_t + z_t \\odot h_{t-1} \\\\\n",
    "\\tilde{h}_t &= \\mathrm{tanh}(\\mathrm{fc}_{\\tilde h}(x_t,r_t \\odot h_{t-1})) \\\\\n",
    "z_t &= \\sigma(\\mathrm{fc}_{z}(x_t,h_{t-1})) \\\\\n",
    "r_t &= \\sigma(\\mathrm{fc}_{r}(x_t,h_{t-1})) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "新なデータ$\\tilde h_t$を生成する際に、古いデータ$h_{t-1}$をどれだけ考慮するかを決めるゲート$r_t$が追加されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc_input = FullyConnected(input_size, hidden_size)\n",
    "        self.gate_update = nn.Sequential(\n",
    "            FullyConnected(input_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.gate_reset = nn.Sequential(\n",
    "            FullyConnected(input_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        r = self.gate_reset(x, h)\n",
    "        h_new = F.tanh(self.fc_input(x, r * h))\n",
    "        z = self.gate_update(x)\n",
    "        h = (1 - z) * h_new + z * h\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、RNN同様、PyTorchにクラスとして`torch.nn.GRU`が用意されている:  \n",
    "[GRU — PyTorch 2.0 documentation](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(input_size, input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## LSTM\n",
    "\n",
    "*Long Short-Term Memory*\n",
    "\n",
    "長短期記憶\n",
    "\n",
    "GRUの進化版。考え方はGRUと同じで、RNNにゲートを取り入れてイイ感じにしたもの。  \n",
    "ちなみに、GRUよりLSTMの方が先に提案されている。GRUはLSTMの簡易版として後から提案された。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMには出力する隠れ状態$h_t$だけでなく、**記憶セル**と呼ばれる変数$c_t$を持つ。記憶セルはLSTMの外に出力されることはなく、LSTM内部でのみ使用される。\n",
    "\n",
    "まず簡単に文字で説明する。  \n",
    "記憶セル$c_t$がGRUでの隠れ状態$h_t$に当たり、ゲートを用いた不要な情報の削除と新たな情報の追加が行われる。なおゲートの生成には入力$x_t$と前の隠れ状態$h_{t-1}$を用いる（記憶セルは用いない）。そしてこの記憶セルを活性化関数に通したものをLSTMの出力=隠れ状態$h_t$とする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体的な構造を見てみよう。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_t &= o_t \\odot \\mathrm{tanh}(c_t) \\\\\n",
    "c_t &= f_t \\odot c_{t-1} + i_t \\odot \\tilde c_t \\\\\n",
    "\\tilde c_t &= \\mathrm{tanh}(\\mathrm{fc}_{\\tilde c}(x_t,h_{t-1})) \\\\\n",
    "i_t &= \\sigma(\\mathrm{fc}_{i}(x_t,h_{t-1})) \\\\\n",
    "f_t &= \\sigma(\\mathrm{fc}_{f}(x_t,h_{t-1})) \\\\\n",
    "o_t &= \\sigma(\\mathrm{fc}_{o}(x_t,h_{t-1})) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "- $\\tilde c_t$: 新たな情報。\n",
    "- $i_t$: inputゲート。新たな情報$\\tilde c_t$をどれだけ取り入れるかを決める。\n",
    "- $f_t$: forgetゲート。古い情報$c_{h-1}$をどれだけ保持するかを決めるゲート。\n",
    "- $o_t$: outputゲート。出力する隠れ状態の量を決めるゲート。\n",
    "\n",
    "GRUでは1つのゲートを用いて新たな情報と古い情報の比率を決めていたが、LSTMでは別々のゲートを用いて決める。\n",
    "\n",
    "実装は以下の通り。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.gate_input = nn.Sequential(\n",
    "            FullyConnected(input_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.gate_forget = nn.Sequential(\n",
    "            FullyConnected(input_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.gate_output = nn.Sequential(\n",
    "            FullyConnected(input_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.fc = FullyConnected(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        c_new = F.tanh(self.fc(x, h))\n",
    "        i = self.gate_input(x, h)\n",
    "        f = self.gate_forget(x, h)\n",
    "        o = self.gate_output(x, h)\n",
    "        c = f * c + i * c_new\n",
    "        h = o * F.tanh(c)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchにも`torch.nn.LSTM`が用意されている:  \n",
    "[LSTM — PyTorch 2.0 documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## LSTMを用いた言語モデル\n",
    "\n",
    "せっかくなので、LSTMで言語モデルを作ってみよう。RNNLMのRNN層をLSTMに変更するだけ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データ。前章と同じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 654])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textfile = 'data/jawiki_1000.txt'\n",
    "with open(textfile) as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "tokenizer_model_prefix = 'models/jawiki_tokenizer'\n",
    "sp = spm.SentencePieceProcessor(f'{tokenizer_model_prefix}.model')\n",
    "data_ids = sp.encode(data)\n",
    "n_vocab = len(sp)\n",
    "pad_id = sp.pad_id()\n",
    "\n",
    "bos_id = sp.bos_id()\n",
    "eos_id = sp.eos_id()\n",
    "for ids in data_ids:\n",
    "    if ids:\n",
    "        ids[0] = bos_id\n",
    "        ids[-1] = eos_id\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids):\n",
    "        self._n_samples = len(data_ids)\n",
    "        self.data = [torch.tensor(ids) for ids in data_ids]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_text = self.data[idx][:-1]\n",
    "        out_text = self.data[idx][1:]\n",
    "        return in_text, out_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._n_samples\n",
    "\n",
    "def collate_fn(batch):\n",
    "    in_text, out_text = zip(*batch)\n",
    "    in_text = pad_sequence(in_text, batch_first=True, padding_value=pad_id)\n",
    "    out_text = pad_sequence(out_text, batch_first=True, padding_value=pad_id)\n",
    "    return in_text, out_text\n",
    "\n",
    "batch_size = 32\n",
    "dataset = TextDataset(data_ids)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn # 取得したミニバッチに対して行う処理の指定\n",
    ")\n",
    "sample = next(iter(dataloader))\n",
    "sample[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, hc=None):\n",
    "        x = self.embedding(x) # (seq_len, embed_size)\n",
    "        y, (h, c) = self.lstm(x, hc) # (seq_len, hidden_size)\n",
    "        y = self.fc(y) # (seq_len, n_vocab)\n",
    "        return y, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "def train(model, optimizer, trunc_len, n_epochs, prog_unit=1):\n",
    "    model.train()\n",
    "    prog.start(n_iter=len(dataloader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        for x, t in dataloader:\n",
    "            hc = None\n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            for i in range(0, len(x), trunc_len):\n",
    "                x_batch = x[:, i:i+trunc_len]\n",
    "                t_batch = t[:, i:i+trunc_len]\n",
    "                optimizer.zero_grad()\n",
    "                y, (h, c) = model(x_batch, hc)\n",
    "                loss = criterion(y.reshape(-1, n_vocab), t_batch.ravel())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                prog.update(loss.item(), advance=0)\n",
    "                h = h.detach()\n",
    "                c = c.detach()\n",
    "                hc = (h, c)\n",
    "            prog.update(advance=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 1024\n",
    "model = LanguageModel(n_vocab, hidden_size, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1-100/1000: ######################################## 100% [00:01:38.34] loss: 3.34043 \n",
      "  101-200/1000: ######################################## 100% [00:01:37.56] loss: 0.23210 \n",
      "  201-300/1000: ######################################## 100% [00:01:37.89] loss: 0.13922 \n",
      "  301-400/1000: ######################################## 100% [00:01:37.80] loss: 0.12624 \n",
      "  401-500/1000: ######################################## 100% [00:01:37.63] loss: 0.12555 \n",
      "  501-600/1000: ######################################## 100% [00:01:37.72] loss: 0.12289 \n",
      "  601-700/1000: ######################################## 100% [00:01:37.93] loss: 0.12313 \n",
      "  701-800/1000: ######################################## 100% [00:01:37.48] loss: 0.12191 \n",
      "  801-900/1000: ######################################## 100% [00:01:37.53] loss: 0.12260 \n",
      " 901-1000/1000: ######################################## 100% [00:01:37.77] loss: 0.12138 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, 64, 1000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_sampling(y):\n",
    "    y = y[-1]\n",
    "    probs = F.softmax(y, dim=-1)\n",
    "    token = random.choices(range(n_vocab), weights=probs)[0]\n",
    "    return token\n",
    "\n",
    "\n",
    "bos_id = sp.bos_id()\n",
    "eos_id = sp.eos_id()\n",
    "@torch.no_grad()\n",
    "def generate_sentence(\n",
    "    model: nn.Module,\n",
    "    start: str = '',\n",
    "    max_len: int = 50\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    start_ids = sp.encode(start) if start else [bos_id]\n",
    "    start_ids = torch.tensor(start_ids, device=device)\n",
    "\n",
    "    y, (h, c) = model(start_ids)\n",
    "    next_token = token_sampling(y)\n",
    "    ids = [next_token]\n",
    "    for _ in range(max_len):\n",
    "        x = torch.tensor([next_token], device=device)\n",
    "        y, (h, c) = model(x, (h, c))\n",
    "        next_token = token_sampling(y)\n",
    "        ids.append(next_token)\n",
    "        if next_token == eos_id:\n",
    "            break\n",
    "    sentence = start + sp.decode(ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "市内(島内)に鉄道は通っていない\n",
      "旧施設の長岡市厚生会館は2009年(平成21年)1月末を以って用途廃止となり、同年7月にかけて解体・撤去・整地が行われ、この間にシティホールの設計作業が進められた。そして同年12\n",
      "キューバ出身のラテン・アコースティック・グループ「ティピコ・オリエンタル」(Tipico Oriental)による本格ライブ。ロストリバーデルタの「ユカタン・ベース\n",
      "CD国際カラーデザイン協会が実施しているカラーの検定試験。世界標準のカラーシステムPantoneを使って学べるような内容になっており、初心者からプロダクトデザイン、インテリア、ファッション、WEBなど様々な仕事で\n",
      "濃度勾配を維持する要因はNa+ - K+ポンプであるが、静止電位の値を支配している大きな要因は、K+漏洩チャネルである。Na+とK+は、開いたイオンチャネルを通して電気化学的勾配の影響の\n",
      "アルビノーニは50曲ほどのオペラを作曲し、そのうち20曲が1723年から1740年にかけて上演されたが、こんにちでは器楽曲、とりわけオーボエ協奏曲が最も有名である。アルビノーニの器楽曲は、ヨハン・ゼ\n",
      "もともと大阪府道5号線は1984年(昭和59年)3月 - 1993年(平成5年)3月までは川西園部線(兵庫県道39号、京都府道42号)だったが、国\n",
      "大宮駅の東口周辺の地域に位置する。西の境界は氷川参道となっている。大字大宮字浅間が町名の由来\n",
      "ディオクレティアヌス時代に整備された中央政府の組織はコンスタンティヌス1世治下で更に発展・整備された。宮廷には皇帝の飲食・衣装・ベッドメイクなど家政部門を担う寝室(Cubiculum\n",
      "紀伊続風土記は、江戸幕府の命を受けた紀州藩が、文化3年(1806年)8月、藩士で儒学者の仁井田好古を総裁とし、仁井田長群・本居内遠・加納\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(generate_sentence(model, max_len=50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
