{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNLM\n",
    "\n",
    "*Recurrent Neural Network Language Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext import transforms\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchvision.transforms import Compose\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Dataset\n",
    "\n",
    "[日英中基本文データ - LANGUAGE MEDIA PROCESSING LAB](https://nlp.ist.i.kyoto-u.ac.jp/?%E6%97%A5%E8%8B%B1%E4%B8%AD%E5%9F%BA%E6%9C%AC%E6%96%87%E3%83%87%E3%83%BC%E3%82%BF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 5304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>japanese</th>\n",
       "      <th>english</th>\n",
       "      <th>chinese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#0001</td>\n",
       "      <td>Xではないかとつくづく疑問に思う</td>\n",
       "      <td>I often wonder if it might be X.</td>\n",
       "      <td>难道不会是X吗，我实在是感到怀疑。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#0002</td>\n",
       "      <td>Xがいいなといつも思います</td>\n",
       "      <td>I always think X would be nice.</td>\n",
       "      <td>我总觉得X不错。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#0003</td>\n",
       "      <td>それがあるようにいつも思います</td>\n",
       "      <td>It always seems like it is there.</td>\n",
       "      <td>我总觉得那好像是有的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#0004</td>\n",
       "      <td>それが多すぎないかと正直思う</td>\n",
       "      <td>I honestly feel like there is too much.</td>\n",
       "      <td>老实说我觉得那太多了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#0005</td>\n",
       "      <td>山田はみんなに好かれるタイプの人だと思う</td>\n",
       "      <td>I think that Yamada is the type everybody likes.</td>\n",
       "      <td>我想山田是受大家欢迎的那种人。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id              japanese   \n",
       "0  #0001      Xではないかとつくづく疑問に思う  \\\n",
       "1  #0002         Xがいいなといつも思います   \n",
       "2  #0003       それがあるようにいつも思います   \n",
       "3  #0004        それが多すぎないかと正直思う   \n",
       "4  #0005  山田はみんなに好かれるタイプの人だと思う   \n",
       "\n",
       "                                            english            chinese  \n",
       "0                  I often wonder if it might be X.  难道不会是X吗，我实在是感到怀疑。  \n",
       "1                   I always think X would be nice.           我总觉得X不错。  \n",
       "2                 It always seems like it is there.        我总觉得那好像是有的。  \n",
       "3           I honestly feel like there is too much.        老实说我觉得那太多了。  \n",
       "4  I think that Yamada is the type everybody likes.    我想山田是受大家欢迎的那种人。  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('data/JEC_basic_sentence_v1-3.xls', header=None)\n",
    "df.columns = ['id', 'japanese', 'english', 'chinese']\n",
    "print('num of data:', len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ja_core_news_sm') # 形態素解析器\n",
    "data = df['japanese']\n",
    "data =  [[token.text for token in nlp(sentence)] for sentence in data]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 前処理\n",
    "\n",
    "このままでは使えないので、前処理を施す"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数値化\n",
    "\n",
    "単語に整数値を割り当てる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['X', 'で', 'は', 'ない', 'か', 'と', 'つくづく', '疑問', 'に', '思う'],\n",
       " ['X', 'が', 'いい', 'な', 'と', 'いつも', '思い', 'ます'],\n",
       " ['それ', 'が', 'ある', 'よう', 'に', 'いつも', '思い', 'ます'],\n",
       " ['それ', 'が', '多', 'すぎ', 'ない', 'か', 'と', '正直', '思う'],\n",
       " ['山田', 'は', 'みんな', 'に', '好か', 'れる', 'タイプ', 'の', '人', 'だ', 'と', '思う']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = data[:5]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 17, 9, 8, 6, 0, 16, 29, 2, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = transforms.VocabTransform(vocab)\n",
    "t(samples[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "\n",
    "前処理をまとめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad, bos, eos, unk = '<pad>', '<bos>', '<eos>', '<unk>'\n",
    "specials = [pad, bos, eos, unk]\n",
    "vocab = build_vocab_from_iterator(data, specials=specials)\n",
    "n_vocab = len(vocab)\n",
    "\n",
    "transform = Compose([\n",
    "    transforms.AddToken(bos, begin=True),\n",
    "    transforms.AddToken(eos, begin=False),\n",
    "    transforms.VocabTransform(vocab),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X', 'で', 'は', 'ない', 'か', 'と', 'つくづく', '疑問', 'に', '思う']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = samples[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 17, 14, 11, 29, 34, 18, 1617, 999, 7, 831, 2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   1,  365,    7,   32,    7, 2997,    4,  180,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0]),\n",
       " tensor([ 365,    7,   32,    7, 2997,    4,  180,    2,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_data, transform):\n",
    "        self._n_samples = len(text_data)\n",
    "        self.data = [transform(text) for text in text_data]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        in_text = self.data[index][:-1]\n",
    "        out_text = self.data[index][1:]\n",
    "        return in_text, out_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._n_samples\n",
    "\n",
    "def to_padded_tensor(text_data: List[int], pad_value: int = 0) -> torch.Tensor:\n",
    "    data = pad_sequence(\n",
    "        [torch.tensor(text) for text in text_data],\n",
    "        batch_first=True,\n",
    "        padding_value=pad_value\n",
    "    )\n",
    "    return data\n",
    "\n",
    "def collate_fn(batch):\n",
    "    in_text, out_text = zip(*batch)\n",
    "    in_text = to_padded_tensor(in_text)\n",
    "    out_text = to_padded_tensor(out_text)\n",
    "    return in_text, out_text\n",
    "\n",
    "dataset = TextDataset(data, transform)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "sample = next(iter(dataloader))\n",
    "sample[0][0], sample[1][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築\n",
    "\n",
    "RNNLMの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, n_vocab, n_hidden):\n",
    "        super().__init__()\n",
    "        self._eye = torch.eye(n_vocab)\n",
    "        self.rnn = nn.RNN(n_vocab, n_hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        x = self._eye.to(x.device)[x]\n",
    "        y, h = self.rnn(x, h)\n",
    "        y = self.fc(y)\n",
    "        return y, h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[pad])\n",
    "def train(model, optimizer, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        for x, t in tqdm(dataloader, desc=f'Epoch {epoch}/{n_epochs}', disable=True):\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y, _ = model(x)\n",
    "            loss = criterion(y.reshape(-1, n_vocab), t.ravel())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'{epoch}/{n_epochs} loss:', loss.item(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNLM(n_vocab, 1024).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20 loss: 6.476790428161621\n",
      "2/20 loss: 5.730929851531982\n",
      "3/20 loss: 5.259773254394531\n",
      "4/20 loss: 4.476101398468018\n",
      "5/20 loss: 4.552317142486572\n",
      "6/20 loss: 4.362425804138184\n",
      "7/20 loss: 4.20493221282959\n",
      "8/20 loss: 4.124094009399414\n",
      "9/20 loss: 3.857569932937622\n",
      "10/20 loss: 3.584982395172119\n",
      "11/20 loss: 3.5141220092773438\n",
      "12/20 loss: 3.2375504970550537\n",
      "13/20 loss: 2.7896676063537598\n",
      "14/20 loss: 2.86423659324646\n",
      "15/20 loss: 2.6452081203460693\n",
      "16/20 loss: 2.2080557346343994\n",
      "17/20 loss: 2.1357791423797607\n",
      "18/20 loss: 1.749802827835083\n",
      "19/20 loss: 1.6092582941055298\n",
      "20/20 loss: 1.677398443222046\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 文章生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sentence(\n",
    "    model: nn.Module,\n",
    "    start: str|None = None,\n",
    "    max_len: int = 30\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    start = start or ''\n",
    "    tokens =  [token.text for token in nlp(start)]\n",
    "    tokens = [vocab[bos]] + [vocab[t] for t in tokens]\n",
    "    tokens = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "\n",
    "    y, h = model(tokens)\n",
    "    next_token = y[0, -1].argmax()\n",
    "\n",
    "    gen_tokens = [next_token.item()]\n",
    "    for _ in range(max_len):\n",
    "        y, h = model(next_token.reshape(1, 1), h)\n",
    "        next_token = y[0, -1].argmax()\n",
    "        gen_tokens.append(next_token.item())\n",
    "        if next_token == vocab[eos]:\n",
    "            break\n",
    "    gen_tokens = [vocab.get_itos()[t] for t in gen_tokens[:-1]]\n",
    "    return start + ''.join(gen_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'私はXには一抹の不安を覚えた'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(model, '私は')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'昨日、やっと今年の仕事が終わりました'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(model, '昨日')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
