{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNLM\n",
    "\n",
    "*Recurrent Neural Network Language Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 18:34:21.002420: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-06 18:34:21.511646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-07-06 18:34:22.102270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-06 18:34:22.103188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-06 18:34:22.103386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext import transforms\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchvision.transforms import Compose\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Dataset\n",
    "\n",
    "[日英中基本文データ - LANGUAGE MEDIA PROCESSING LAB](https://nlp.ist.i.kyoto-u.ac.jp/?%E6%97%A5%E8%8B%B1%E4%B8%AD%E5%9F%BA%E6%9C%AC%E6%96%87%E3%83%87%E3%83%BC%E3%82%BF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 5304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>japanese</th>\n",
       "      <th>english</th>\n",
       "      <th>chinese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#0001</td>\n",
       "      <td>Xではないかとつくづく疑問に思う</td>\n",
       "      <td>I often wonder if it might be X.</td>\n",
       "      <td>难道不会是X吗，我实在是感到怀疑。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#0002</td>\n",
       "      <td>Xがいいなといつも思います</td>\n",
       "      <td>I always think X would be nice.</td>\n",
       "      <td>我总觉得X不错。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#0003</td>\n",
       "      <td>それがあるようにいつも思います</td>\n",
       "      <td>It always seems like it is there.</td>\n",
       "      <td>我总觉得那好像是有的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#0004</td>\n",
       "      <td>それが多すぎないかと正直思う</td>\n",
       "      <td>I honestly feel like there is too much.</td>\n",
       "      <td>老实说我觉得那太多了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#0005</td>\n",
       "      <td>山田はみんなに好かれるタイプの人だと思う</td>\n",
       "      <td>I think that Yamada is the type everybody likes.</td>\n",
       "      <td>我想山田是受大家欢迎的那种人。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id              japanese   \n",
       "0  #0001      Xではないかとつくづく疑問に思う  \\\n",
       "1  #0002         Xがいいなといつも思います   \n",
       "2  #0003       それがあるようにいつも思います   \n",
       "3  #0004        それが多すぎないかと正直思う   \n",
       "4  #0005  山田はみんなに好かれるタイプの人だと思う   \n",
       "\n",
       "                                            english            chinese  \n",
       "0                  I often wonder if it might be X.  难道不会是X吗，我实在是感到怀疑。  \n",
       "1                   I always think X would be nice.           我总觉得X不错。  \n",
       "2                 It always seems like it is there.        我总觉得那好像是有的。  \n",
       "3           I honestly feel like there is too much.        老实说我觉得那太多了。  \n",
       "4  I think that Yamada is the type everybody likes.    我想山田是受大家欢迎的那种人。  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('data/JEC_basic_sentence_v1-3.xls', header=None)\n",
    "df.columns = ['id', 'japanese', 'english', 'chinese']\n",
    "print('num of data:', len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ja_core_news_sm') # 形態素解析器\n",
    "data = df['japanese']\n",
    "data =  [[token.text for token in nlp(sentence)] for sentence in data]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 前処理\n",
    "\n",
    "このままでは使えないので、前処理を施す"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数値化\n",
    "\n",
    "単語に整数値を割り当てる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['X', 'で', 'は', 'ない', 'か', 'と', 'つくづく', '疑問', 'に', '思う'],\n",
       " ['X', 'が', 'いい', 'な', 'と', 'いつも', '思い', 'ます'],\n",
       " ['それ', 'が', 'ある', 'よう', 'に', 'いつも', '思い', 'ます'],\n",
       " ['それ', 'が', '多', 'すぎ', 'ない', 'か', 'と', '正直', '思う'],\n",
       " ['山田', 'は', 'みんな', 'に', '好か', 'れる', 'タイプ', 'の', '人', 'だ', 'と', '思う']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = data[:5]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'疑問': 29,\n",
       " 'ある': 12,\n",
       " '思い': 11,\n",
       " 'ない': 8,\n",
       " 'つくづく': 16,\n",
       " 'タイプ': 23,\n",
       " 'X': 4,\n",
       " '山田': 27,\n",
       " '思う': 3,\n",
       " 'いつも': 5,\n",
       " 'に': 2,\n",
       " 'ます': 10,\n",
       " 'は': 9,\n",
       " 'が': 1,\n",
       " '人': 24,\n",
       " '正直': 28,\n",
       " 'か': 6,\n",
       " 'と': 0,\n",
       " 'いい': 13,\n",
       " 'それ': 7,\n",
       " 'で': 17,\n",
       " 'すぎ': 14,\n",
       " 'の': 19,\n",
       " 'よう': 21,\n",
       " 'れる': 22,\n",
       " 'みんな': 20,\n",
       " 'だ': 15,\n",
       " '多': 25,\n",
       " 'な': 18,\n",
       " '好か': 26}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 17, 9, 8, 6, 0, 16, 29, 2, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = transforms.VocabTransform(vocab)\n",
    "t(samples[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "\n",
    "前処理をまとめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad, bos, eos, unk = '<pad>', '<bos>', '<eos>', '<unk>'\n",
    "specials = [pad, bos, eos, unk]\n",
    "vocab = build_vocab_from_iterator(data, specials=specials)\n",
    "n_vocab = len(vocab)\n",
    "\n",
    "transform = Compose([\n",
    "    transforms.AddToken(bos, begin=True),\n",
    "    transforms.AddToken(eos, begin=False),\n",
    "    transforms.VocabTransform(vocab),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X', 'で', 'は', 'ない', 'か', 'と', 'つくづく', '疑問', 'に', '思う']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = samples[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,   17,   14,   11,   29,   34,   18, 1617,  999,    7,  831,    2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  1, 109, 443,   5, 218,   4, 868,  45,  42,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0]),\n",
       " tensor([109, 443,   5, 218,   4, 868,  45,  42,   2,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_data, transform):\n",
    "        self._n_samples = len(text_data)\n",
    "        self.data = [transform(text) for text in text_data]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        in_text = self.data[index][:-1]\n",
    "        out_text = self.data[index][1:]\n",
    "        return in_text, out_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._n_samples\n",
    "\n",
    "def to_padded_tensor(text_data: List[int], pad_value: int = 0) -> torch.Tensor:\n",
    "    data = pad_sequence(text_data, batch_first=True, padding_value=pad_value)\n",
    "    return data\n",
    "\n",
    "def collate_fn(batch):\n",
    "    in_text, out_text = zip(*batch)\n",
    "    in_text = to_padded_tensor(in_text)\n",
    "    out_text = to_padded_tensor(out_text)\n",
    "    return in_text, out_text\n",
    "\n",
    "dataset = TextDataset(data, transform)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "sample = next(iter(dataloader))\n",
    "sample[0][0], sample[1][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築\n",
    "\n",
    "RNNLMの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, n_vocab, n_hidden):\n",
    "        super().__init__()\n",
    "        self._eye = torch.eye(n_vocab)\n",
    "        self.rnn = nn.RNN(n_vocab, n_hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        x = self._eye.to(x.device)[x]\n",
    "        y, h = self.rnn(x, h)\n",
    "        y = self.fc(y)\n",
    "        return y, h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[pad])\n",
    "def train(model, optimizer, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        for x, t in tqdm(dataloader, desc=f'Epoch {epoch}/{n_epochs}', disable=True):\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y, _ = model(x)\n",
    "            loss = criterion(y.reshape(-1, n_vocab), t.ravel())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'{epoch}/{n_epochs} loss:', loss.item(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNLM(n_vocab, 1024).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20 loss: 6.084326267242432\n",
      "2/20 loss: 5.663784980773926\n",
      "3/20 loss: 5.520203590393066\n",
      "4/20 loss: 4.741633892059326\n",
      "5/20 loss: 4.393838405609131\n",
      "6/20 loss: 4.088660717010498\n",
      "7/20 loss: 3.863466262817383\n",
      "8/20 loss: 3.3202033042907715\n",
      "9/20 loss: 3.1980020999908447\n",
      "10/20 loss: 3.0173561573028564\n",
      "11/20 loss: 2.645954132080078\n",
      "12/20 loss: 2.491687536239624\n",
      "13/20 loss: 2.1107072830200195\n",
      "14/20 loss: 1.9268825054168701\n",
      "15/20 loss: 1.8158470392227173\n",
      "16/20 loss: 1.5304409265518188\n",
      "17/20 loss: 1.4114043712615967\n",
      "18/20 loss: 1.248309850692749\n",
      "19/20 loss: 1.2060176134109497\n",
      "20/20 loss: 1.32364821434021\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 文章生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sentence(\n",
    "    model: nn.Module,\n",
    "    start: str|None = None,\n",
    "    max_len: int = 30\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    start = start or ''\n",
    "    tokens =  [token.text for token in nlp(start)]\n",
    "    tokens = [vocab[bos]] + [vocab[t] for t in tokens]\n",
    "    tokens = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "\n",
    "    y, h = model(tokens)\n",
    "    next_token = y[0, -1].argmax()\n",
    "\n",
    "    gen_tokens = [next_token.item()]\n",
    "    for _ in range(max_len):\n",
    "        y, h = model(next_token.reshape(1, 1), h)\n",
    "        next_token = y[0, -1].argmax()\n",
    "        gen_tokens.append(next_token.item())\n",
    "        if next_token == vocab[eos]:\n",
    "            break\n",
    "    gen_tokens = [vocab.get_itos()[t] for t in gen_tokens[:-1]]\n",
    "    return start + ''.join(gen_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'私はXを視野に入れるべきだ'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(model, '私は')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'昨日、やっと今年の仕事が終わりました'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(model, '昨日')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
