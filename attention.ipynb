{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "言語モデルを学ぶ上では欠かせないAttentionという機構について。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import (\n",
    "    pad_sequence,\n",
    "    pack_padded_sequence,\n",
    "    pad_packed_sequence,\n",
    ")\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress(\n",
    "    width=20,\n",
    "    with_test=True,\n",
    "    label=\"ppl train\",\n",
    "    round=2,\n",
    "    agg_fn=lambda s, w: math.exp(s / w)\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 223108\n",
      "num of vocabrary (ja): 8000\n",
      "num of vocabrary (en): 8000\n"
     ]
    }
   ],
   "source": [
    "textfile_ja = \"data/iwslt2017_ja.txt\"\n",
    "textfile_en = \"data/iwslt2017_en.txt\"\n",
    "tokenizer_prefix_ja = f\"models/tokenizer_iwslt2017_ja\"\n",
    "tokenizer_prefix_en = f\"models/tokenizer_iwslt2017_en\"\n",
    "\n",
    "with open(textfile_en) as f:\n",
    "    data_en = f.read().splitlines()\n",
    "\n",
    "with open(textfile_ja) as f:\n",
    "    data_ja = f.read().splitlines()\n",
    "\n",
    "n_data = len(data_en)\n",
    "print(\"num of data:\", n_data)\n",
    "\n",
    "sp_ja = spm.SentencePieceProcessor(f\"{tokenizer_prefix_ja}.model\")\n",
    "sp_en = spm.SentencePieceProcessor(f\"{tokenizer_prefix_en}.model\")\n",
    "unk_id = sp_ja.unk_id()\n",
    "bos_id = sp_ja.bos_id()\n",
    "eos_id = sp_ja.eos_id()\n",
    "pad_id = sp_ja.pad_id()\n",
    "n_vocab_ja = len(sp_ja)\n",
    "n_vocab_en = len(sp_en)\n",
    "print(\"num of vocabrary (ja):\", n_vocab_ja)\n",
    "print(\"num of vocabrary (en):\", n_vocab_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids_ja = sp_ja.encode(data_ja)\n",
    "data_ids_en = sp_en.encode(data_en)\n",
    "\n",
    "for ids_ja, ids_en in zip(data_ids_ja, data_ids_en):\n",
    "    ids_ja.append(eos_id)\n",
    "    ids_en.insert(0, bos_id)\n",
    "    ids_en.append(eos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train data: 178487\n",
      "num of test data: 44621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 65]), torch.Size([32, 57]), torch.Size([32, 57]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids_ja, data_ids_en):\n",
    "        self.data_ja = [torch.tensor(ids) for ids in data_ids_ja]\n",
    "        self.data_en = [torch.tensor(ids) for ids in data_ids_en]\n",
    "        self.n_data = len(self.data_ja)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ja = self.data_ja[idx]\n",
    "        en = self.data_en[idx]\n",
    "        x_enc = ja\n",
    "        x_dec = en[:-1]\n",
    "        y_dec = en[1:]\n",
    "        return x_enc, x_dec, y_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x_enc, x_dec, y_dec= zip(*batch)\n",
    "    x_enc = pad_sequence(x_enc, batch_first=True, padding_value=pad_id)\n",
    "    x_dec = pad_sequence(x_dec, batch_first=True, padding_value=pad_id)\n",
    "    y_dec = pad_sequence(y_dec, batch_first=True, padding_value=pad_id)\n",
    "    return x_enc, x_dec, y_dec\n",
    "\n",
    "dataset = TextDataset(data_ids_ja, data_ids_en)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "print(\"num of train data:\", len(train_dataset))\n",
    "print(\"num of test data:\", len(test_dataset))\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "x_enc, x_dec, y_dec = next(iter(train_loader))\n",
    "x_enc.shape, x_dec.shape, y_dec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Attention機構\n",
    "\n",
    "複数のデータの中から重要なデータに着目する仕組み。Attention = 注意、注目、着目。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ある一つの入力と、関連する複数のデータを考える。関連する複数のデータはmemoryと呼ぶ。入力を元に、memoryの中のどのデータに着目するかを定めることがAttentionの目的である。各データに重要度を割り当てるという感じ。\n",
    "\n",
    "重要度は重みと呼ばれ、$w_i$で表すことにする。重みは総和が1になるようにsoftmaxなどで正規化する。正規化前の値はスコアと呼んだりする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各memoryに対応する重みは入力との内積で求める。別に内積じゃなくてもいいけど、内積が一番簡単だし性能も良い。内積が取れるように、memoryの各ベクトルは入力と同じ次元にする必要がある。\n",
    "\n",
    "重みを求めた後は、その重みでmemoryの重み付き和をとる。そうすることで、memoryの中から重要な要素を多めに取り出した固定長のベクトルが得られる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "やってみよう。まず、入力と、三つのデータからなるmemoryを用意する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7003,  0.3728,  0.9630, -0.7548,  1.4186],\n",
       "        [-0.3834,  0.7128, -0.5010, -0.5049,  0.4977],\n",
       "        [-0.0481,  0.5967,  2.3803,  0.1163,  0.1539]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, d = 3, 5\n",
    "\n",
    "x = torch.randn(d)\n",
    "memory = torch.randn(n, d)\n",
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力とmemory内の全てのデータで内積を取る。これがスコアに当たる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3264, 0.1925, 0.4811])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.tensor([m @ x for m in memory])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでもいい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = x @ memory.T\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmaxで正規化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = F.softmax(scores, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この重みがmemoryの各データの重要度を表す。これで重み付き和をとる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2286,  0.1217,  0.3143, -0.2464,  0.4630],\n",
       "        [-0.0738,  0.1372, -0.0964, -0.0972,  0.0958],\n",
       "        [-0.0231,  0.2871,  1.1452,  0.0560,  0.0740]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重みをかける\n",
    "weighted_memory = torch.stack([w * m for w, m in zip(weights, memory)])\n",
    "weighted_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1316,  0.5460,  1.3631, -0.2876,  0.6329])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 和をとる\n",
    "y = weighted_memory.sum(dim=0)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のようにまとめられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1316,  0.5460,  1.3631, -0.2876,  0.6329])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = weights @ memory # 重み付き和\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上がattention機構の演算の流れである。まとめるとこう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1316,  0.5460,  1.3631, -0.2876,  0.6329])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = memory @ x # スコア\n",
    "weights = F.softmax(scores, dim=-1) # 重み\n",
    "y = weights @ memory # 重み付き和\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数式だとこうなる。\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\boldsymbol x,M) = \\text{softmax}(\\boldsymbol xM^T)M\n",
    "$$\n",
    "\n",
    "- $\\boldsymbol x\\in\\mathbb R^{d}$ : 入力\n",
    "- $M\\in\\mathbb R^{n\\times d}$ : memory\n",
    "\n",
    "\\*列ベクトルと行ベクトルを区別していないので厳密ではない。厳密に書くならこう:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\boldsymbol x,M) = (\\text{softmax}(\\boldsymbol x^TM^T)M)^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Attentionを用いたSeq2Seq\n",
    "\n",
    "Seq2SeqにAttentionを導入し、Encoderが出力した全ての隠れ状態をDecoderから参照する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecoderにAttentionを取り入れる。入力は前の層からの出力で、memoryはEncoderが出力した全ての隠れ状態である。こうすることで、Decoderは各時刻でその時着目すべき情報に着目した演算が行える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4589,  0.3038,  1.8257, -0.6385,  0.2316])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 3\n",
    "hidden_size = 5\n",
    "hs_enc = torch.randn(seq_len, hidden_size) # encoderが出力した全ての隠れ状態\n",
    "h_dec = torch.randn(hidden_size) # ある時間tのdecoderの隠れ状態\n",
    "\n",
    "scores = h_dec @ hs_enc.T # (seq_len,)\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "y = weights @ hs_enc # (hidden_size,)\n",
    "y # encoderの全ての隠れ状態から重要な部分を多く抜き出したベクトル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに、重みが正しく着目すべき点を表すかは、学習させてみないと分からない。この目的も、学習前の段階では期待に過ぎない。この仕組みを取り入れて学習させれば、次第に適切な重みが出力されるようになり、適切な出力が得られるようになるだろう。そうだといいな、ってだけ。\n",
    "\n",
    "全ての隠れ状態を参照した固定長のベクトルを得るだけであれば、単に全ての隠れ状態を足すだけでもいい。ただ、重みを変えられるような枠組みを取り入れてあげれば学習が上手くいくんじゃね？ってだけ。そして本当にうまくいったからここで紹介されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、重みを求める関数が内積でないといけない理由はない。2つのベクトルからスカラーを得る関数であれば何でもよい。\n",
    "\n",
    "内積は類似度を測ることができ、類似度が高いものに着目するという意味では適切に見えるが、そもそも比較するベクトルはいくつかの層を経て複雑に変化するため、それらの類似度は意味を持たない。重みを求める関数を内積として学習を進めれば、重要度が高くなるべきタイミングでその2つのベクトルが類似するように学習される、というだけ。\n",
    "\n",
    "ただ実際はほとんどの場合で内積が使われる。それは内積という計算がシンプルだからってだけ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention層\n",
    "\n",
    "Decoderの中の、Attentionによって都合のいい隠れ状態を出力する部分は一つの層として見られる。複数時刻の入力を考慮して以下のように表す。\n",
    "\n",
    "$$\n",
    "\\text{Attention}(X,M) = \\text{softmax}(XM^T)M\n",
    "$$\n",
    "\n",
    "- $X\\in\\mathbb R^{n_i\\times d}$ : 層への入力\n",
    "- $M\\in\\mathbb R^{n_m\\times d}$ : memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def forward(self, x, hs):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len_dec, hidden_size)\n",
    "        hs: (batch_size, seq_len_enc, hidden_size)\n",
    "        \"\"\"\n",
    "        scores = x @ hs.mT # (batch_size, seq_len_dec, seq_len_enc)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        h = weights @ hs # (batch_size, seq_len_dec, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_len_dec, seq_len_enc, hidden_size = 2, 3, 4, 5\n",
    "x = torch.randn(batch_size, seq_len_dec, hidden_size)\n",
    "hs = torch.randn(batch_size, seq_len_enc, hidden_size)\n",
    "\n",
    "attention = Attention()\n",
    "h = attention(x, hs)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Attention\n",
    "\n",
    "padトークンがAttentionの計算に含まれてしまうことを回避する。maskをかけてpadトークンに対応する重みが0になるようにする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スコアに対して、対応する位置の値を$-\\infty$にする。そうすればsoftmaxを計算したときにその部分が0になる。\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\boldsymbol x,M) = \\text{softmax}(\\boldsymbol xM^T-\\infty\\,\\text{mask}) M\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こんなスコアがあったとする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7205, -0.7592,  1.8133, -0.2011, -0.0768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.randn(5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後ろの2つがpadトークンだったとすると、こんな感じでmaskをかけてやればいい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7205, -0.7592,  1.8133,    -inf,    -inf])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = [False, False, False, True, True]\n",
    "scores[mask] = -torch.inf\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こう書いてもいい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2911, -1.3570, -0.8464,    -inf,    -inf])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.randn(5)\n",
    "mask = torch.tensor([0, 0, 0, 1, 1])\n",
    "scores.masked_fill_(mask, -torch.inf)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後はこれをsoftmaxに通す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6609, 0.1272, 0.2119, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = F.softmax(scores, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。これでpadトークンが無視されるようになる。\n",
    "\n",
    "層としても実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def forward(self, x, hs, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len_dec, hidden_size)\n",
    "        hs: (batch_size, seq_len_enc, hidden_size)\n",
    "        mask: (batch_size, seq_len_enc), bool, padトークンの位置\n",
    "        \"\"\"\n",
    "        scores = x @ hs.mT # (batch_size, seq_len_dec, seq_len_enc)\n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask.unsqueeze(1), -torch.inf) # maskを適用\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        h = weights @ hs # (batch_size, seq_len_dec, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 実践\n",
    "\n",
    "実際にAttentionをSeqSeqに取り入れて翻訳モデルを学習させてみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル構築\n",
    "\n",
    "前章のSeq2Seqをベースとする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずEncoder。全ての時刻の隠れ状態を出力する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x_pack = pack_padded_sequence(\n",
    "            x, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        hs, (h, _) = self.lstm(x_pack)\n",
    "        hs, _ = pad_packed_sequence(hs, batch_first=True)\n",
    "        return hs, h\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm1 = PackedLSTM(embed_size, hidden_size, True)\n",
    "        self.lstm2 = PackedLSTM(hidden_size * 2, hidden_size, True)\n",
    "        self.lstm3 = PackedLSTM(hidden_size * 2, hidden_size, False)\n",
    "        # self.fc_skip1 = nn.Linear(embed_size, hidden_size * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lengths = (x != pad_id).sum(dim=1).to(\"cpu\")\n",
    "\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "\n",
    "        hs, _ = self.lstm1(x, lengths) # (batch_size, seq_len, hidden_size)\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        skip = hs\n",
    "        hs, _ = self.lstm2(hs, lengths)\n",
    "        hs = hs + skip\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        hs, _ = self.lstm3(hs, lengths)\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        return hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にDecoder。LSTMの後にAttention層を追加する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm1 = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.attention = Attention()\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hs_enc, hc=None, mask=None):\n",
    "        hc1, hc2, hc3 = hc or (None, None, None)\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "\n",
    "        hs, hc1 = self.lstm1(x, hc1) # (batch_size, seq_len, hidden_size)\n",
    "        # hs = self.attention(hs, hs_enc, mask) # (batch_size, seq_len, hidden_size)\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        skip = hs\n",
    "        hs, hc2 = self.lstm2(hs, hc2)\n",
    "        hs = self.attention(hs, hs_enc, mask)\n",
    "        hs = hs + skip\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        skip = hs\n",
    "        hs, hc3 = self.lstm3(hs)\n",
    "        hs = self.attention(hs, hs_enc, mask)\n",
    "        hs = hs + skip\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        y = self.fc(hs) # (batch_size, seq_len, n_vocab)\n",
    "        return y, (hc1, hc2, hc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、これらをまとめる。\n",
    "\n",
    "全ての隠れ状態とpadトークンの位置をDecoderに渡すようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        hs = self.encoder(x_enc)\n",
    "        mask = x_enc == pad_id # (batch_size, seq_len_enc)\n",
    "        y, _ = self.decoder(x_dec, hs, mask=mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of parameters: 32,251,712\n"
     ]
    }
   ],
   "source": [
    "hidden_size, embed_size = 512, 512\n",
    "encoder = Encoder(n_vocab_ja, embed_size, hidden_size)\n",
    "decoder = Decoder(n_vocab_en, embed_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "model_path = \"models/lm_seq2seq_attn.pth\"\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"num of parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention層はパラメータがないのでパラメータ数は変わらない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "def loss_fn(y, t):\n",
    "    loss = cross_entropy(y.reshape(-1, n_vocab_ja), t.ravel())\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for x_enc, x_dec, y_dec in test_loader:\n",
    "        x_enc = x_enc.to(device)\n",
    "        x_dec = x_dec.to(device)\n",
    "        y_dec = y_dec.to(device)\n",
    "\n",
    "        y = model(x_enc, x_dec)\n",
    "        loss = loss_fn(y, y_dec)\n",
    "        losses.append(loss.item())\n",
    "    loss = sum(losses) / len(losses)\n",
    "    ppl = math.exp(loss)\n",
    "    return ppl\n",
    "\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    prog.start(n_iter=len(train_loader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for x_enc, x_dec, y_dec in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prog.update(loss.item())\n",
    "\n",
    "        if prog.now_epoch % prog_unit == 0:\n",
    "            test_ppl = eval_model(model)\n",
    "            prog.memo(f\"test: {test_ppl:.2f}\", no_step=True)\n",
    "        prog.memo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/20: #################### 100% [00:06:30.02] ppl train: 158.61, test: 96.51 \n",
      " 2/20: #################### 100% [00:06:37.14] ppl train: 89.48, test: 72.64 \n",
      " 3/20: #################### 100% [00:06:52.74] ppl train: 69.79, test: 60.01 \n",
      " 4/20: #################### 100% [00:06:42.79] ppl train: 57.25, test: 50.24 \n",
      " 5/20: #################### 100% [00:06:50.31] ppl train: 47.81, test: 43.32 \n",
      " 6/20: #################### 100% [00:06:55.76] ppl train: 40.74, test: 38.34 \n",
      " 7/20: #################### 100% [00:06:59.73] ppl train: 35.50, test: 34.87 \n",
      " 8/20: #################### 100% [00:07:10.61] ppl train: 31.50, test: 32.33 \n",
      " 9/20: #################### 100% [00:07:11.84] ppl train: 28.43, test: 30.49 \n",
      "10/20: #################### 100% [00:07:03.03] ppl train: 25.98, test: 29.15 \n",
      "11/20: #################### 100% [00:06:59.14] ppl train: 23.97, test: 28.01 \n",
      "12/20: #################### 100% [00:06:58.95] ppl train: 22.28, test: 27.27 \n",
      "13/20: #################### 100% [00:07:03.81] ppl train: 20.86, test: 26.72 \n",
      "14/20: #################### 100% [00:07:03.88] ppl train: 19.63, test: 26.23 \n",
      "15/20: #################### 100% [00:08:35.13] ppl train: 18.57, test: 26.01 \n",
      "16/20: #################### 100% [00:07:59.92] ppl train: 17.61, test: 25.88 \n",
      "17/20: #################### 100% [00:19:58.04] ppl train: 16.78, test: 25.73 \n",
      "18/20: #################### 100% [00:33:02.87] ppl train: 16.03, test: 25.59 \n",
      "19/20: #################### 100% [00:27:12.00] ppl train: 15.35, test: 25.78 \n",
      "20/20: #################### 100% [00:26:37.08] ppl train: 14.73, test: 25.65 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, n_epochs=20, prog_unit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 翻訳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_sampling(y, decisive=True):\n",
    "    y = y.squeeze(0, 1)\n",
    "    if decisive:\n",
    "        token = y.argmax().item()\n",
    "    else:\n",
    "        y[unk_id] = -torch.inf\n",
    "        probs = F.softmax(y, dim=-1)\n",
    "        token, = random.choices(range(n_vocab_en), weights=probs)\n",
    "    return token\n",
    "\n",
    "\n",
    "bos_id = sp_en.bos_id()\n",
    "eos_id = sp_en.eos_id()\n",
    "@torch.no_grad()\n",
    "def translate(\n",
    "    model: nn.Module,\n",
    "    in_text: str,\n",
    "    max_len: int = 100,\n",
    "    decisive: bool = True,\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    in_ids = sp_ja.encode(in_text)\n",
    "    in_ids = torch.tensor(in_ids + [eos_id], device=device).unsqueeze(0)\n",
    "\n",
    "    hs_enc = model.encoder(in_ids)\n",
    "    hc = None\n",
    "    next_token = bos_id\n",
    "\n",
    "    token_ids = []\n",
    "    while len(token_ids) <= max_len and next_token != eos_id:\n",
    "        x = torch.tensor([next_token], device=device).reshape(1, 1)\n",
    "        y, hc = model.decoder(x, hs_enc, hc)\n",
    "        next_token = token_sampling(y, decisive)\n",
    "        token_ids.append(next_token)\n",
    "\n",
    "    sentence = sp_en.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 計画は変更を余儀なくされました\n",
      "output: The plan was to change their plans.\n",
      "answer: We needed to change our schedule.\n",
      "\n",
      "input: このような転換は 脳を理解する方法でも見られます\n",
      "output: And we can see this shift in the brain.\n",
      "answer: We can see this shift in the way we try to understand the brain.\n",
      "\n",
      "input: そして 他にどんな特徴があるでしょう?\n",
      "output: And what else is the other characteristic?\n",
      "answer: But what else is life characterized by?\n",
      "\n",
      "input: グーグル・グラスの開発では どうすれば手を自由にできるかと いうことを考え抜きました\n",
      "output: So, I thought, \"How can we do this with Google Earth, and do we do something about what we can do with this thing to do with this thing we can do with Google Earth, and do we do something about what we can do with this.\n",
      "answer: So when we developed Glass, we thought really about, can we make something that frees your hands?\n",
      "\n",
      "input: 私たちが作ったものは ユニークではありませんが\n",
      "output: We're not unique.\n",
      "answer: What we've built isn't unique.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(train_dataset))\n",
    "    x, _, t = train_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print(\"input:\", x)\n",
    "    print(\"output:\", translate(model, x))\n",
    "    print(\"answer:\", t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: そのころ裕福な男性は考えていました 「自分はなんてバカなんだ。神がパンを必要とするはずはない」\n",
      "output: And that was a rich man, and he said, \"You know, God's really stupid, God's God's God's God's God's God, God's bread must be.\"\n",
      "answer: Meanwhile, the rich man thinks to himself, \"I'm an idiot. God wants bread?\n",
      "\n",
      "input: 人生とはパラドックスを受け入れること。\n",
      "output: My life is not going to accept the paradox of life.\n",
      "answer: It's about embracing the paradox.\n",
      "\n",
      "input: 地球温暖化は事実です この件に関し 私は専門家ですが\n",
      "output: Global warming, and I'm going to do this, and I'm going to do this, and I'm a expert.\n",
      "answer: I'm a believer in global warming, and my record is good on the subject.\n",
      "\n",
      "input: 細菌はウイルスがいる環境に 対処しなければなりません ウイルス感染とは チクタク動く時限爆弾のようなもの つまり 細菌は起爆までの数分間で 信管を外す必要があるのです\n",
      "output: And the bacteria have been able to deal with viruses that bacteria have viruses in the virus, and the virus can be used to take the hole to the bacteria, and they need to take the tubes to take the tubes of their DNA, and they need to take the tubes to take out the door to the bacteria, and they need to take the tubes to take out the door to the bacteria, and they need to take the tubes to take out the door to the bacteria, and\n",
      "answer: Bacteria have to deal with viruses in their environment, and we can think about a viral infection like a ticking time bomb -- a bacterium has only a few minutes to defuse the bomb before it gets destroyed.\n",
      "\n",
      "input: 皆知っているのですから 規制なんて必要ありません\n",
      "output: We know that we don't need regulations.\n",
      "answer: Now, there is no need for these restrictions at all.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(test_dataset))\n",
    "    x, _, t = test_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print(\"input:\", x)\n",
    "    print(\"output:\", translate(model, x))\n",
    "    print(\"answer:\", t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ありがとう。\n",
      "output: Thank you.\n",
      "\n",
      "input: 猫はかわいいね。\n",
      "output: The cat is good.\n",
      "\n",
      "input: 上手く文章が書けるようになりました。\n",
      "output: And he wrote a book.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "sentences = [\n",
    "    \"ありがとう。\",\n",
    "    \"猫はかわいいね。\",\n",
    "    \"上手く文章が書けるようになりました。\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"input:\", sentence)\n",
    "    print(\"output:\", translate(model, sentence))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "少しマシになったかも。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Attentionの可視化\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
