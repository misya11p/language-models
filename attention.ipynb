{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 443596\n"
     ]
    }
   ],
   "source": [
    "textfile_ja = 'data/kyoto_ja.txt'\n",
    "textfile_en = 'data/kyoto_en.txt'\n",
    "\n",
    "with open(textfile_en) as f:\n",
    "    data_en = f.readlines()\n",
    "\n",
    "with open(textfile_ja) as f:\n",
    "    data_ja = f.readlines()\n",
    "\n",
    "n_data = len(data_en)\n",
    "print('num of data:', n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary (ja): 8000\n",
      "num of vocabrary (en): 8000\n"
     ]
    }
   ],
   "source": [
    "tokenizer_prefix_ja = 'models/tokenizer_kyoto_ja'\n",
    "tokenizer_prefix_en = 'models/tokenizer_kyoto_en'\n",
    "sp_ja = spm.SentencePieceProcessor(f'{tokenizer_prefix_ja}.model')\n",
    "sp_en = spm.SentencePieceProcessor(f'{tokenizer_prefix_en}.model')\n",
    "n_vocab_ja = len(sp_ja)\n",
    "n_vocab_en = len(sp_en)\n",
    "print('num of vocabrary (ja):', n_vocab_ja)\n",
    "print('num of vocabrary (en):', n_vocab_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids_ja = sp_ja.encode(data_ja)\n",
    "data_ids_en = sp_en.encode(data_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_id = sp_ja.bos_id()\n",
    "eos_id = sp_ja.eos_id()\n",
    "for ids_ja, ids_en in zip(data_ids_ja, data_ids_en):\n",
    "    ids_en.insert(0, bos_id)\n",
    "    ids_ja.append(eos_id)\n",
    "    ids_en.append(eos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1023, 1600,  233, 1843,  386,    9,  911,  355, 1014, 1067,  248,  391,\n",
       "            90,    5, 2206,   11,  554,   28,  248,  391,   90,   34,   27, 1711,\n",
       "           148,  317,   24, 2911,  294,   44,    4, 3483,  563, 3657, 1785,   27,\n",
       "           670,  104, 1512,  248,  391,   90,   15, 3936,    5, 1785,   27, 7355,\n",
       "            72, 3349,  417,    4,  911,  355, 1014, 1067,  294,  220,  341, 1314,\n",
       "          1847, 1993,  450,    7,    2]]),\n",
       " tensor([[   1,  268,   48,  355,  117,   13, 3086,   90, 7082,    8,    9, 1855,\n",
       "          2691,    8,   21, 1119,    4, 1270,    8, 1142,  276,  170, 1628,    6,\n",
       "            13, 3086,   90,   20,   50,  703,    6, 3329,   14,  254,  260,  163,\n",
       "           111,    5,    9,  295,  110,   48, 2446,    9, 1265,    8,   11, 3564,\n",
       "          3359, 1973,    9, 5646,   21,   48,  742,   12,  273,    6,   13, 3086,\n",
       "            90,    5,  180, 3253,   36,  379, 1008,   20,   50,  703,    6, 3329,\n",
       "            14,  254,  260,  163,  111,    7]]),\n",
       " tensor([[ 268,   48,  355,  117,   13, 3086,   90, 7082,    8,    9, 1855, 2691,\n",
       "             8,   21, 1119,    4, 1270,    8, 1142,  276,  170, 1628,    6,   13,\n",
       "          3086,   90,   20,   50,  703,    6, 3329,   14,  254,  260,  163,  111,\n",
       "             5,    9,  295,  110,   48, 2446,    9, 1265,    8,   11, 3564, 3359,\n",
       "          1973,    9, 5646,   21,   48,  742,   12,  273,    6,   13, 3086,   90,\n",
       "             5,  180, 3253,   36,  379, 1008,   20,   50,  703,    6, 3329,   14,\n",
       "           254,  260,  163,  111,    7,    2]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids_ja, data_ids_en):\n",
    "        self.data_ja = [torch.tensor(ids) for ids in data_ids_ja]\n",
    "        self.data_en = [torch.tensor(ids) for ids in data_ids_en]\n",
    "        self.n_data = len(self.data_ja)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ja = self.data_ja[idx]\n",
    "        en = self.data_en[idx]\n",
    "        x_enc = ja # encoderへの入力\n",
    "        x_dec = en[:-1] # decoderへの入力\n",
    "        y_dec = en[1:] # decoderの出力\n",
    "        return x_enc, x_dec, y_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "batch_size = 1\n",
    "dataset = TextDataset(data_ids_ja, data_ids_en)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "x_enc, x_dec, y_dec = next(iter(dataloader))\n",
    "x_enc, x_dec, y_dec # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        _, h = self.rnn(x) # h: (1, batch_size, hidden_size)\n",
    "        h = self.fc(h) # (1, batch_size, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        x = self.embedding(x) # (seq_len, embed_size)\n",
    "        y, h = self.rnn(x, h) # y: (seq_len, hidden_size), h: (1, hidden_size)\n",
    "        y = self.fc(y) # (seq_len, n_vocab)\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        h = self.encoder(x_enc)\n",
    "        y, _ = self.decoder(x_dec, h)\n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
