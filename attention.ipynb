{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "複数のデータの中から重要なデータに注目する仕組み。  \n",
    "Attentionを用いることで、decoderが、encoderが出力した情報の中の重要な情報に注目するようになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前章で作成した翻訳モデルは、入力文をencoderによって固定長のベクトルに変換し、それをdecoderに渡すことで、入力文に基づいた出力文を生成した。  \n",
    "encoderがRNNであるとき、encoderは各時間で隠れ状態を出力する。この中から最後の時間の隠れ状態のみをdecoderに渡していたものがこれまでのseq2seqである。\n",
    "\n",
    "このとき、encoderが出力する全ての隠れ状態を利用したいと考える。その方が入力文の多くの情報を参照でき、より適切な出力が得られそうだ。Attentionはそれを実現する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Attention機構\n",
    "\n",
    "Attention機構について詳しく見ていく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "といっても、そんなに難しいことはない。  \n",
    "複数のデータがあった時に、各データに重要度を割り当てるだけである。これは重みと呼ばれ$w_i$で表す。重みの総和は1とする。正規化前の重みはスコアと呼んだりする。\n",
    "\n",
    "重要度は別の何らかのデータに基づいて設定される。一般的に、そのデータとの内積を取ることが多い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3つの隠れ状態$h_i$があるとする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1926,  0.7712, -1.2606,  0.1052, -0.3315],\n",
       "        [-1.6132,  0.5774, -1.1709, -0.2905, -0.2942],\n",
       "        [-0.4514,  1.4576,  0.9403,  0.1590, -0.2817]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, hidden_size = 3, 5\n",
    "hs = torch.randn(batch_size, hidden_size)\n",
    "hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隠れ状態$h_i$と同じサイズの適当なデータ$x$があるとする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0412,  0.1183,  0.6055, -0.6576,  0.1853])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x$と全ての隠れ状態$h_i$で内積を取り、softmaxで正規化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1544, 0.2206, 0.6250])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = hs @ x\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この3つが重みで、$x$にとっての$h_i$の重要度を表す。\n",
    "\n",
    "この重みで$h_i$の重み付き和をとることで、$x$にとっての重要度を反映したただ1つの隠れ状態を得ることが出来る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6677,  1.1575,  0.1347,  0.0515, -0.2921])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = weights @ hs\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seqのdecoderでは、このattentionを利用して、encoderが出力した全ての隠れ状態を参照する。  \n",
    "\n",
    "encoderの隠れ状態はencoderへの入力数によって変わる。可変長のデータをモデル内で扱うことは困難とされている。  \n",
    "そこで、attentionを用いて1つの固定長のベクトルに変換する。decoderの演算時に、decoderのRNNから出力された隠れ状態を用いてencoderの隠れ状態の重要度を計算し、重み付き和をとる。こうすることで、encoderの隠れ状態から注目すべき重要な情報を都合よく抽出した固定長のベクトルを得ることが出来る。後はそれを以降の層に渡すだけ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお、重みが正しく着目すべき点を表すかは、学習させてみないと分からない。  \n",
    "この目的も、学習前の段階では期待に過ぎない。この仕組みを取り入れて学習させれば、次第に適切な重みが出力されるようになり、適切な出力が得られるようになるだろう。そうだといいな、ってだけ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、重みを求める関数が内積でないといけない理由はない。2つのベクトルからスカラーを得る関数であれば何でもよい。  \n",
    "内積は類似度を測れ、類似度が高いものに着目するという意味では適切に見えるが、そもそも内積をとるベクトルは入力されてから幾度の変換を経て得られたもので、それらの類似度は意味を持たない。  \n",
    "重みを求める関数を内積として学習を進めれば、重要度が高くなるべきタイミングでその2つのベクトルが似るように学習される、というだけ。\n",
    "\n",
    "ただ実際はほとんどの場合で内積が使われる。それは内積という計算がシンプルだからってだけだと思う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention層\n",
    "\n",
    "decoderの中で、attentionによって都合のいい隠れ状態を出力する部分は1つの層として見られる。  \n",
    "実装してみよう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずはシンプルなものから。  \n",
    "単一の時間で機能するものを作る。バッチサイズも1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def forward(self, x, hs):\n",
    "        \"\"\"\n",
    "        x: (hidden_size)\n",
    "        hs: (seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        attention_score = hs @ x # (seq_len,)\n",
    "        weights = F.softmax(attention_score, dim=-1) # (seq_len,)\n",
    "        y = weights @ hs # (hidden_size,)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8188,  1.4933, -0.5871, -0.7322, -1.0259])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs = torch.randn(batch_size, hidden_size)\n",
    "x = torch.randn(hidden_size)\n",
    "\n",
    "attention = AttentionLayer()\n",
    "h = attention(x, hs)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次はより実践的なものを作る。  \n",
    "バッチサイズを考慮した上で全ての時間を処理する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def forward(self, x, hs, pad_positions=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len_dec, hidden_size)\n",
    "        hs: (batch_size, seq_len_enc, hidden_size)\n",
    "        pad_positions: (batch_size, seq_len_enc, hidden_size)\n",
    "        \"\"\"\n",
    "        seq_len_dec = x.shape[1]\n",
    "        attention_score = torch.matmul(x, hs.transpose(1, 2))\n",
    "            # (batch_size, seq_len(dec), seq_len(enc))\n",
    "        weights = F.softmax(attention_score, dim=-1)\n",
    "        y = [\n",
    "            (hs * weights[:, i].unsqueeze(-1)).sum(dim=1) \\\n",
    "                for i in range(seq_len_dec)\n",
    "        ] # (seq_len(dec), batch_size, hidden_size)\n",
    "        y = torch.stack(y, dim=1) # (batch_size, seq_len(dec), hidden_size)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これもうちょい綺麗に実装する方法ないのかな。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 11])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_len_dec, seq_len_enc, hidden_size = 3, 5, 7, 11\n",
    "x = torch.randn(batch_size, seq_len_dec, hidden_size)\n",
    "hs = torch.randn(batch_size, seq_len_enc, hidden_size)\n",
    "\n",
    "attention = AttentionLayer()\n",
    "h = attention(x, hs)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 言語モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 10000\n"
     ]
    }
   ],
   "source": [
    "textfile_ja = 'data/kyoto_ja_10000.txt'\n",
    "textfile_en = 'data/kyoto_en_10000.txt'\n",
    "\n",
    "with open(textfile_en) as f:\n",
    "    data_en = f.readlines()\n",
    "\n",
    "with open(textfile_ja) as f:\n",
    "    data_ja = f.readlines()\n",
    "\n",
    "n_data = len(data_en)\n",
    "print('num of data:', n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary (ja): 8000\n",
      "num of vocabrary (en): 8000\n"
     ]
    }
   ],
   "source": [
    "tokenizer_prefix_ja = 'models/tokenizer_kyoto_ja_10000'\n",
    "tokenizer_prefix_en = 'models/tokenizer_kyoto_en_10000'\n",
    "sp_ja = spm.SentencePieceProcessor(f'{tokenizer_prefix_ja}.model')\n",
    "sp_en = spm.SentencePieceProcessor(f'{tokenizer_prefix_en}.model')\n",
    "n_vocab_ja = len(sp_ja)\n",
    "n_vocab_en = len(sp_en)\n",
    "pad_id = sp_ja.pad_id()\n",
    "print('num of vocabrary (ja):', n_vocab_ja)\n",
    "print('num of vocabrary (en):', n_vocab_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids_ja = sp_ja.encode(data_ja)\n",
    "data_ids_en = sp_en.encode(data_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_id = sp_ja.bos_id()\n",
    "eos_id = sp_ja.eos_id()\n",
    "for ids_ja, ids_en in zip(data_ids_ja, data_ids_en):\n",
    "    ids_en.insert(0, bos_id)\n",
    "    ids_ja.append(eos_id)\n",
    "    ids_en.append(eos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids_ja, data_ids_en):\n",
    "        self.data_ja = [torch.tensor(ids) for ids in data_ids_ja]\n",
    "        self.data_en = [torch.tensor(ids) for ids in data_ids_en]\n",
    "        self.n_data = len(self.data_ja)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ja = self.data_ja[idx]\n",
    "        en = self.data_en[idx]\n",
    "        x_enc = ja # encoderへの入力\n",
    "        x_dec = en[:-1] # decoderへの入力\n",
    "        y_dec = en[1:] # decoderの出力\n",
    "        return x_enc, x_dec, y_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "def collate_fn(batch): # padding\n",
    "    x_enc, x_dec, y_dec= zip(*batch)\n",
    "    x_enc = pad_sequence(x_enc, batch_first=True, padding_value=pad_id)\n",
    "    x_dec = pad_sequence(x_dec, batch_first=True, padding_value=pad_id)\n",
    "    y_dec = pad_sequence(y_dec, batch_first=True, padding_value=pad_id)\n",
    "    return x_enc, x_dec, y_dec\n",
    "\n",
    "batch_size = 1\n",
    "batch_size = 64\n",
    "dataset = TextDataset(data_ids_ja, data_ids_en)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        hs, h = self.rnn(x)\n",
    "            # hs: (batch_size, seq_len, hidden_size)\n",
    "            # h: (1, batch_size, hidden_size)\n",
    "        hs = self.fc(hs) # (batch_size, seq_len, hidden_size)\n",
    "        return hs, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.attention = AttentionLayer()\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, h, hs):\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        hs_dec, h = self.rnn(x, h)\n",
    "            # hs_dec: (batch_size, seq_len, hidden_size)\n",
    "            # h: (1, batch_size, hidden_size)\n",
    "        y = self.attention(hs_dec, hs) # (batch_size, seq_len, hidden_size)\n",
    "        y = self.fc(y) # (batch_size, seq_len, n_vocab)\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        hs, h = self.encoder(x_enc)\n",
    "        y, _ = self.decoder(x_dec, h, hs)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, n_epochs, prog_unit=1):\n",
    "    model.train()\n",
    "    prog.start(n_iter=len(dataloader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        for x_enc, x_dec, y_dec in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "\n",
    "            y_pred = model(x_enc, x_dec)\n",
    "            loss = criterion(y_pred.reshape(-1, n_vocab_ja), y_dec.ravel())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prog.update(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size, embed_size = 1024, 1024\n",
    "encoder = Encoder(n_vocab_ja, embed_size, hidden_size)\n",
    "decoder = Decoder(n_vocab_en, embed_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1-10/100: ######################################## 100% [00:16:17.81] loss: 1.73452 \n",
      "  11-20/100: ####################                      50% [00:09:13.56] loss: 1.54642 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/komiya/private/study/language-models/attention.ipynb セル 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/komiya/private/study/language-models/attention.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train(model, optimizer, criterion, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, prog_unit\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/home/komiya/private/study/language-models/attention.ipynb セル 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/komiya/private/study/language-models/attention.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(x_enc, x_dec)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/komiya/private/study/language-models/attention.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_pred\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, n_vocab_ja), y_dec\u001b[39m.\u001b[39mravel())\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/komiya/private/study/language-models/attention.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/komiya/private/study/language-models/attention.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/komiya/private/study/language-models/attention.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m prog\u001b[39m.\u001b[39mupdate(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/nlp/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/nlp/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, n_epochs=100, prog_unit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_id = sp_en.unk_id() # UNKのID\n",
    "def token_sampling(y: List[float]) -> int:\n",
    "    \"\"\"モデルの出力から単語をサンプリングする\"\"\"\n",
    "    y[unk_id] = -torch.inf\n",
    "    probs = F.softmax(y, dim=-1)\n",
    "    token, = random.choices(range(n_vocab_en), weights=probs)\n",
    "    return token\n",
    "\n",
    "\n",
    "bos_id = sp_en.bos_id()\n",
    "eos_id = sp_en.eos_id()\n",
    "@torch.no_grad()\n",
    "def translate(\n",
    "    model: nn.Module,\n",
    "    in_text: str, # 入力文（日本語）\n",
    "    max_len: int = 100, # 出力のトークン数の上限\n",
    "    decisive: bool = True, # サンプリングを決定的にするか\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    in_ids = sp_ja.encode(in_text)\n",
    "    in_ids = torch.tensor(in_ids + [eos_id], device=device).unsqueeze(0)\n",
    "\n",
    "    hs, h = model.encoder(in_ids)\n",
    "    next_token = bos_id\n",
    "\n",
    "    token_ids = []\n",
    "    for _ in range(max_len):\n",
    "        x = torch.tensor([[next_token]], device=device)\n",
    "        y, h = model.decoder(x, h, hs)\n",
    "        y = y[0]\n",
    "        if decisive:\n",
    "            next_token = y.argmax().item()\n",
    "        else:\n",
    "            next_token = token_sampling(y)\n",
    "        token_ids.append(next_token)\n",
    "        if next_token == eos_id:\n",
    "            break\n",
    "    sentence = sp_en.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 駅情報\n",
      "\n",
      "output: \n",
      "answer: Information\n",
      "\n",
      "\n",
      "input: 三条京阪駅（さんじょうけいはんえき）は、京都市東山区にある、京都市営地下鉄東西線の鉄道駅。\n",
      "\n",
      "output: cho Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto Kyoto\n",
      "answer: Located in the Higashiyama Ward of Kyoto City, Sanjyo-Keihan Station is a stop on the Tozai Line, a Kyoto Municipal Subway Line.\n",
      "\n",
      "\n",
      "input: 駅番号はT11。\n",
      "\n",
      "output: The station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station\n",
      "answer: The station number is T11.\n",
      "\n",
      "\n",
      "input: 京阪電気鉄道\n",
      "\n",
      "output: Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric Electric\n",
      "answer: The Keihan Electric Railway\n",
      "\n",
      "\n",
      "input: 京阪本線・京阪鴨東線（三条駅 (京都府)）\n",
      "\n",
      "output: Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station Station\n",
      "answer: Keihan Main Line and Keihan Oto Line in Sanjyo Station (in Kyoto Prefecture)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for x, t in zip(data_ja[:n], data_en[:n]):\n",
    "    print('input:', x)\n",
    "    print('output:', translate(model, x))\n",
    "    print('answer:', t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
