{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "複数のデータの中から重要なデータに注目する仕組み。  \n",
    "attentionを用いることで、decoderが、encoderが出力した情報の中の重要な情報に注目するようになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前章で作成した翻訳モデルは、入力文をencoderによって固定長のベクトルに変換し、それをdecoderに渡すことで、入力文に基づいた出力文を生成した。  \n",
    "encoderがRNNであるとき、encoderは各時間で隠れ状態を出力する。この中から最後の時間の隠れ状態のみをdecoderに渡していたものがこれまでのseq2seqである。\n",
    "\n",
    "このとき、encoderが出力する全ての隠れ状態を利用したいと考える。その方が入力文の多くの情報を参照でき、より適切な出力が得られそうだ。attentionはそれを実現する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress(with_test=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データの用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 10000\n",
      "num of vocabrary (ja): 8000\n",
      "num of vocabrary (en): 8000\n"
     ]
    }
   ],
   "source": [
    "textfile_ja = 'data/iwslt2017_ja_10000.txt'\n",
    "textfile_en = 'data/iwslt2017_en_10000.txt'\n",
    "tokenizer_prefix_ja = f'models/tokenizer_iwslt2017_ja_10000'\n",
    "tokenizer_prefix_en = f'models/tokenizer_iwslt2017_en_10000'\n",
    "\n",
    "with open(textfile_en) as f:\n",
    "    data_en = f.read().splitlines()\n",
    "\n",
    "with open(textfile_ja) as f:\n",
    "    data_ja = f.read().splitlines()\n",
    "\n",
    "n_data = len(data_en)\n",
    "print('num of data:', n_data)\n",
    "\n",
    "sp_ja = spm.SentencePieceProcessor(f'{tokenizer_prefix_ja}.model')\n",
    "sp_en = spm.SentencePieceProcessor(f'{tokenizer_prefix_en}.model')\n",
    "unk_id = sp_ja.unk_id()\n",
    "bos_id = sp_ja.bos_id()\n",
    "eos_id = sp_ja.eos_id()\n",
    "pad_id = sp_ja.pad_id()\n",
    "n_vocab_ja = len(sp_ja)\n",
    "n_vocab_en = len(sp_en)\n",
    "print('num of vocabrary (ja):', n_vocab_ja)\n",
    "print('num of vocabrary (en):', n_vocab_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids_ja = sp_ja.encode(data_ja)\n",
    "data_ids_en = sp_en.encode(data_en)\n",
    "\n",
    "for ids_ja, ids_en in zip(data_ids_ja, data_ids_en):\n",
    "    ids_ja.append(eos_id)\n",
    "    ids_en.insert(0, bos_id)\n",
    "    ids_en.append(eos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids_ja, data_ids_en):\n",
    "        self.data_ja = [torch.tensor(ids) for ids in data_ids_ja]\n",
    "        self.data_en = [torch.tensor(ids) for ids in data_ids_en]\n",
    "        self.n_data = len(self.data_ja)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ja = self.data_ja[idx]\n",
    "        en = self.data_en[idx]\n",
    "        x_enc = ja\n",
    "        x_dec = en[:-1]\n",
    "        y_dec = en[1:]\n",
    "        return x_enc, x_dec, y_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x_enc, x_dec, y_dec= zip(*batch)\n",
    "    x_enc = pad_sequence(x_enc, batch_first=True, padding_value=pad_id)\n",
    "    x_dec = pad_sequence(x_dec, batch_first=True, padding_value=pad_id)\n",
    "    y_dec = pad_sequence(y_dec, batch_first=True, padding_value=pad_id)\n",
    "    return x_enc, x_dec, y_dec\n",
    "\n",
    "dataset = TextDataset(data_ids_ja, data_ids_en)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Attention\n",
    "\n",
    "attention機構について詳しく見ていく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ある1つの入力と、関連する複数のデータを考える。複数のデータはmemoryと呼ぶ。  \n",
    "入力を元に、memoryの中のどのデータに着目するかを定めることがattentionの目的である。各データに重要度を割り当てるという感じ。\n",
    "\n",
    "重要度は重みと呼ばれ、$w_i$で表すことにする。  \n",
    "重みは総和が1になるようにsoftmaxなどで正規化する。正規化前の値はスコアと呼んだりする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各重みは入力との内積で求める。別に内積じゃなくてもいいけど、内積が一番簡単だし性能も良い。  \n",
    "内積が取れるように、memoryの各ベクトルは入力と同じ次元にする必要がある。\n",
    "\n",
    "重みを求めた後は、その重みでmemoryの重み付き和をとる。そうすることで、memoryの中から重要な要素を多めに取り出した固定長のベクトルが得られる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "やってみよう。  \n",
    "入力と、3つのデータからなるmemoryを用意する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2305,  0.4712,  1.5261, -0.8287,  1.9600],\n",
       "        [-1.3293, -1.4682,  1.5644,  0.0796, -1.7792],\n",
       "        [-0.6182,  0.9205, -0.0433,  0.8591, -0.6128]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, d = 3, 5\n",
    "\n",
    "x = torch.randn(d) # 入力\n",
    "memory = torch.randn(n, d) # memory\n",
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力とmemory内の全てのデータで内積を取り、softmaxで正規化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6799, 0.1905, 0.1297])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = memory @ x # スコア (内積)\n",
    "weights = F.softmax(scores, dim=-1) # 重み\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スコアはこれと同じ意味。\n",
    "\n",
    "```python\n",
    "scores = torch.tensor([m @ x for m in memory])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この重みがmemoryの各データの重要度を表す。これで重み付き和をとる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1567,  0.3204,  1.0375, -0.5634,  1.3326],\n",
       "        [-0.2532, -0.2796,  0.2979,  0.0152, -0.3389],\n",
       "        [-0.0802,  0.1194, -0.0056,  0.1114, -0.0795]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重みをかける\n",
    "weighted_memory = torch.stack([w * m for w, m in zip(weights, memory)])\n",
    "weighted_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4900,  0.1601,  1.3299, -0.4369,  0.9143])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 和をとる\n",
    "y = weighted_memory.sum(dim=0)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のようにまとめられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4900,  0.1601,  1.3299, -0.4369,  0.9143])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = weights @ memory # 重み付き和\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上がattention機構の演算の流れである。  \n",
    "まとめるとこう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4900,  0.1601,  1.3299, -0.4369,  0.9143])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = memory @ x # スコア (内積)\n",
    "weights = F.softmax(scores, dim=-1) # 重み\n",
    "y = memory.T @ weights # 重み付き和\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数式だとこうなる。\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\boldsymbol x,M) = M^T\\text{softmax}(M\\boldsymbol x)\n",
    "$$\n",
    "\n",
    "- $\\boldsymbol x\\in\\R^{d}$ : 入力\n",
    "- $M\\in\\R^{n\\times d}$ : memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seqへの導入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seqのdecoderでは、このattentionを利用して、encoderが出力した全ての隠れ状態を参照する。  \n",
    "\n",
    "現状、encoderの隠れ状態は最後のもののみがdecoderに渡っているが、可能なら全ての隠れ状態をdecoderに渡したい。しかし、encoderの隠れ状態の数はencoderへの入力の数によって変化し、可変長のデータをモデル内で扱うことは難しい。  \n",
    "そこでattentionを用いる。decoderのある時間の隠れ状態を入力、encoderの全ての隠れ状態をmemoryとしてattentionを行う。こうすることで、encoderの隠れ状態から注目すべき重要な情報を都合よく抽出した固定長のベクトルを得ることが出来る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4763, -0.0759,  0.3263,  1.7235, -0.4055])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 3\n",
    "hidden_size = 5\n",
    "hs_enc = torch.randn(seq_len, hidden_size) # encoderが出力した全ての隠れ状態\n",
    "h_dec = torch.randn(hidden_size) # ある時間tのdecoderの隠れ状態\n",
    "\n",
    "scores = h_dec @ hs_enc.T # (seq_len,)\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "y = weights @ hs_enc # (hidden_size,)\n",
    "y # encoderの全ての隠れ状態の重要な部分に着目して得たベクトル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに、重みが正しく着目すべき点を表すかは、学習させてみないと分からない。  \n",
    "この目的も、学習前の段階では期待に過ぎない。この仕組みを取り入れて学習させれば、次第に適切な重みが出力されるようになり、適切な出力が得られるようになるだろう。そうだといいな、ってだけ。\n",
    "\n",
    "全ての隠れ状態を参照した固定長のベクトルを得るだけであれば、単に全ての隠れ状態を足すだけでもいい。ただ、重みを変えられるような枠組みを取り入れてあげれば学習が上手くいくんじゃね？ってだけ。そして本当にうまくいったからここで紹介されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、重みを求める関数が内積でないといけない理由はない。2つのベクトルからスカラーを得る関数であれば何でもよい。  \n",
    "内積は類似度を測ることができ、類似度が高いものに着目するという意味では適切に見えるが、そもそもデータ空間（？）が異なるので、それらの類似度は意味を持たない。  \n",
    "重みを求める関数を内積として学習を進めれば、重要度が高くなるべきタイミングでその2つのベクトルが類似するように学習される、というだけ。\n",
    "\n",
    "ただ実際はほとんどの場合で内積が使われる。それは内積という計算がシンプルだからってだけ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention層\n",
    "\n",
    "decoderの中の、attentionによって都合のいい隠れ状態を出力する部分は1つの層として見られる。  \n",
    "複数時間の入力を考慮して以下のように表す。\n",
    "\n",
    "$$\n",
    "\\text{Attention}(X,M) = \\text{softmax}(XM^T)M\n",
    "$$\n",
    "\n",
    "- $X\\in\\R^{n_i\\times d}$ : 入力\n",
    "- $M\\in\\R^{n_m\\times d}$ : memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def forward(self, x, hs):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len_dec, hidden_size)\n",
    "        hs: (batch_size, seq_len_enc, hidden_size)\n",
    "        \"\"\"\n",
    "        scores = x @ hs.mT # (batch_size, seq_len_dec, seq_len_enc)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        h = weights @ hs # (batch_size, seq_len_dec, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_len_dec, seq_len_enc, hidden_size = 2, 3, 4, 5\n",
    "x = torch.randn(batch_size, seq_len_dec, hidden_size)\n",
    "hs = torch.randn(batch_size, seq_len_enc, hidden_size)\n",
    "\n",
    "attention = Attention()\n",
    "h = attention(x, hs)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## MASK\n",
    "\n",
    "padトークンがattentionの計算に含まれてしまうことを回避する。  \n",
    "maskをかけてpadトークンに対応する重みが0になるようにする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スコアに対して、対応する位置の値を$-\\infty$にする。そうすればsoftmaxを計算したときにその部分が0になる。\n",
    "\n",
    "$$\n",
    "\\mathrm{Attention}(\\boldsymbol x,M) = \\mathrm{softmax}(\\boldsymbol xM^T-\\infty) M\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こんなスコアがあったとする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7732,  1.5053, -0.1447, -1.7741, -1.2302])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.randn(5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後ろの2つがpadトークンだったとすると、こんな感じでmaskをかけてやればいい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7732,  1.5053, -0.1447,    -inf,    -inf])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = [False, False, False, True, True]\n",
    "scores[mask] = -torch.inf\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こう書いてもいい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2346, -0.7934,  1.1168,    -inf,    -inf])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.randn(5)\n",
    "mask = torch.tensor([0, 0, 0, 1, 1])\n",
    "scores.masked_fill_(mask, -torch.inf)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後はこれをsoftmaxに通す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1840, 0.1052, 0.7108, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = F.softmax(scores, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。これでpadトークンが無視されるようになる。\n",
    "\n",
    "層としても実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def forward(self, x, hs, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len_dec, hidden_size)\n",
    "        hs: (batch_size, seq_len_enc, hidden_size)\n",
    "        mask: (batch_size, seq_len_enc), bool, padトークンの位置\n",
    "        \"\"\"\n",
    "        scores = x @ hs.mT # (batch_size, seq_len_dec, seq_len_enc)\n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask.unsqueeze(1), -torch.inf) # maskを適用\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        h = weights @ hs # (batch_size, seq_len_dec, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Attentionを用いたSeq2Seq\n",
    "\n",
    "seq2seqにattention層を取り入れて翻訳モデルを作ってみよう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        eos_positions = x == eos_id # (batch_size, seq_len)\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        hs, _ = self.lstm(x) # (batch_size, seq_len, hidden_size)\n",
    "        h = hs[eos_positions].unsqueeze(0) # (1, batch_size, hidden_size)\n",
    "        return hs, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "前章のものにattention層を追加する。attention層の前後は残差結合で繋ぐ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.attention = Attention()\n",
    "        self.fc = nn.Linear(hidden_size * 2, n_vocab)\n",
    "\n",
    "    def forward(self, x, hs_enc, hc, mask=None):\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        hs, hc = self.lstm(x, hc)\n",
    "            # hs: (batch_size, seq_len, hidden_size)\n",
    "            # h: (1, batch_size, hidden_size)\n",
    "            # c: (1, batch_size, hidden_size)\n",
    "        z = self.attention(hs, hs_enc, mask) # (batch_size, seq_len, hidden_size)\n",
    "        z = torch.cat([z, hs], dim=-1) # skip connection\n",
    "            # (batch_size, seq_len, hidden_size * 2)\n",
    "        y = self.fc(z) # (batch_size, seq_len, n_vocab)\n",
    "        return y, hc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "全ての隠れ状態とpadトークンの位置をdecoderに渡すようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        hs, h = self.encoder(x_enc)\n",
    "        mask = x_enc == pad_id\n",
    "        hc = (h, torch.zeros_like(h))\n",
    "        y, _ = self.decoder(x_dec, hs, hc, mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "def loss_fn(y, t):\n",
    "    loss = cross_entropy(y.reshape(-1, n_vocab_ja), t.ravel())\n",
    "    return loss\n",
    "\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x_enc, x_dec, y_dec in test_loader:\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            losses.append(loss.item())\n",
    "    loss = sum(losses) / len(losses)\n",
    "    return loss\n",
    "\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    prog.start(n_iter=len(train_loader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for x_enc, x_dec, y_dec in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prog.update(loss.item())\n",
    "\n",
    "        if prog.now_epoch % prog_unit == 0:\n",
    "            test_loss = eval_model(model)\n",
    "            prog.memo(f'test: {test_loss:.5f}', no_step=True)\n",
    "        prog.memo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size, embed_size = 512, 512\n",
    "encoder = Encoder(n_vocab_ja, embed_size, hidden_size)\n",
    "decoder = Decoder(n_vocab_en, embed_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1-20/200: ############################## 100% [00:02:18.57] loss train: 4.39482, test: 5.05719 \n",
      "  21-40/200: ############################## 100% [00:02:16.78] loss train: 2.38485, test: 5.83749 \n",
      "  41-60/200: ############################## 100% [00:02:20.04] loss train: 1.15364, test: 6.90943 \n",
      "  61-80/200: ############################## 100% [00:02:19.90] loss train: 0.48066, test: 8.01863 \n",
      " 81-100/200: ############################## 100% [00:02:18.72] loss train: 0.17752, test: 9.01301 \n",
      "101-120/200: ############################## 100% [00:02:18.66] loss train: 0.07526, test: 9.78767 \n",
      "121-140/200: ############################## 100% [00:02:18.80] loss train: 0.04308, test: 10.07249 \n",
      "141-160/200: ############################## 100% [00:02:15.35] loss train: 0.03062, test: 10.54114 \n",
      "161-180/200: ############################## 100% [00:02:14.11] loss train: 0.02107, test: 10.89103 \n",
      "181-200/200: ############################## 100% [00:02:13.86] loss train: 0.01707, test: 11.14737 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, n_epochs=200, prog_unit=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常のseq2seqよりも訓練データの誤差が小さくなった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 翻訳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_sampling(y, decisive=True):\n",
    "    y.squeeze_(0)\n",
    "    if decisive:\n",
    "        token = y.argmax().item()\n",
    "    else:\n",
    "        y[unk_id] = -torch.inf\n",
    "        probs = F.softmax(y, dim=-1)\n",
    "        token, = random.choices(range(n_vocab_en), weights=probs)\n",
    "    return token\n",
    "\n",
    "\n",
    "bos_id = sp_en.bos_id()\n",
    "eos_id = sp_en.eos_id()\n",
    "@torch.no_grad()\n",
    "def translate(\n",
    "    model: nn.Module,\n",
    "    in_text: str,\n",
    "    max_len: int = 100,\n",
    "    decisive: bool = True,\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    in_ids = sp_ja.encode(in_text)\n",
    "    in_ids = torch.tensor([in_ids + [eos_id]], device=device)\n",
    "\n",
    "    hs, h = model.encoder(in_ids)\n",
    "    hc = (h, torch.zeros_like(h))\n",
    "    next_token = bos_id\n",
    "\n",
    "    token_ids = []\n",
    "    while len(token_ids) < max_len and next_token != eos_id:\n",
    "        x = torch.tensor([[next_token]], device=device)\n",
    "        y, hc = model.decoder(x, hs, hc)\n",
    "        next_token = token_sampling(y, decisive)\n",
    "        token_ids.append(next_token)\n",
    "\n",
    "    sentence = sp_en.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 彼にセンスのない詩を書いたり セーターを編んだりしました\n",
      "output: And I was writing terrible poetry and knitting sweaters for him.\n",
      "answer: And I was writing terrible poetry and knitting sweaters for him.\n",
      "\n",
      "input: その後 2分間に 3人が2才のワン・ユーの側を通り過ぎます\n",
      "output: Within two minutes, three people pass two-year-old Wang Yue by.\n",
      "answer: Within two minutes, three people pass two-year-old Wang Yue by.\n",
      "\n",
      "input: もう1つはNoksha-Yug Acceesで、 農村地域の物流を統合することを目的に マイクロファイナンスに基づく自助組織財団から基金を受けました。\n",
      "output: And the other is Moksha-Yug Access, which is integrating rural supply chain on the foundations of self-help group-based microfinance.\n",
      "answer: And the other is Moksha-Yug Access, which is integrating rural supply chain on the foundations of self-help group-based microfinance.\n",
      "\n",
      "input: この小さな機器で何でもします\n",
      "output: I do so many things on this little device.\n",
      "answer: I do so many things on this little device.\n",
      "\n",
      "input: 我々は暴力には力で対峙し 混沌には混沌で対峙しました\n",
      "output: We met violence with force and chaos with chaos.\n",
      "answer: We met violence with force and chaos with chaos.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(train_dataset))\n",
    "    x, _, t = train_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print('input:', x)\n",
    "    print('output:', translate(model, x))\n",
    "    print('answer:', t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: しかし 私の実務経験から言うと 民間団体がこれを正しく実行し 他の団体と一緒に行動すると 特に政府や 国際機関 それらは大きな国際的団体と協力し それらの団体が 社会的責任を果たせるようになるのです その時 この魔法の三角形 すなわち民間団体と 政府と企業による三角形が より良い世界を作るために 私達すべてに大きなチャンスをもたらすのです\n",
      "output: And if, there was a look, there's a ly Cord and alle that we had inside.\n",
      "answer: But what I'm saying from my very practical experience: If civil society does it right and joins the other actors -- in particular, governments, governments and their international institutions, but also large international actors, in particular those which have committed themselves to corporate social responsibility -- then in this magical triangle between civil society, government and private sector, there is a tremendous chance for all of us to create a better world.\n",
      "\n",
      "input: 地球全体としての生命を考えてみましょう ある意味 地球全体で生命ですし\n",
      "output: It's been more or than a third system that have been a part of our species.\n",
      "answer: Let's think of life as that entire planet because, in a sense, it is.\n",
      "\n",
      "input: この会社のドアをくぐったとき 私はついにカミングアウトする―\n",
      "output: He's a young right now there, on the surface of our highs.\n",
      "answer: When I walk through the doors of this company, I will finally come out.\n",
      "\n",
      "input: 冷蔵庫も手に入れ\n",
      "output: I didn't want it.\n",
      "answer: It's the refrigerators.\n",
      "\n",
      "input: 脳内出血を起こしていたようですが 小さなステントと呼ばれる血管を狭める 金属製の留め具で治療されています\n",
      "output: You're like -- you can't even be one of the things that you're used to -- in it's good to say -- the first one you can be used to, but you're in a  ⁇ ware to even with one of the first studies came out of the planet.\n",
      "answer: She had a bleeding up in the brain, and that's been fixed with a little stent, a metal clamp that's tightening up the vessel.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(test_dataset))\n",
    "    x, _, t = test_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print('input:', x)\n",
    "    print('output:', translate(model, x))\n",
    "    print('answer:', t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ありがとう。\n",
      "output: Thank you.\n",
      "\n",
      "input: 猫はかわいいね。\n",
      "output: I'm here to do a magic.\n",
      "\n",
      "input: 上手く文章が書けるようになりました。\n",
      "output: I wanted to race that. I wanted to look at this time, and I was now going to make it all the first time in the  ⁇ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "sentences = [\n",
    "    'ありがとう。',\n",
    "    '猫はかわいいね。',\n",
    "    '上手く文章が書けるようになりました。'\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print('input:', sentence)\n",
    "    print('output:', translate(model, sentence))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "びみょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Attentionの可視化\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
