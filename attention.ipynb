{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import (\n",
    "    pad_sequence,\n",
    "    pack_padded_sequence,\n",
    "    pad_packed_sequence,\n",
    ")\n",
    "from dlprog import train_progress\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress(\n",
    "    width=20,\n",
    "    with_test=True,\n",
    "    label=\"ppl train\",\n",
    "    round=2,\n",
    "    agg_fn=lambda s, w: math.exp(s / w)\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 223108\n",
      "num of vocabrary (ja): 8000\n",
      "num of vocabrary (en): 8000\n"
     ]
    }
   ],
   "source": [
    "textfile_ja = \"data/iwslt2017_ja.txt\"\n",
    "textfile_en = \"data/iwslt2017_en.txt\"\n",
    "tokenizer_prefix_ja = f\"models/tokenizer_iwslt2017_ja\"\n",
    "tokenizer_prefix_en = f\"models/tokenizer_iwslt2017_en\"\n",
    "\n",
    "with open(textfile_en) as f:\n",
    "    data_en = f.read().splitlines()\n",
    "\n",
    "with open(textfile_ja) as f:\n",
    "    data_ja = f.read().splitlines()\n",
    "\n",
    "n_data = len(data_en)\n",
    "print(\"num of data:\", n_data)\n",
    "\n",
    "sp_ja = spm.SentencePieceProcessor(f\"{tokenizer_prefix_ja}.model\")\n",
    "sp_en = spm.SentencePieceProcessor(f\"{tokenizer_prefix_en}.model\")\n",
    "unk_id = sp_ja.unk_id()\n",
    "bos_id = sp_ja.bos_id()\n",
    "eos_id = sp_ja.eos_id()\n",
    "pad_id = sp_ja.pad_id()\n",
    "n_vocab_ja = len(sp_ja)\n",
    "n_vocab_en = len(sp_en)\n",
    "print(\"num of vocabrary (ja):\", n_vocab_ja)\n",
    "print(\"num of vocabrary (en):\", n_vocab_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids_ja = sp_ja.encode(data_ja)\n",
    "data_ids_en = sp_en.encode(data_en)\n",
    "\n",
    "for ids_ja, ids_en in zip(data_ids_ja, data_ids_en):\n",
    "    ids_ja.append(eos_id)\n",
    "    ids_en.insert(0, bos_id)\n",
    "    ids_en.append(eos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train data: 178487\n",
      "num of test data: 44621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 52]), torch.Size([32, 52]), torch.Size([32, 52]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids_ja, data_ids_en):\n",
    "        self.data_ja = [torch.tensor(ids) for ids in data_ids_ja]\n",
    "        self.data_en = [torch.tensor(ids) for ids in data_ids_en]\n",
    "        self.n_data = len(self.data_ja)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ja = self.data_ja[idx]\n",
    "        en = self.data_en[idx]\n",
    "        x_enc = ja\n",
    "        x_dec = en[:-1]\n",
    "        y_dec = en[1:]\n",
    "        return x_enc, x_dec, y_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x_enc, x_dec, y_dec= zip(*batch)\n",
    "    x_enc = pad_sequence(x_enc, batch_first=True, padding_value=pad_id)\n",
    "    x_dec = pad_sequence(x_dec, batch_first=True, padding_value=pad_id)\n",
    "    y_dec = pad_sequence(y_dec, batch_first=True, padding_value=pad_id)\n",
    "    return x_enc, x_dec, y_dec\n",
    "\n",
    "dataset = TextDataset(data_ids_ja, data_ids_en)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "print(\"num of train data:\", len(train_dataset))\n",
    "print(\"num of test data:\", len(test_dataset))\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "x_enc, x_dec, y_dec = next(iter(train_loader))\n",
    "x_enc.shape, x_dec.shape, y_dec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Attention機構\n",
    "\n",
    "複数のデータの中から重要なデータに着目する仕組み。Attention = 注意、注目、着目。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ある一つの入力と、関連する複数のデータを考える。関連する複数のデータはmemoryと呼ぶ。入力を元に、memoryの中のどのデータに着目するかを定めることがAttentionの目的である。各データに重要度を割り当てるという感じ。\n",
    "\n",
    "重要度は重みと呼ばれ、$w_i$で表すことにする。重みは総和が1になるようにsoftmaxなどで正規化する。正規化前の値はスコアと呼んだりする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各memoryに対応する重みは入力との内積で求める。別に内積じゃなくてもいいけど、内積が一番簡単だし性能も良い。内積が取れるように、memoryの各ベクトルは入力と同じ次元にする必要がある。\n",
    "\n",
    "重みを求めた後は、その重みでmemoryの重み付き和をとる。そうすることで、memoryの中から重要な要素を多めに取り出した固定長のベクトルが得られる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "やってみよう。まず、入力と、三つのデータからなるmemoryを用意する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7003,  0.3728,  0.9630, -0.7548,  1.4186],\n",
       "        [-0.3834,  0.7128, -0.5010, -0.5049,  0.4977],\n",
       "        [-0.0481,  0.5967,  2.3803,  0.1163,  0.1539]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, d = 3, 5\n",
    "\n",
    "x = torch.randn(d)\n",
    "memory = torch.randn(n, d)\n",
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力とmemory内の全てのデータで内積を取る。これがスコアに当たる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3264, 0.1925, 0.4811])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.tensor([m @ x for m in memory])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでもいい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = x @ memory.T\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmaxで正規化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = F.softmax(scores, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この重みがmemoryの各データの重要度を表す。これで重み付き和をとる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2286,  0.1217,  0.3143, -0.2464,  0.4630],\n",
       "        [-0.0738,  0.1372, -0.0964, -0.0972,  0.0958],\n",
       "        [-0.0231,  0.2871,  1.1452,  0.0560,  0.0740]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重みをかける\n",
    "weighted_memory = torch.stack([w * m for w, m in zip(weights, memory)])\n",
    "weighted_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1316,  0.5460,  1.3631, -0.2876,  0.6329])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 和をとる\n",
    "y = weighted_memory.sum(dim=0)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のようにまとめられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1316,  0.5460,  1.3631, -0.2876,  0.6329])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = weights @ memory # 重み付き和\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上がattention機構の演算の流れである。まとめるとこう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1316,  0.5460,  1.3631, -0.2876,  0.6329])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = memory @ x # スコア\n",
    "weights = F.softmax(scores, dim=-1) # 重み\n",
    "y = weights @ memory # 重み付き和\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数式だとこうなる。\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\boldsymbol x,M) = \\text{softmax}(\\boldsymbol xM^T)M\n",
    "$$\n",
    "\n",
    "- $\\boldsymbol x\\in\\mathbb R^{d}$ : 入力\n",
    "- $M\\in\\mathbb R^{n\\times d}$ : memory\n",
    "\n",
    "\\*列ベクトルと行ベクトルを区別していないので厳密ではない。厳密に書くならこう:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\boldsymbol x,M) = (\\text{softmax}(\\boldsymbol x^TM^T)M)^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Attentionを用いたSeq2Seq\n",
    "\n",
    "Seq2SeqにAttentionを導入し、Encoderが出力した全ての隠れ状態をDecoderから参照する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecoderにAttentionを取り入れる。入力は前の層からの出力で、memoryはEncoderが出力した全ての隠れ状態である。こうすることで、Decoderは各時刻でその時着目すべき情報に着目した演算が行える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4589,  0.3038,  1.8257, -0.6385,  0.2316])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 3\n",
    "hidden_size = 5\n",
    "hs_enc = torch.randn(seq_len, hidden_size) # encoderが出力した全ての隠れ状態\n",
    "h_dec = torch.randn(hidden_size) # ある時間tのdecoderの隠れ状態\n",
    "\n",
    "scores = h_dec @ hs_enc.T # (seq_len,)\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "y = weights @ hs_enc # (hidden_size,)\n",
    "y # encoderの全ての隠れ状態から重要な部分を多く抜き出したベクトル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに、重みが正しく着目すべき点を表すかは、学習させてみないと分からない。この目的も、学習前の段階では期待に過ぎない。この仕組みを取り入れて学習させれば、次第に適切な重みが出力されるようになり、適切な出力が得られるようになるだろう。そうだといいな、ってだけ。\n",
    "\n",
    "全ての隠れ状態を参照した固定長のベクトルを得るだけであれば、単に全ての隠れ状態を足すだけでもいい。ただ、重みを変えられるような枠組みを取り入れてあげれば学習が上手くいくんじゃね？ってだけ。そして本当にうまくいったからここで紹介されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、重みを求める関数が内積でないといけない理由はない。2つのベクトルからスカラーを得る関数であれば何でもよい。\n",
    "\n",
    "内積は類似度を測ることができ、類似度が高いものに着目するという意味では適切に見えるが、そもそも比較するベクトルはいくつかの層を経て複雑に変化するため、それらの類似度は意味を持たない。重みを求める関数を内積として学習を進めれば、重要度が高くなるべきタイミングでその2つのベクトルが類似するように学習される、というだけ。\n",
    "\n",
    "ただ実際はほとんどの場合で内積が使われる。それは内積という計算がシンプルだからってだけ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention層\n",
    "\n",
    "Decoderの中の、Attentionによって都合のいい隠れ状態を出力する部分は一つの層として見られる。複数時刻の入力を考慮して以下のように表す。\n",
    "\n",
    "$$\n",
    "\\text{Attention}(X,M) = \\text{softmax}(XM^T)M\n",
    "$$\n",
    "\n",
    "- $X\\in\\mathbb R^{n_i\\times d}$ : 層への入力\n",
    "- $M\\in\\mathbb R^{n_m\\times d}$ : memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def forward(self, x, hs):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len_dec, hidden_size)\n",
    "        hs: (batch_size, seq_len_enc, hidden_size)\n",
    "        \"\"\"\n",
    "        scores = x @ hs.mT # (batch_size, seq_len_dec, seq_len_enc)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        h = weights @ hs # (batch_size, seq_len_dec, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_len_dec, seq_len_enc, hidden_size = 2, 3, 4, 5\n",
    "x = torch.randn(batch_size, seq_len_dec, hidden_size)\n",
    "hs = torch.randn(batch_size, seq_len_enc, hidden_size)\n",
    "\n",
    "attention = Attention()\n",
    "h = attention(x, hs)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Attention\n",
    "\n",
    "padトークンがAttentionの計算に含まれてしまうことを回避する。maskをかけてpadトークンに対応する重みが0になるようにする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スコアに対して、対応する位置の値を$-\\infty$にする。そうすればsoftmaxを計算したときにその部分が0になる。\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\boldsymbol x,M) = \\text{softmax}(\\boldsymbol xM^T-\\infty\\,\\text{mask}) M\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こんなスコアがあったとする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7205, -0.7592,  1.8133, -0.2011, -0.0768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.randn(5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後ろの2つがpadトークンだったとすると、こんな感じでmaskをかけてやればいい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7205, -0.7592,  1.8133,    -inf,    -inf])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = [False, False, False, True, True]\n",
    "scores[mask] = -torch.inf\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こう書いてもいい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2911, -1.3570, -0.8464,    -inf,    -inf])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.randn(5)\n",
    "mask = torch.tensor([0, 0, 0, 1, 1])\n",
    "scores.masked_fill_(mask, -torch.inf)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後はこれをsoftmaxに通す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6609, 0.1272, 0.2119, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = F.softmax(scores, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。これでpadトークンが無視されるようになる。\n",
    "\n",
    "層としても実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def forward(self, x, hs, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len_dec, hidden_size)\n",
    "        hs: (batch_size, seq_len_enc, hidden_size)\n",
    "        mask: (batch_size, seq_len_enc), bool, padトークンの位置\n",
    "        \"\"\"\n",
    "        scores = x @ hs.mT # (batch_size, seq_len_dec, seq_len_enc)\n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask.unsqueeze(1), -torch.inf) # maskを適用\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        h = weights @ hs # (batch_size, seq_len_dec, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 実践\n",
    "\n",
    "実際にAttentionをSeqSeqに取り入れて翻訳モデルを学習させてみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル構築\n",
    "\n",
    "前章のSeq2Seqをベースとする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずEncoder。全ての時刻の隠れ状態を出力する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x_pack = pack_padded_sequence(\n",
    "            x, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        hs, (h, _) = self.lstm(x_pack)\n",
    "        hs, _ = pad_packed_sequence(hs, batch_first=True)\n",
    "        return hs, h\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm1 = PackedLSTM(embed_size, hidden_size // 2, True)\n",
    "        self.lstm2 = PackedLSTM(hidden_size, hidden_size, False)\n",
    "        self.lstm3 = PackedLSTM(hidden_size, hidden_size, False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lengths = (x != pad_id).sum(dim=1).cpu()\n",
    "\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "\n",
    "        hs, _ = self.lstm1(x, lengths) # (batch_size, seq_len, hidden_size)\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        skip = hs\n",
    "        hs, _ = self.lstm2(hs, lengths)\n",
    "        hs = hs + skip\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        skip = hs\n",
    "        hs, _ = self.lstm3(hs, lengths)\n",
    "        hs = hs + skip\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        return hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にDecoder。LSTMの後にAttention層を追加する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm1 = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.attention = Attention()\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hs_enc, hc=None, mask=None):\n",
    "        hc1, hc2, hc3 = hc or (None, None, None)\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "\n",
    "        hs, hc1 = self.lstm1(x, hc1) # (batch_size, seq_len, hidden_size)\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        skip = hs\n",
    "        hs, hc2 = self.lstm2(hs, hc2)\n",
    "        hs = self.attention(hs, hs_enc, mask)\n",
    "        hs = hs + skip\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        skip = hs\n",
    "        hs, hc3 = self.lstm3(hs)\n",
    "        hs = self.attention(hs, hs_enc, mask)\n",
    "        hs = hs + skip\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        y = self.fc(hs) # (batch_size, seq_len, n_vocab)\n",
    "        return y, (hc1, hc2, hc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、これらをまとめる。\n",
    "\n",
    "全ての隠れ状態とpadトークンの位置をDecoderに渡すようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        hs = self.encoder(x_enc)\n",
    "        mask = x_enc == pad_id # (batch_size, seq_len_enc)\n",
    "        y, _ = self.decoder(x_dec, hs, mask=mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of parameters: 24,379,200\n"
     ]
    }
   ],
   "source": [
    "hidden_size, embed_size = 512, 512\n",
    "encoder = Encoder(n_vocab_ja, embed_size, hidden_size)\n",
    "decoder = Decoder(n_vocab_en, embed_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "model_path = \"models/lm_seq2seq_attn.pth\"\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"num of parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention層はパラメータがないのでパラメータ数は変わらない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前章では最後の隠れ状態をLSTMの初期値として渡したが、今回は全ての隠れ状態をAttention層に渡す。LSTMの初期値は0ベクトル。LSTMの初期値として最後の隠れ状態も渡すという手もあるが、今回はしないでおく。渡した方が精度上がったりするんかな。でも試すのめんどいからいいや。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "def loss_fn(y, t):\n",
    "    loss = cross_entropy(y.reshape(-1, n_vocab_ja), t.ravel())\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for x_enc, x_dec, y_dec in test_loader:\n",
    "        x_enc = x_enc.to(device)\n",
    "        x_dec = x_dec.to(device)\n",
    "        y_dec = y_dec.to(device)\n",
    "\n",
    "        y = model(x_enc, x_dec)\n",
    "        loss = loss_fn(y, y_dec)\n",
    "        losses.append(loss.item())\n",
    "    loss = sum(losses) / len(losses)\n",
    "    ppl = math.exp(loss)\n",
    "    return ppl\n",
    "\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    prog.start(n_iter=len(train_loader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for x_enc, x_dec, y_dec in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prog.update(loss.item())\n",
    "\n",
    "        if prog.now_epoch % prog_unit == 0:\n",
    "            test_ppl = eval_model(model)\n",
    "            prog.memo(f\"test: {test_ppl:.2f}\", no_step=True)\n",
    "        prog.memo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/20: #################### 100% [00:05:37.37] ppl train: 163.44, test: 96.76 \n",
      " 2/20: #################### 100% [00:05:27.32] ppl train: 87.08, test: 67.08 \n",
      " 3/20: #################### 100% [00:05:32.71] ppl train: 63.37, test: 51.04 \n",
      " 4/20: #################### 100% [00:05:29.29] ppl train: 50.02, test: 42.23 \n",
      " 5/20: #################### 100% [00:05:32.49] ppl train: 41.81, test: 36.87 \n",
      " 6/20: #################### 100% [00:05:35.21] ppl train: 36.39, test: 33.28 \n",
      " 7/20: #################### 100% [00:05:34.07] ppl train: 32.59, test: 30.86 \n",
      " 8/20: #################### 100% [00:05:33.51] ppl train: 29.71, test: 29.05 \n",
      " 9/20: #################### 100% [00:05:35.21] ppl train: 27.48, test: 27.66 \n",
      "10/20: #################### 100% [00:05:33.87] ppl train: 25.66, test: 26.57 \n",
      "11/20: #################### 100% [00:05:33.15] ppl train: 24.18, test: 25.72 \n",
      "12/20: #################### 100% [00:05:33.86] ppl train: 22.93, test: 25.04 \n",
      "13/20: #################### 100% [00:05:34.59] ppl train: 21.82, test: 24.44 \n",
      "14/20: #################### 100% [00:05:31.35] ppl train: 20.90, test: 23.97 \n",
      "15/20: #################### 100% [00:05:37.75] ppl train: 20.05, test: 23.66 \n",
      "16/20: #################### 100% [00:05:36.19] ppl train: 19.32, test: 23.34 \n",
      "17/20: #################### 100% [00:05:39.62] ppl train: 18.66, test: 23.03 \n",
      "18/20: #################### 100% [00:05:41.39] ppl train: 18.07, test: 22.88 \n",
      "19/20: #################### 100% [00:05:41.56] ppl train: 17.52, test: 22.72 \n",
      "20/20: #################### 100% [00:05:42.04] ppl train: 17.03, test: 22.54 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, n_epochs=20, prog_unit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attentionなしの時は20epochでtrain pplが25、test pplが36だったので、精度が上がっていることが期待される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 翻訳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self):\n",
    "        self.sentence = []\n",
    "        self.hc = None\n",
    "        self.ll = 0.0\n",
    "        self.norm_ll = 0.0\n",
    "        self.finished = False\n",
    "        self.next_token = None\n",
    "\n",
    "    def update(self, token_id, log_prob):\n",
    "        self.append(token_id)\n",
    "        self.ll += log_prob\n",
    "        self.norm_ll = self.ll / len(self.sentence)\n",
    "        self.next_token = token_id\n",
    "\n",
    "    def append(self, token_id):\n",
    "        if token_id == eos_id:\n",
    "            self.finished = True\n",
    "        self.sentence.append(token_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentence)\n",
    "\n",
    "\n",
    "bos_id = sp_en.bos_id()\n",
    "eos_id = sp_en.eos_id()\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate(\n",
    "    model: nn.Module,\n",
    "    in_text: str, # 入力文（日本語）\n",
    "    lim_len: int = 100, # 出力のトークン数の上限\n",
    "    k: int = 4, # ビーム幅\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    in_ids = sp_ja.encode(in_text)\n",
    "    in_ids = torch.tensor(in_ids + [eos_id], device=device).unsqueeze(0)\n",
    "\n",
    "    hs = model.encoder(in_ids)\n",
    "\n",
    "    first_sentence = Sentence()\n",
    "    first_sentence.next_token = bos_id\n",
    "    sentences = [first_sentence]\n",
    "    max_len = 1\n",
    "\n",
    "    while not all(s.finished for s in sentences) and max_len < lim_len:\n",
    "        new_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if sentence.finished:\n",
    "                new_sentences.append(sentence)\n",
    "                continue\n",
    "            x = torch.tensor([[sentence.next_token]], device=device)\n",
    "            y, hc = model.decoder(x, hs, sentence.hc)\n",
    "            y = F.log_softmax(y, dim=-1).squeeze(0, 1)\n",
    "            sentence.hc = hc\n",
    "            for token_id in y.topk(k).indices.tolist():\n",
    "                new_sentence = copy.deepcopy(sentence)\n",
    "                new_sentence.update(token_id, y[token_id].item())\n",
    "                new_sentences.append(new_sentence)\n",
    "        sentences = sorted(new_sentences, key=lambda s: s.norm_ll, reverse=True)\n",
    "        sentences = sentences[:k]\n",
    "        max_len = max(map(len, sentences))\n",
    "\n",
    "    best_sentence = max(sentences, key=lambda s: s.norm_ll).sentence\n",
    "    return sp_en.decode(best_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 犬の散歩\n",
      "output: It's a dog.\n",
      "correct: Walk the dog.\n",
      "\n",
      "input: だから先手を打って 大金を小金に変える一番早い方法を— 探していたんだと 答えることにしています\n",
      "output: So I'm trying to figure out how to do it, and I'm trying to figure out how to make a small amount of money, and I'm trying to figure out what's going on.\n",
      "correct: And so I tell people, well, I was trying to figure out the fastest way to turn a large fortune into a small one.\n",
      "\n",
      "input: 寿司作りからプログラミングまで あらゆる講座を Skillshareで受けることができます DogVacayを使えば ペットまでも共有できます\n",
      "output: We're going to share with you all the things that we're going to do with a lot of these things, and we're going to be able to share the Skillows, and I can share with you.\n",
      "correct: We are trading lessons on everything from sushi-making to coding on Skillshare, and we're even sharing our pets on DogVacay.\n",
      "\n",
      "input: 私たちが恩恵を受けてきた時のように 図書館と出版社の両方が共存する世界をどうやって作ればいいのか?\n",
      "output: How do we make a world of libraries and publishing the world together?\n",
      "correct: How do we go and have a world where we both have libraries and publishing in the future, just as we basically benefited as we were growing up?\n",
      "\n",
      "input: もし あの月が夢を見ることができたら 月の見る夢は私たちにとっての夢になるでしょう\n",
      "output: And if that month dream, the dream is dreaming of the dream, the dream of the moon is for us to be dreaming for us.\n",
      "correct: If the Moon could dream, I think that would be its dream for us.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(train_dataset))\n",
    "    x, _, t = train_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print(\"input:\", x)\n",
    "    print(\"output:\", translate(model, x))\n",
    "    print(\"correct:\", t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: より幸せになるために毎日すべき 5つのこととは何でしょう?\n",
      "output: What are five things to be happier?\n",
      "correct: What are the five things that you should do every day to be happier?\n",
      "\n",
      "input: この機械は その場で音を作り出すマシンです リアルタイム音楽製造機です この機械を使えば 私の声だけを使い 頭の中に聞こえたままの音楽を リアルタイムで 肉体から生じる制約に 邪魔されることなく 作り出すことができます\n",
      "output: This is a machine that's going to be able to use the sound of the real-time machine, because it's a real-time machine, and I can't be able to use this machine in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time\n",
      "correct: We've made a system which is basically a live production machine, a real-time music production machine, and it enables me to, using nothing but my voice, create music in real time as I hear it in my head unimpeded by any physical restrictions that my body might place on me.\n",
      "\n",
      "input: 世界は2グループに 分かれていました\n",
      "output: The world was divided in two groups.\n",
      "correct: The world was two groups.\n",
      "\n",
      "input: そして私は自分のブログを少しずつ閉じていきました\n",
      "output: And I've got a little bit of my blog.\n",
      "correct: And I started to kill my blog slowly.\n",
      "\n",
      "input: 二階に2つ目のトイレを作るのもいけません\n",
      "output: We need to make the second two-story toilet.\n",
      "correct: Then, another bathroom upstairs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(test_dataset))\n",
    "    x, _, t = test_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print(\"input:\", x)\n",
    "    print(\"output:\", translate(model, x))\n",
    "    print(\"correct:\", t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ありがとう\n",
      "output: Thank you.\n",
      "\n",
      "input: 私はかわいい猫を飼っています。\n",
      "output: I'm a cute cat.\n",
      "\n",
      "input: 上手く文章が書けるようになりました\n",
      "output: It's got to write a paper.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "sentences = [\n",
    "    \"ありがとう\",\n",
    "    \"私はかわいい猫を飼っています。\",\n",
    "    \"上手く文章が書けるようになりました\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"input:\", sentence)\n",
    "    print(\"output:\", translate(model, sentence))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "少しマシになったかも。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Attentionの可視化\n",
    "\n",
    "学習させたモデルのAttention層がどのように動いているかを可視化して確かめてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attentionはデータの一部分に着目する枠組みを提供する。先で学習させたSeq2Seqは、これをDecoderに取り入れ、Encoderから出力された隠れ状態の特定の位置に着目する。本節では、推論時のAttention層の重みを見て、学習させたモデルがどのデータに着目して推論を行っているかを確かめる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず適当な入力を用意する。「この映画は面白そうです。」という文章を翻訳させてみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  20,  626,    7, 6223,  612,   30,    2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ja = \"この映画は面白そうです。\"\n",
    "tokens_ja = sp_ja.encode(text_ja) + [eos_id]\n",
    "x_enc = torch.tensor(tokens_ja).unsqueeze(0)\n",
    "x_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これをEncoderに与え、隠れ状態を得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_enc = model.encoder(x_enc)\n",
    "hs_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に適当な分脈と隠れ状態とDecoderに与え、次の単語を予測させる。「This movie looks」と与えてみよう。「interesting」が予測されて欲しい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  101, 1394,  620]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_en = \"This movie looks\"\n",
    "tokens_en = [bos_id] + sp_en.encode(text_en)\n",
    "x_dec = torch.tensor(tokens_en).unsqueeze(0)\n",
    "x_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, _ = model.decoder(x_dec, hs_enc)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後の時刻の出力から、確率の高い上位4つのトークンを取り出す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like funny interesting fascinating'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top4 = y.squeeze(0)[-1].topk(4)\n",
    "sp_en.decode(top4.indices.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちゃんと「interesting」が含まれている。\n",
    "\n",
    "さて、この時Decoderがどの隠れ状態に着目していたのかを見てみよう。初めのAttention層の重みを見る。まずAttention層への入力（LSTM層の出力）を得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = model.decoder.embedding(x_dec)\n",
    "hs_dec, _ = model.decoder.lstm1(embed)\n",
    "hs_dec, _ = model.decoder.lstm2(hs_dec)\n",
    "hs_dec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチ次元を削除し、最後の時刻の隠れ状態を取り出す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 512]), torch.Size([512]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_enc = hs_enc.squeeze(0)\n",
    "hs_dec = hs_dec.squeeze(0)\n",
    "h_dec = hs_dec[-1]\n",
    "hs_enc.shape, h_dec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内積をとって重みを見てみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0997, 0.0462, 0.0632, 0.3655, 0.2366, 0.0523, 0.1366],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = hs_enc @ h_dec\n",
    "weights = F.softmax(score, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力文は以下のようにトークン化されているので"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁この', '映画', 'は', '面白', 'そうです', '。']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_ja.encode(text_ja, out_type=str) # + EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一番確率の高い隠れ状態と対応するトークンは「面白」であることが分かる。\n",
    "\n",
    "「この映画は面白そうです。」という文章を翻訳する際、「This movie looks」という文脈を与えると、Decoderは入力文の「面白」という部分に着目し、「interesting」という単語を予測していることが分かった。これは人間が翻訳を行う過程と一致しており、非常に直感に即した結果となっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "他の時刻での重みも見てみよう。翻訳文全体を与えた上で、各時刻での重みを見てみる。また、先ほどは一つ目のAttention層のみの重みを見たが、全てのAttention層の重みを見てみる。二つのAttention層の重みを平均する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_weights(model, text_ja, text_en):\n",
    "    tokens_ja = sp_ja.encode(text_ja) + [eos_id]\n",
    "    x_enc = torch.tensor(tokens_ja).unsqueeze(0)\n",
    "    hs_enc = model.encoder(x_enc)\n",
    "\n",
    "    tokens_en = [bos_id] + sp_en.encode(text_en)\n",
    "    x_dec = torch.tensor(tokens_en).unsqueeze(0)\n",
    "    embed = model.decoder.embedding(x_dec)\n",
    "    hs_dec, _ = model.decoder.lstm1(embed)\n",
    "    skip = hs_dec\n",
    "    hs_dec1, _ = model.decoder.lstm2(hs_dec)\n",
    "    hs_dec = model.decoder.attention(hs_dec1, hs_enc)\n",
    "    hs_dec = hs_dec + skip\n",
    "    hs_dec2, _ = model.decoder.lstm3(hs_dec)\n",
    "    hs_dec = (hs_dec1 + hs_dec2) / 2\n",
    "\n",
    "    hs_enc = hs_enc.squeeze(0) # (seq_len_enc, hidden_size)\n",
    "    hs_dec = hs_dec.squeeze(0) # (seq_len_dec, hidden_size)\n",
    "\n",
    "    score = hs_enc @ hs_dec.T\n",
    "    weights = F.softmax(score, dim=-1)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2316, 0.4319, 0.0967, 0.0858, 0.0658, 0.0881],\n",
       "        [0.0667, 0.8166, 0.0697, 0.0148, 0.0067, 0.0255],\n",
       "        [0.2040, 0.3915, 0.1623, 0.0693, 0.1284, 0.0444],\n",
       "        [0.0096, 0.0276, 0.0248, 0.8926, 0.0304, 0.0150],\n",
       "        [0.0553, 0.0593, 0.0879, 0.3104, 0.4408, 0.0462],\n",
       "        [0.1079, 0.1319, 0.1013, 0.0658, 0.3376, 0.2555],\n",
       "        [0.1022, 0.1180, 0.1533, 0.0586, 0.1344, 0.4336]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ja = \"この映画は面白そうです。\"\n",
    "text_en = \"This movie looks interesting.\"\n",
    "weights = get_attention_weights(model, text_ja, text_en)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_ja = sp_ja.encode(text_ja) + [eos_id]\n",
    "x_enc = torch.tensor(tokens_ja).unsqueeze(0)\n",
    "hs_enc = model.encoder(x_enc)\n",
    "hs_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoderが各時刻で入力文のどの位置にどの程度着目しているかを2次元のデータとして得られた。分かり易くするために、ヒートマップとして表してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids_en = sp_en.encode(text_en) + [eos_id]\n",
    "ids_ja = sp_ja.encode(text_ja) + [eos_id]\n",
    "ticks_x = list(map(lambda x: sp_en.id_to_piece(x).replace(\"▁\", \"\"), ids_en))\n",
    "ticks_y = list(map(lambda x: sp_ja.id_to_piece(x).replace(\"▁\", \"\"), ids_ja))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(weights.detach(), cmap=\"gray\")\n",
    "plt.xticks(range(len(ticks_x)), ticks_x, rotation=90, position=(0, 1.05), va=\"bottom\")\n",
    "plt.yticks(range(len(ticks_y)), ticks_y)\n",
    "plt.tick_params(axis='x', length=0)\n",
    "plt.tick_params(axis='y', length=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "横がDecoderの時間軸、縦がEncoderの時間軸。Decoderが各時刻でどのトークンに着目しているかが色で表されている。白いほど強く着目されている。\n",
    "\n",
    "例えば、左から二列目は、『movie』より前のトークン（BOS, This）を文脈として与えたときに『映画』に着目して推論を行っている、と解釈する。「『movie』を与えたとき」ではないので注意。『movie』を予測する際に『映画』に着目しているとも見られる。ただこの説明は少し不適切で、ここではモデルによる実際の翻訳結果を与えていないため、その単語が本当に予測されているかは分からない。『interesting』の場合は上位に予測されることが先の実験で確認されたが、それ以外のトークンについては不明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "『interesting』は『面白』、『movie』は『映画』と、トークン単位での翻訳結果と一致するように着目されていることが分かる。他にも、薄い色ではあるが、『This』と『この』、『.』と『。』、『EOS』と『EOS』といった対応も見られる。\n",
    "\n",
    "Attentionを導入することでモデルの解釈性が上がり、解釈結果がこのように我々の直感と即していると面白いね。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
