{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "複数のデータの中から重要なデータに注目する仕組み。  \n",
    "attentionを用いることで、decoderが、encoderが出力した情報の中の重要な情報に注目するようになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前章で作成した翻訳モデルは、入力文をencoderによって固定長のベクトルに変換し、それをdecoderに渡すことで、入力文に基づいた出力文を生成した。  \n",
    "encoderがRNNであるとき、encoderは各時間で隠れ状態を出力する。この中から最後の時間の隠れ状態のみをdecoderに渡していたものがこれまでのseq2seqである。\n",
    "\n",
    "このとき、encoderが出力する全ての隠れ状態を利用したいと考える。その方が入力文の多くの情報を参照でき、より適切な出力が得られそうだ。attentionはそれを実現する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress(with_test=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile_ja = 'data/kyoto_ja_10000.txt'\n",
    "textfile_en = 'data/kyoto_en_10000.txt'\n",
    "tokenizer_prefix_ja = 'models/tokenizer_kyoto_ja_10000'\n",
    "tokenizer_prefix_en = 'models/tokenizer_kyoto_en_10000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 10000\n",
      "num of vocabrary (ja): 8000\n",
      "num of vocabrary (en): 8000\n"
     ]
    }
   ],
   "source": [
    "with open(textfile_en) as f:\n",
    "    data_en = f.readlines()\n",
    "\n",
    "with open(textfile_ja) as f:\n",
    "    data_ja = f.readlines()\n",
    "\n",
    "n_data = len(data_en)\n",
    "print('num of data:', n_data)\n",
    "\n",
    "sp_ja = spm.SentencePieceProcessor(f'{tokenizer_prefix_ja}.model')\n",
    "sp_en = spm.SentencePieceProcessor(f'{tokenizer_prefix_en}.model')\n",
    "unk_id = sp_ja.unk_id()\n",
    "bos_id = sp_ja.bos_id()\n",
    "eos_id = sp_ja.eos_id()\n",
    "pad_id = sp_ja.pad_id()\n",
    "n_vocab_ja = len(sp_ja)\n",
    "n_vocab_en = len(sp_en)\n",
    "print('num of vocabrary (ja):', n_vocab_ja)\n",
    "print('num of vocabrary (en):', n_vocab_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids_ja = sp_ja.encode(data_ja)\n",
    "data_ids_en = sp_en.encode(data_en)\n",
    "\n",
    "for ids_ja, ids_en in zip(data_ids_ja, data_ids_en):\n",
    "    ids_en.insert(0, bos_id)\n",
    "    ids_ja.append(eos_id)\n",
    "    ids_en.append(eos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids_ja, data_ids_en):\n",
    "        self.data_ja = [torch.tensor(ids) for ids in data_ids_ja]\n",
    "        self.data_en = [torch.tensor(ids) for ids in data_ids_en]\n",
    "        self.n_data = len(self.data_ja)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ja = self.data_ja[idx]\n",
    "        en = self.data_en[idx]\n",
    "        x_enc = ja\n",
    "        x_dec = en[:-1]\n",
    "        y_dec = en[1:]\n",
    "        return x_enc, x_dec, y_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x_enc, x_dec, y_dec= zip(*batch)\n",
    "    x_enc = pad_sequence(x_enc, batch_first=True, padding_value=pad_id)\n",
    "    x_dec = pad_sequence(x_dec, batch_first=True, padding_value=pad_id)\n",
    "    y_dec = pad_sequence(y_dec, batch_first=True, padding_value=pad_id)\n",
    "    return x_enc, x_dec, y_dec\n",
    "\n",
    "dataset = TextDataset(data_ids_ja, data_ids_en)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Attention\n",
    "\n",
    "attention機構について詳しく見ていく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "といっても、そんなに難しいことはない。  \n",
    "複数のデータがあった時に、各データに重要度を割り当てるだけである。これは重みと呼ばれ、$w_i$で表すことにする。  \n",
    "重みの総和は1になるように正規化する。正規化前の値はスコアと呼んだりする。\n",
    "\n",
    "重要度は別の何らかのデータに基づいて設定される。一般的に、そのデータとの内積を取ることが多い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3つの隠れ状態$h_i$があるとする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6353, -0.5284,  0.2080,  0.8854,  1.8761],\n",
       "        [-0.2456,  2.2601,  2.0749,  1.7099,  2.3480],\n",
       "        [ 1.1584, -0.7575, -2.3605, -1.0838, -1.2678]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, hidden_size = 3, 5\n",
    "hs = torch.randn(batch_size, hidden_size)\n",
    "hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隠れ状態$h_i$と同じサイズの適当なデータ$x$があるとする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3501,  0.3938,  0.0569,  0.8265, -1.1349])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x$と、全ての隠れ状態$h_i$で内積を取り、softmaxで正規化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0457, 0.2867, 0.6677])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.tensor([h @ x for h in hs])\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スコアはこれでもOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = hs @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この3つが重みで、$x$にとっての$h_i$の重要度を表す。\n",
    "\n",
    "この重みで$h_i$の重み付き和をとることで、$x$にとっての重要度を反映したただ1つの隠れ状態を得ることが出来る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0747, -0.0241,  0.0095,  0.0404,  0.0857],\n",
       "        [-0.0704,  0.6479,  0.5948,  0.4902,  0.6731],\n",
       "        [ 0.7734, -0.5058, -1.5760, -0.7236, -0.8465]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重みをかける\n",
    "weighted_hs = torch.stack([w * h for w, h in zip(weights, hs)])\n",
    "weighted_hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6283,  0.1180, -0.9717, -0.1930, -0.0877])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 和をとる\n",
    "h = weighted_hs.sum(dim=0)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のようにまとめられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6283,  0.1180, -0.9717, -0.1930, -0.0877])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = weights @ hs # 重み付き和\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seqのdecoderでは、このattentionを利用して、encoderが出力した全ての隠れ状態を参照する。  \n",
    "\n",
    "encoderの隠れ状態の数はencoderへの入力の数によって変わる。ただ、可変長のデータをモデル内で扱うことは難しい。  \n",
    "そこで、attentionを用いて1つの固定長のベクトルに変換する。decoderでの演算時に、decoderのRNNから出力された隠れ状態を用いてencoderの隠れ状態の重要度を計算し、重み付き和をとる。こうすることで、encoderの隠れ状態から注目すべき重要な情報を都合よく抽出した固定長のベクトルを得ることが出来る。後はそれを以降の層に渡すだけ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお、重みが正しく着目すべき点を表すかは、学習させてみないと分からない。  \n",
    "この目的も、学習前の段階では期待に過ぎない。この仕組みを取り入れて学習させれば、次第に適切な重みが出力されるようになり、適切な出力が得られるようになるだろう。そうだといいな、ってだけ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、重みを求める関数が内積でないといけない理由はない。2つのベクトルからスカラーを得る関数であれば何でもよい。  \n",
    "内積は類似度を測ることができ、類似度が高いものに着目するという意味では適切に見えるが、そもそもベクトル空間が異なるので、それらの類似度は意味を持たない。  \n",
    "重みを求める関数を内積として学習を進めれば、重要度が高くなるべきタイミングでその2つのベクトルが類似するように学習される、というだけ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ただ実際はほとんどの場合で内積が使われる。それは内積という計算がシンプルだからってだけ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention層\n",
    "\n",
    "decoderの中の、attentionによって都合のいい隠れ状態を出力する部分は1つの層として見られる。  \n",
    "実装してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def forward(self, x, hs):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len_dec, hidden_size)\n",
    "        hs: (batch_size, seq_len_enc, hidden_size)\n",
    "        \"\"\"\n",
    "        scores = x @ hs.mT # (batch_size, seq_len_dec, seq_len_enc)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        h = weights @ hs # (batch_size, seq_len_dec, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_len_dec, seq_len_enc, hidden_size = 2, 3, 4, 5\n",
    "x = torch.randn(batch_size, seq_len_dec, hidden_size)\n",
    "hs = torch.randn(batch_size, seq_len_enc, hidden_size)\n",
    "\n",
    "attention = Attention()\n",
    "h = attention(x, hs)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## MASK\n",
    "\n",
    "padトークンがattentionの計算に含まれてしまうことを回避する。  \n",
    "maskをかけてpadトークンに対応する重みが0になるようにする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スコアに対して、対応する位置の値を$-\\infty$にする。そうすればsoftmaxを計算したときにその部分が0になる。\n",
    "\n",
    "こんなスコアがあったとする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3461, -0.4953, -0.3809, -2.0183, -0.8126])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.randn(5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後ろの2つがpadトークンだったとすると、こんな感じでmaskをかけてやればいい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3461, -0.4953, -0.3809,    -inf,    -inf])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = [False, False, False, True, True]\n",
    "scores[mask] = -torch.inf\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こう書いてもいい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3672,  0.1310,  0.3589,    -inf,    -inf])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.randn(5)\n",
    "mask = torch.tensor([0, 0, 0, 1, 1])\n",
    "scores.masked_fill_(mask, -torch.inf)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後はこれをsoftmaxに通す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2122, 0.3492, 0.4386, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = F.softmax(scores, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。これでpadトークンが無視されるようになる。\n",
    "\n",
    "層としても実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def forward(self, x, hs, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len_dec, hidden_size)\n",
    "        hs: (batch_size, seq_len_enc, hidden_size)\n",
    "        mask: (batch_size, seq_len_enc), bool, padトークンの位置\n",
    "        \"\"\"\n",
    "        scores = x @ hs.mT # (batch_size, seq_len_dec, seq_len_enc)\n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask.unsqueeze(1), -torch.inf) # maskを適用\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        h = weights @ hs # (batch_size, seq_len_dec, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Attentionを用いたSeq2Seq\n",
    "\n",
    "seq2seqにattention層を取り入れて翻訳モデルを作ってみよう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        eos_positions = x == eos_id # (batch_size, seq_len)\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        hs, h = self.rnn(x) # hs: (batch_size, seq_len, hidden_size)\n",
    "            # hs: (batch_size, seq_len, hidden_size)\n",
    "            # h: (1, batch_size, hidden_size)\n",
    "        hs = self.fc(hs) # (batch_size, seq_len, hidden_size)\n",
    "        h = hs[eos_positions].unsqueeze(0) # (1, batch_size, hidden_size)\n",
    "        return hs, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "前章のものにattention層を追加する。attention層の前後は残差結合で繋ぐ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.attention = Attention()\n",
    "        self.fc = nn.Linear(hidden_size * 2, n_vocab)\n",
    "\n",
    "    def forward(self, x, h_enc, hs_enc, mask=None):\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        hs, h = self.rnn(x, h_enc)\n",
    "            # hs: (batch_size, seq_len, hidden_size)\n",
    "            # h: (1, batch_size, hidden_size)\n",
    "        h = self.attention(hs, hs_enc, mask) # (batch_size, seq_len, hidden_size)\n",
    "        z = torch.cat([hs, h], dim=-1) # (batch_size, seq_len, hidden_size * 2)\n",
    "        y = self.fc(z) # (batch_size, seq_len, n_vocab)\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "全ての隠れ状態とpadトークンの位置をdecoderに渡すようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        mask = x_enc == pad_id\n",
    "        hs, h = self.encoder(x_enc)\n",
    "        y, _ = self.decoder(x_dec, h, hs, mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "def loss_fn(y, t):\n",
    "    loss = cross_entropy(y.reshape(-1, n_vocab_ja), t.ravel())\n",
    "    return loss\n",
    "\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_enc, x_dec, y_dec in test_loader:\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            total_loss += loss.item()\n",
    "    loss = total_loss / len(test_loader)\n",
    "    return loss\n",
    "\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    prog.start(n_iter=len(train_loader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for x_enc, x_dec, y_dec in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prog.update(loss.item())\n",
    "\n",
    "        if prog.now_epoch % prog_unit == 0:\n",
    "            test_loss = eval_model(model)\n",
    "            prog.memo(f'test: {test_loss:.5f}', no_step=True)\n",
    "        prog.memo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size, embed_size = 512, 512\n",
    "encoder = Encoder(n_vocab_ja, embed_size, hidden_size)\n",
    "decoder = Decoder(n_vocab_en, embed_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1-5/50: ############################## 100% [00:03:17.08] loss train: 5.14457, test: 4.57513 \n",
      " 6-10/50: ############################## 100% [00:03:11.39] loss train: 3.91416, test: 4.16876 \n",
      "11-15/50: ############################## 100% [00:03:09.16] loss train: 3.32283, test: 3.98527 \n",
      "16-20/50: ############################## 100% [00:03:10.80] loss train: 2.88578, test: 3.91809 \n",
      "21-25/50: ############################## 100% [00:03:09.96] loss train: 2.53864, test: 3.90768 \n",
      "26-30/50: ############################## 100% [00:03:11.72] loss train: 2.24531, test: 3.94121 \n",
      "31-35/50: ############################## 100% [00:03:13.58] loss train: 1.99422, test: 4.00357 \n",
      "36-40/50: ############################## 100% [00:03:12.77] loss train: 1.76944, test: 4.08644 \n",
      "41-45/50: ############################## 100% [00:03:31.26] loss train: 1.57332, test: 4.18753 \n",
      "46-50/50: ############################## 100% [00:03:11.87] loss train: 1.39651, test: 4.30312 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, n_epochs=50, prog_unit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 翻訳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_id = sp_en.unk_id() # UNKのID\n",
    "def token_sampling(y: List[float]) -> int:\n",
    "    \"\"\"モデルの出力から単語をサンプリングする\"\"\"\n",
    "    y[unk_id] = -torch.inf\n",
    "    probs = F.softmax(y, dim=-1)\n",
    "    token, = random.choices(range(n_vocab_en), weights=probs)\n",
    "    return token\n",
    "\n",
    "\n",
    "bos_id = sp_en.bos_id()\n",
    "eos_id = sp_en.eos_id()\n",
    "@torch.no_grad()\n",
    "def translate(\n",
    "    model: nn.Module,\n",
    "    in_text: str, # 入力文（日本語）\n",
    "    max_len: int = 100, # 出力のトークン数の上限\n",
    "    decisive: bool = True, # サンプリングを決定的にするか\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    in_ids = sp_ja.encode(in_text)\n",
    "    in_ids = torch.tensor(in_ids + [eos_id], device=device).unsqueeze(0)\n",
    "\n",
    "    hs, h = model.encoder(in_ids)\n",
    "    next_token = bos_id\n",
    "\n",
    "    token_ids = []\n",
    "    for _ in range(max_len):\n",
    "        x = torch.tensor([[next_token]], device=device)\n",
    "        y, h = model.decoder(x, h, hs)\n",
    "        y = y[0]\n",
    "        if decisive:\n",
    "            next_token = y.argmax().item()\n",
    "        else:\n",
    "            next_token = token_sampling(y)\n",
    "        token_ids.append(next_token)\n",
    "        if next_token == eos_id:\n",
    "            break\n",
    "    sentence = sp_en.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 駅情報\n",
      "\n",
      "output: North Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo Station, Jo\n",
      "answer: Information\n",
      "\n",
      "\n",
      "input: 三条京阪駅（さんじょうけいはんえき）は、京都市東山区にある、京都市営地下鉄東西線の鉄道駅。\n",
      "\n",
      "output: Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station Higashiyama Station\n",
      "answer: Located in the Higashiyama Ward of Kyoto City, Sanjyo-Keihan Station is a stop on the Tozai Line, a Kyoto Municipal Subway Line.\n",
      "\n",
      "\n",
      "input: 駅番号はT11。\n",
      "\n",
      "output: The  Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station (\n",
      "answer: The station number is T11.\n",
      "\n",
      "\n",
      "input: 京阪電気鉄道\n",
      "\n",
      "output: Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan Keihan\n",
      "answer: The Keihan Electric Railway\n",
      "\n",
      "\n",
      "input: 京阪本線・京阪鴨東線（三条駅 (京都府)）\n",
      "\n",
      "output: Keihan ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station ( Station (\n",
      "answer: Keihan Main Line and Keihan Oto Line in Sanjyo Station (in Kyoto Prefecture)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for x, t in zip(data_ja[:n], data_en[:n]):\n",
    "    print('input:', x)\n",
    "    print('output:', translate(model, x))\n",
    "    print('answer:', t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'この駅は京都市内の中心部にあります。',\n",
    "    '京都'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: この駅は京都市内の中心部にあります。\n",
      "output: A---------------------------------------------------------------------------------------------------\n",
      "\n",
      "input: 京都\n",
      "output: Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s Kyoto Station (-s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print('input:', sentence)\n",
    "    print('output:', translate(model, sentence))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
