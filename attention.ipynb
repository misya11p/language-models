{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import (\n",
    "    pad_sequence,\n",
    "    pack_padded_sequence,\n",
    "    pad_packed_sequence,\n",
    ")\n",
    "from dlprog import train_progress\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress(\n",
    "    width=20,\n",
    "    with_test=True,\n",
    "    label=\"ppl train\",\n",
    "    round=2,\n",
    "    agg_fn=lambda s, w: math.exp(s / w)\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 223108\n",
      "num of vocabrary (ja): 8000\n",
      "num of vocabrary (en): 8000\n"
     ]
    }
   ],
   "source": [
    "textfile_ja = \"data/iwslt2017_ja.txt\"\n",
    "textfile_en = \"data/iwslt2017_en.txt\"\n",
    "tokenizer_prefix_ja = f\"models/tokenizer_iwslt2017_ja\"\n",
    "tokenizer_prefix_en = f\"models/tokenizer_iwslt2017_en\"\n",
    "\n",
    "with open(textfile_en) as f:\n",
    "    data_en = f.read().splitlines()\n",
    "\n",
    "with open(textfile_ja) as f:\n",
    "    data_ja = f.read().splitlines()\n",
    "\n",
    "n_data = len(data_en)\n",
    "print(\"num of data:\", n_data)\n",
    "\n",
    "sp_ja = spm.SentencePieceProcessor(f\"{tokenizer_prefix_ja}.model\")\n",
    "sp_en = spm.SentencePieceProcessor(f\"{tokenizer_prefix_en}.model\")\n",
    "unk_id = sp_ja.unk_id()\n",
    "bos_id = sp_ja.bos_id()\n",
    "eos_id = sp_ja.eos_id()\n",
    "pad_id = sp_ja.pad_id()\n",
    "n_vocab_ja = len(sp_ja)\n",
    "n_vocab_en = len(sp_en)\n",
    "print(\"num of vocabrary (ja):\", n_vocab_ja)\n",
    "print(\"num of vocabrary (en):\", n_vocab_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids_ja = sp_ja.encode(data_ja)\n",
    "data_ids_en = sp_en.encode(data_en)\n",
    "\n",
    "for ids_ja, ids_en in zip(data_ids_ja, data_ids_en):\n",
    "    ids_ja.append(eos_id)\n",
    "    ids_en.insert(0, bos_id)\n",
    "    ids_en.append(eos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train data: 178487\n",
      "num of test data: 44621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 71]), torch.Size([32, 71]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids_ja, data_ids_en):\n",
    "        self.data_ja = [torch.tensor(ids) for ids in data_ids_ja]\n",
    "        self.data_en = [torch.tensor(ids) for ids in data_ids_en]\n",
    "        self.n_data = len(self.data_ja)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ja = self.data_ja[idx]\n",
    "        en = self.data_en[idx]\n",
    "        x_enc = ja\n",
    "        x_dec = en[:-1]\n",
    "        y_dec = en[1:]\n",
    "        return x_enc, x_dec, y_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x_enc, x_dec, y_dec= zip(*batch)\n",
    "    x_enc = pad_sequence(x_enc, batch_first=True, padding_value=pad_id)\n",
    "    x_dec = pad_sequence(x_dec, batch_first=True, padding_value=pad_id)\n",
    "    y_dec = pad_sequence(y_dec, batch_first=True, padding_value=pad_id)\n",
    "    return x_enc, x_dec, y_dec\n",
    "\n",
    "dataset = TextDataset(data_ids_ja, data_ids_en)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "print(\"num of train data:\", len(train_dataset))\n",
    "print(\"num of test data:\", len(test_dataset))\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "x_enc, x_dec, y_dec = next(iter(train_loader))\n",
    "x_enc.shape, x_dec.shape, y_dec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Attention機構\n",
    "\n",
    "複数のデータの中から重要なデータに着目する仕組み。Attention = 注意、注目、着目。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ある一つの入力と、関連する複数のデータを考える。関連する複数のデータはmemoryと呼ぶ。入力を元に、memoryの中のどのデータに着目するかを定めることがAttentionの目的である。各データに重要度を割り当てるという感じ。\n",
    "\n",
    "重要度は重みと呼ばれ、$w_i$で表すことにする。重みは総和が1になるようにsoftmaxなどで正規化する。正規化前の値はスコアと呼んだりする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各memoryに対応する重みは入力との内積で求める。別に内積じゃなくてもいいけど、内積が一番簡単だし性能も良い。内積が取れるように、memoryの各ベクトルは入力と同じ次元にする必要がある。\n",
    "\n",
    "重みを求めた後は、その重みでmemoryの重み付き和をとる。そうすることで、memoryの中から重要な要素を多めに取り出した固定長のベクトルが得られる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "やってみよう。まず、入力と、三つのデータからなるmemoryを用意する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7003,  0.3728,  0.9630, -0.7548,  1.4186],\n",
       "        [-0.3834,  0.7128, -0.5010, -0.5049,  0.4977],\n",
       "        [-0.0481,  0.5967,  2.3803,  0.1163,  0.1539]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, d = 3, 5\n",
    "\n",
    "x = torch.randn(d)\n",
    "memory = torch.randn(n, d)\n",
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力とmemory内の全てのデータで内積を取る。これがスコアに当たる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3264, 0.1925, 0.4811])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.tensor([m @ x for m in memory])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでもいい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = x @ memory.T\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmaxで正規化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = F.softmax(scores, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この重みがmemoryの各データの重要度を表す。これで重み付き和をとる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2286,  0.1217,  0.3143, -0.2464,  0.4630],\n",
       "        [-0.0738,  0.1372, -0.0964, -0.0972,  0.0958],\n",
       "        [-0.0231,  0.2871,  1.1452,  0.0560,  0.0740]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重みをかける\n",
    "weighted_memory = torch.stack([w * m for w, m in zip(weights, memory)])\n",
    "weighted_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1316,  0.5460,  1.3631, -0.2876,  0.6329])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 和をとる\n",
    "y = weighted_memory.sum(dim=0)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のようにまとめられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1316,  0.5460,  1.3631, -0.2876,  0.6329])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = weights @ memory # 重み付き和\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上がattention機構の演算の流れである。まとめるとこう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1316,  0.5460,  1.3631, -0.2876,  0.6329])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = memory @ x # スコア\n",
    "weights = F.softmax(scores, dim=-1) # 重み\n",
    "y = weights @ memory # 重み付き和\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数式だとこうなる。\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\boldsymbol x,M) = \\text{softmax}(\\boldsymbol xM^T)M\n",
    "$$\n",
    "\n",
    "- $\\boldsymbol x\\in\\mathbb R^{d}$ : 入力\n",
    "- $M\\in\\mathbb R^{n\\times d}$ : memory\n",
    "\n",
    "\\*列ベクトルと行ベクトルを区別していないので厳密ではない。厳密に書くならこう:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\boldsymbol x,M) = (\\text{softmax}(\\boldsymbol x^TM^T)M)^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Attentionを用いたSeq2Seq\n",
    "\n",
    "Seq2SeqにAttentionを導入し、Encoderが出力した全ての隠れ状態をDecoderから参照する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecoderにAttentionを取り入れる。入力は前の層からの出力で、memoryはEncoderが出力した全ての隠れ状態である。こうすることで、Decoderは各時刻でその時着目すべき情報に着目した演算が行える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4589,  0.3038,  1.8257, -0.6385,  0.2316])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 3\n",
    "hidden_size = 5\n",
    "hs_enc = torch.randn(seq_len, hidden_size) # encoderが出力した全ての隠れ状態\n",
    "h_dec = torch.randn(hidden_size) # ある時間tのdecoderの隠れ状態\n",
    "\n",
    "scores = h_dec @ hs_enc.T # (seq_len,)\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "y = weights @ hs_enc # (hidden_size,)\n",
    "y # encoderの全ての隠れ状態から重要な部分を多く抜き出したベクトル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに、重みが正しく着目すべき点を表すかは、学習させてみないと分からない。この目的も、学習前の段階では期待に過ぎない。この仕組みを取り入れて学習させれば、次第に適切な重みが出力されるようになり、適切な出力が得られるようになるだろう。そうだといいな、ってだけ。\n",
    "\n",
    "全ての隠れ状態を参照した固定長のベクトルを得るだけであれば、単に全ての隠れ状態を足すだけでもいい。ただ、重みを変えられるような枠組みを取り入れてあげれば学習が上手くいくんじゃね？ってだけ。そして本当にうまくいったからここで紹介されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、重みを求める関数が内積でないといけない理由はない。2つのベクトルからスカラーを得る関数であれば何でもよい。\n",
    "\n",
    "内積は類似度を測ることができ、類似度が高いものに着目するという意味では適切に見えるが、そもそも比較するベクトルはいくつかの層を経て複雑に変化するため、それらの類似度は意味を持たない。重みを求める関数を内積として学習を進めれば、重要度が高くなるべきタイミングでその2つのベクトルが類似するように学習される、というだけ。\n",
    "\n",
    "ただ実際はほとんどの場合で内積が使われる。それは内積という計算がシンプルだからってだけ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention層\n",
    "\n",
    "Decoderの中の、Attentionによって都合のいい隠れ状態を出力する部分は一つの層として見られる。複数時刻の入力を考慮して以下のように表す。\n",
    "\n",
    "$$\n",
    "\\text{Attention}(X,M) = \\text{softmax}(XM^T)M\n",
    "$$\n",
    "\n",
    "- $X\\in\\mathbb R^{n_i\\times d}$ : 層への入力\n",
    "- $M\\in\\mathbb R^{n_m\\times d}$ : memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def forward(self, x, hs):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len_dec, hidden_size)\n",
    "        hs: (batch_size, seq_len_enc, hidden_size)\n",
    "        \"\"\"\n",
    "        scores = x @ hs.mT # (batch_size, seq_len_dec, seq_len_enc)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        h = weights @ hs # (batch_size, seq_len_dec, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_len_dec, seq_len_enc, hidden_size = 2, 3, 4, 5\n",
    "x = torch.randn(batch_size, seq_len_dec, hidden_size)\n",
    "hs = torch.randn(batch_size, seq_len_enc, hidden_size)\n",
    "\n",
    "attention = Attention()\n",
    "h = attention(x, hs)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Attention\n",
    "\n",
    "padトークンがAttentionの計算に含まれてしまうことを回避する。maskをかけてpadトークンに対応する重みが0になるようにする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スコアに対して、対応する位置の値を$-\\infty$にする。そうすればsoftmaxを計算したときにその部分が0になる。\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\boldsymbol x,M) = \\text{softmax}(\\boldsymbol xM^T-\\infty\\,\\text{mask}) M\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こんなスコアがあったとする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7205, -0.7592,  1.8133, -0.2011, -0.0768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.randn(5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後ろの2つがpadトークンだったとすると、こんな感じでmaskをかけてやればいい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7205, -0.7592,  1.8133,    -inf,    -inf])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = [False, False, False, True, True]\n",
    "scores[mask] = -torch.inf\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こう書いてもいい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2911, -1.3570, -0.8464,    -inf,    -inf])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = torch.randn(5)\n",
    "mask = torch.tensor([0, 0, 0, 1, 1])\n",
    "scores.masked_fill_(mask, -torch.inf)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後はこれをsoftmaxに通す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6609, 0.1272, 0.2119, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = F.softmax(scores, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。これでpadトークンが無視されるようになる。\n",
    "\n",
    "層としても実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def forward(self, x, hs, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len_dec, hidden_size)\n",
    "        hs: (batch_size, seq_len_enc, hidden_size)\n",
    "        mask: (batch_size, seq_len_enc), bool, padトークンの位置\n",
    "        \"\"\"\n",
    "        scores = x @ hs.mT # (batch_size, seq_len_dec, seq_len_enc)\n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask.unsqueeze(1), -torch.inf) # maskを適用\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        h = weights @ hs # (batch_size, seq_len_dec, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 実践\n",
    "\n",
    "実際にAttentionをSeqSeqに取り入れて翻訳モデルを学習させてみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル構築\n",
    "\n",
    "前章のSeq2Seqをベースとする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずEncoder。全ての時刻の隠れ状態を出力する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x_pack = pack_padded_sequence(\n",
    "            x, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        hs, (h, _) = self.lstm(x_pack)\n",
    "        hs, _ = pad_packed_sequence(hs, batch_first=True)\n",
    "        return hs, h\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm1 = PackedLSTM(embed_size, hidden_size // 2, True)\n",
    "        self.lstm2 = PackedLSTM(hidden_size, hidden_size, False)\n",
    "        self.lstm3 = PackedLSTM(hidden_size, hidden_size, False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lengths = (x != pad_id).sum(dim=1).cpu()\n",
    "\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "\n",
    "        hs, _ = self.lstm1(x, lengths) # (batch_size, seq_len, hidden_size)\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        skip = hs\n",
    "        hs, _ = self.lstm2(hs, lengths)\n",
    "        hs = hs + skip\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        skip = hs\n",
    "        hs, _ = self.lstm3(hs, lengths)\n",
    "        hs = hs + skip\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        return hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にDecoder。LSTMの後にAttention層を追加する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm1 = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.attention = Attention()\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hs_enc, hc=None, mask=None):\n",
    "        hc1, hc2, hc3 = hc or (None, None, None)\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "\n",
    "        hs, hc1 = self.lstm1(x, hc1) # (batch_size, seq_len, hidden_size)\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        skip = hs\n",
    "        hs, hc2 = self.lstm2(hs, hc2)\n",
    "        hs = self.attention(hs, hs_enc, mask)\n",
    "        hs = hs + skip\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        skip = hs\n",
    "        hs, hc3 = self.lstm3(hs)\n",
    "        hs = self.attention(hs, hs_enc, mask)\n",
    "        hs = hs + skip\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        y = self.fc(hs) # (batch_size, seq_len, n_vocab)\n",
    "        return y, (hc1, hc2, hc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、これらをまとめる。\n",
    "\n",
    "全ての隠れ状態とpadトークンの位置をDecoderに渡すようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        hs = self.encoder(x_enc)\n",
    "        mask = x_enc == pad_id # (batch_size, seq_len_enc)\n",
    "        y, _ = self.decoder(x_dec, hs, mask=mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of parameters: 24,379,200\n"
     ]
    }
   ],
   "source": [
    "hidden_size, embed_size = 512, 512\n",
    "encoder = Encoder(n_vocab_ja, embed_size, hidden_size)\n",
    "decoder = Decoder(n_vocab_en, embed_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "model_path = \"models/lm_seq2seq_attn.pth\"\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"num of parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention層はパラメータがないのでパラメータ数は変わらない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前章では最後の隠れ状態をLSTMの初期値として渡したが、今回は全ての隠れ状態をAttention層に渡す。LSTMの初期値は0ベクトル。LSTMの初期値として最後の隠れ状態も渡すという手もあるが、今回はしないでおく。渡した方が精度上がったりするんかな。でも試すのめんどいからいいや。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "def loss_fn(y, t):\n",
    "    loss = cross_entropy(y.reshape(-1, n_vocab_ja), t.ravel())\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for x_enc, x_dec, y_dec in test_loader:\n",
    "        x_enc = x_enc.to(device)\n",
    "        x_dec = x_dec.to(device)\n",
    "        y_dec = y_dec.to(device)\n",
    "\n",
    "        y = model(x_enc, x_dec)\n",
    "        loss = loss_fn(y, y_dec)\n",
    "        losses.append(loss.item())\n",
    "    loss = sum(losses) / len(losses)\n",
    "    ppl = math.exp(loss)\n",
    "    return ppl\n",
    "\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    prog.start(n_iter=len(train_loader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for x_enc, x_dec, y_dec in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prog.update(loss.item())\n",
    "\n",
    "        if prog.now_epoch % prog_unit == 0:\n",
    "            test_ppl = eval_model(model)\n",
    "            prog.memo(f\"test: {test_ppl:.2f}\", no_step=True)\n",
    "        prog.memo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/20: #################### 100% [00:05:37.37] ppl train: 163.44, test: 96.76 \n",
      " 2/20: #################### 100% [00:05:27.32] ppl train: 87.08, test: 67.08 \n",
      " 3/20: #################### 100% [00:05:32.71] ppl train: 63.37, test: 51.04 \n",
      " 4/20: #################### 100% [00:05:29.29] ppl train: 50.02, test: 42.23 \n",
      " 5/20: #################### 100% [00:05:32.49] ppl train: 41.81, test: 36.87 \n",
      " 6/20: #################### 100% [00:05:35.21] ppl train: 36.39, test: 33.28 \n",
      " 7/20: #################### 100% [00:05:34.07] ppl train: 32.59, test: 30.86 \n",
      " 8/20: #################### 100% [00:05:33.51] ppl train: 29.71, test: 29.05 \n",
      " 9/20: #################### 100% [00:05:35.21] ppl train: 27.48, test: 27.66 \n",
      "10/20: #################### 100% [00:05:33.87] ppl train: 25.66, test: 26.57 \n",
      "11/20: #################### 100% [00:05:33.15] ppl train: 24.18, test: 25.72 \n",
      "12/20: #################### 100% [00:05:33.86] ppl train: 22.93, test: 25.04 \n",
      "13/20: #################### 100% [00:05:34.59] ppl train: 21.82, test: 24.44 \n",
      "14/20: #################### 100% [00:05:31.35] ppl train: 20.90, test: 23.97 \n",
      "15/20: #################### 100% [00:05:37.75] ppl train: 20.05, test: 23.66 \n",
      "16/20: #################### 100% [00:05:36.19] ppl train: 19.32, test: 23.34 \n",
      "17/20: #################### 100% [00:05:39.62] ppl train: 18.66, test: 23.03 \n",
      "18/20: #################### 100% [00:05:41.39] ppl train: 18.07, test: 22.88 \n",
      "19/20: #################### 100% [00:05:41.56] ppl train: 17.52, test: 22.72 \n",
      "20/20: #################### 100% [00:05:42.04] ppl train: 17.03, test: 22.54 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, n_epochs=20, prog_unit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attentionなしの時は20epochでtrain pplが25、test pplが36だったので、精度が上がっていることが期待される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 翻訳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self):\n",
    "        self.sentence = []\n",
    "        self.hc = None\n",
    "        self.ll = 0.0\n",
    "        self.norm_ll = 0.0\n",
    "        self.finished = False\n",
    "        self.next_token = None\n",
    "\n",
    "    def update(self, token_id, log_prob):\n",
    "        self.append(token_id)\n",
    "        self.ll += log_prob\n",
    "        self.norm_ll = self.ll / len(self.sentence)\n",
    "        self.next_token = token_id\n",
    "\n",
    "    def append(self, token_id):\n",
    "        if token_id == eos_id:\n",
    "            self.finished = True\n",
    "        self.sentence.append(token_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.sentence)\n",
    "\n",
    "\n",
    "bos_id = sp_en.bos_id()\n",
    "eos_id = sp_en.eos_id()\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate(\n",
    "    model: nn.Module,\n",
    "    in_text: str, # 入力文（日本語）\n",
    "    lim_len: int = 100, # 出力のトークン数の上限\n",
    "    k: int = 4, # ビーム幅\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    in_ids = sp_ja.encode(in_text)\n",
    "    in_ids = torch.tensor(in_ids + [eos_id], device=device).unsqueeze(0)\n",
    "\n",
    "    hs = model.encoder(in_ids)\n",
    "\n",
    "    first_sentence = Sentence()\n",
    "    first_sentence.next_token = bos_id\n",
    "    sentences = [first_sentence]\n",
    "    max_len = 1\n",
    "\n",
    "    while not all(s.finished for s in sentences) and max_len < lim_len:\n",
    "        new_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if sentence.finished:\n",
    "                new_sentences.append(sentence)\n",
    "                continue\n",
    "            x = torch.tensor([[sentence.next_token]], device=device)\n",
    "            y, hc = model.decoder(x, hs, sentence.hc)\n",
    "            y = F.log_softmax(y, dim=-1).squeeze(0, 1)\n",
    "            sentence.hc = hc\n",
    "            for token_id in y.topk(k).indices.tolist():\n",
    "                new_sentence = copy.deepcopy(sentence)\n",
    "                new_sentence.update(token_id, y[token_id].item())\n",
    "                new_sentences.append(new_sentence)\n",
    "        sentences = sorted(new_sentences, key=lambda s: s.norm_ll, reverse=True)\n",
    "        sentences = sentences[:k]\n",
    "        max_len = max(map(len, sentences))\n",
    "\n",
    "    best_sentence = max(sentences, key=lambda s: s.norm_ll).sentence\n",
    "    return sp_en.decode(best_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 犬の散歩\n",
      "output: It's a dog.\n",
      "correct: Walk the dog.\n",
      "\n",
      "input: だから先手を打って 大金を小金に変える一番早い方法を— 探していたんだと 答えることにしています\n",
      "output: So I'm trying to figure out how to do it, and I'm trying to figure out how to make a small amount of money, and I'm trying to figure out what's going on.\n",
      "correct: And so I tell people, well, I was trying to figure out the fastest way to turn a large fortune into a small one.\n",
      "\n",
      "input: 寿司作りからプログラミングまで あらゆる講座を Skillshareで受けることができます DogVacayを使えば ペットまでも共有できます\n",
      "output: We're going to share with you all the things that we're going to do with a lot of these things, and we're going to be able to share the Skillows, and I can share with you.\n",
      "correct: We are trading lessons on everything from sushi-making to coding on Skillshare, and we're even sharing our pets on DogVacay.\n",
      "\n",
      "input: 私たちが恩恵を受けてきた時のように 図書館と出版社の両方が共存する世界をどうやって作ればいいのか?\n",
      "output: How do we make a world of libraries and publishing the world together?\n",
      "correct: How do we go and have a world where we both have libraries and publishing in the future, just as we basically benefited as we were growing up?\n",
      "\n",
      "input: もし あの月が夢を見ることができたら 月の見る夢は私たちにとっての夢になるでしょう\n",
      "output: And if that month dream, the dream is dreaming of the dream, the dream of the moon is for us to be dreaming for us.\n",
      "correct: If the Moon could dream, I think that would be its dream for us.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(train_dataset))\n",
    "    x, _, t = train_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print(\"input:\", x)\n",
    "    print(\"output:\", translate(model, x))\n",
    "    print(\"correct:\", t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: より幸せになるために毎日すべき 5つのこととは何でしょう?\n",
      "output: What are five things to be happier?\n",
      "correct: What are the five things that you should do every day to be happier?\n",
      "\n",
      "input: この機械は その場で音を作り出すマシンです リアルタイム音楽製造機です この機械を使えば 私の声だけを使い 頭の中に聞こえたままの音楽を リアルタイムで 肉体から生じる制約に 邪魔されることなく 作り出すことができます\n",
      "output: This is a machine that's going to be able to use the sound of the real-time machine, because it's a real-time machine, and I can't be able to use this machine in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time in real time\n",
      "correct: We've made a system which is basically a live production machine, a real-time music production machine, and it enables me to, using nothing but my voice, create music in real time as I hear it in my head unimpeded by any physical restrictions that my body might place on me.\n",
      "\n",
      "input: 世界は2グループに 分かれていました\n",
      "output: The world was divided in two groups.\n",
      "correct: The world was two groups.\n",
      "\n",
      "input: そして私は自分のブログを少しずつ閉じていきました\n",
      "output: And I've got a little bit of my blog.\n",
      "correct: And I started to kill my blog slowly.\n",
      "\n",
      "input: 二階に2つ目のトイレを作るのもいけません\n",
      "output: We need to make the second two-story toilet.\n",
      "correct: Then, another bathroom upstairs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(test_dataset))\n",
    "    x, _, t = test_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print(\"input:\", x)\n",
    "    print(\"output:\", translate(model, x))\n",
    "    print(\"correct:\", t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ありがとう\n",
      "output: Thank you.\n",
      "\n",
      "input: 私はかわいい猫を飼っています。\n",
      "output: I'm a cute cat.\n",
      "\n",
      "input: 上手く文章が書けるようになりました\n",
      "output: It's got to write a paper.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "sentences = [\n",
    "    \"ありがとう\",\n",
    "    \"私はかわいい猫を飼っています。\",\n",
    "    \"上手く文章が書けるようになりました\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"input:\", sentence)\n",
    "    print(\"output:\", translate(model, sentence))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "少しマシになったかも。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Attentionの可視化\n",
    "\n",
    "学習させたモデルのAttention層がどのように動いているかを可視化して確かめてみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attentionはデータの一部分に着目する枠組みを提供する。先で学習させたSeq2Seqは、これをDecoderに取り入れ、Encoderから出力された隠れ状態の特定の位置に着目する。本節では、推論時のAttention層の重みを見て、学習させたモデルがどのデータに着目して推論を行っているかを確かめる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11]), torch.Size([13]))"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.randint(0, len(train_dataset))\n",
    "# i = 12\n",
    "# i = 14\n",
    "# i = 35129\n",
    "\n",
    "\n",
    "x_enc, x_dec, _ = train_dataset[i]\n",
    "x_enc.shape, x_dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_ja = sp_ja.decode(x_enc.tolist())\n",
    "tokens_ja = sp_ja.encode(text_ja, out_type=str)\n",
    "text_en = sp_en.decode(x_dec.tolist())\n",
    "tokens_en = sp_en.encode(text_en, out_type=str)\n",
    "\n",
    "\n",
    "hs_enc = model.encoder(x_enc.unsqueeze(0))\n",
    "\n",
    "embed = model.decoder.embedding(x_dec.unsqueeze(0))\n",
    "hs_dec, _ = model.decoder.lstm1(embed)\n",
    "skip = hs_dec\n",
    "hs_dec1, _ = model.decoder.lstm2(hs_dec)\n",
    "\n",
    "hs_dec = model.decoder.attention(hs_dec1, hs_enc)\n",
    "hs_dec = hs_dec + skip\n",
    "hs_dec2, _ = model.decoder.lstm3(hs_dec)\n",
    "\n",
    "\n",
    "hs_enc = hs_enc.squeeze(0).detach()\n",
    "hs_dec1 = hs_dec1.squeeze(0).detach()\n",
    "hs_dec2 = hs_dec2.squeeze(0).detach()\n",
    "hs_enc.shape, hs_dec.shape\n",
    "\n",
    "\n",
    "\n",
    "scores1 = hs_dec1 @ hs_enc.T\n",
    "scores2 = hs_dec2 @ hs_enc.T\n",
    "weights1 = F.softmax(scores1, dim=-1)\n",
    "weights2 = F.softmax(scores2, dim=-1)\n",
    "weights = (weights1 + weights2) / 2\n",
    "weights.shape\n",
    "\n",
    "\n",
    "tokens_en = list(map(lambda x: x.replace(\"▁\", \"\"), tokens_en))\n",
    "tokens_ja = list(map(lambda x: x.replace(\"▁\", \"\"), tokens_ja))\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(weights.T, cmap=\"gray\")\n",
    "plt.xticks(range(len(tokens_en)), tokens_en, rotation=45)\n",
    "plt.yticks(range(len(tokens_ja)), tokens_ja);\n",
    "\n",
    "# for i, wi in enumerate(weights):\n",
    "#     for j, wij in enumerate(wi):\n",
    "#         plt.text(i, j, f\"{wij:.2f}\", ha=\"center\", va=\"center\", color=\"red\", fontsize=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
