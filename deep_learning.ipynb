{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習を用いた言語モデル\n",
    "\n",
    "*Deep Learning*\n",
    "\n",
    "深層学習を用いて言語モデルを作成する。  \n",
    "前章で作成したマルコフモデルの様な「ある単語から次の単語を予測するモデル」をニューラルネットワークを用いて作成する。\n",
    "\n",
    "本章では、深層学習を活用した言語モデル実装の基礎を学ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## データセット\n",
    "\n",
    "wiki40b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 89698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。\\n',\n",
       " 'ライブドア社員であった初代代表取締役社長の山名真由によって企業内起業の形で創業。2005年に株式会社ライブドアから分割されて設立。かつてはライブドアホールディングス（現・LDH）の子会社であったが、ノンコア事業の整理にともない、株式会社ゲオ（現：株式会社ゲオホールディングス）に所有する全株式を譲渡し、同社の完全子会社となった。「ぽすれん」「ゲオ宅配レンタル」のオンラインDVD・CD・コミックレンタルサービス及び「GEO Online」と「ゲオアプリ」のアプリ・ウェブサイト運営の大きく分けて2事業を展開している。以前はDVD販売等のEコマースサービス「ぽすれんストア」、動画配信コンテンツ「ぽすれんBB」や電子書籍配信サービスの「GEO☆Books」事業も行っていた。オンラインDVDレンタル事業では会員数は10万人（2005年9月時点）。2006年5月よりCDレンタルを開始。同業他社には、カルチュア・コンビニエンス・クラブが運営する『TSUTAYA DISCAS』のほか、DMM.comが運営する『DMM.com オンラインDVDレンタル』がある。過去には「Yahoo!レンタルDVD」と「楽天レンタル」の運営を受託していた。\\n',\n",
       " '2005年の一時期、東京のラジオ局、InterFMで、「堀江社長も使っているライブドアのぽすれん」というキャッチコピーでラジオCMを頻繁に行っていたことがあった。\\n',\n",
       " '香川県内の農業協同組合の信用事業を統括する県域農協系金融機関であり、県内農業協同組合を会員とする。香川県は全県単一農協の香川県農業協同組合となったが、先に単一農協となった奈良県や沖縄県のケースと異なり、信連の統合は行われなかった。通称は「JA香川信連」または「JAバンク香川」。統一金融機関コードは3037。主に法人顧客を中心としており、個人取引は殆どない。県内の大型商業施設にある、他金融機関管理の共同ATMには香川信連の管轄のものがある。\\n',\n",
       " '534年（永熙3年）、独孤信の子として生まれた。独孤信が父母妻子を捨てて長安に入ったため、独孤羅は東魏に取り残されて高氏の虜囚となった。独孤信が宇文護により処刑されると、ようやく釈放されて、中山に寓居した。北斉の独孤永業の一族として田宅を与えられた。北斉が滅亡し、楊堅が定州総管となると、楊堅の妻の独孤伽羅が兄の行方を捜索させて独孤羅を見つけ出し、初めて対面した。579年（大象元年）、功臣の子として楚安郡太守に任じられた。まもなく病のため辞職し、長安に帰った。580年（大象2年）、楊堅が北周の丞相となると、独孤羅は儀同大将軍の位を受け、楊堅の側近に仕えた。581年（開皇元年）、隋が建国されると、使持節・上開府・儀同大将軍の位を受けた。11月、右武衛将軍に転じた。582年（開皇2年）、父の趙国公の爵位を嗣いだ。592年（開皇12年）、大将軍・太子右衛率となった。593年（開皇13年）、涼州刺史に任じられた。597年（開皇17年）、涼州総管に任じられた。599年（開皇19年）2月6日、死去した。諡は徳といった。\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textfile = 'data/jawiki.txt'\n",
    "with open(textfile) as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "print('num of data:', len(data))\n",
    "data[:5] # examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多すぎるので減らす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:1000]\n",
    "\n",
    "textfile = 'data/jawiki_1000.txt'\n",
    "with open(textfile, 'w') as f:\n",
    "    for sentence in data:\n",
    "        f.write(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## サブワード分割\n",
    "\n",
    "トークン化の手法。\n",
    "\n",
    "NNでも、マルコフモデル同様、文章をトークン化して学習を行う。  \n",
    "NNでは文字列をそのまま扱えないので、各トークンにID=クラスラベルを割り当てて、そのシーケンスとして文章を扱う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前章とは異なり、サブワード分割という新たな手法を使う。ちゃんと理由もある。\n",
    "\n",
    "1トークンあたりの文字数が多いことは、いくつかのデメリットを生む。  \n",
    "例えば、トークンの種類が多くなること。深層学習を用いた言語モデルはトークンの種類に比例してパラメータの数が増え、学習が困難になる。  \n",
    "それから、未知語が増えること。長いトークンは、それだけ多くの情報を持った限定的な言葉ということとなり、これらで語彙が埋まると表現力が落ちる。またデータセットに存在しない言葉を扱うことが困難になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サブワード分割は、これらの問題を解決する。  \n",
    "サブワードと呼ばれる、単語よりも小さな単位に分割することで、1トークンあたりの文字数を減らす。\n",
    "\n",
    "この手法では、データセットから頻出する文字の並びを学習し、その並びをトークンとして分割する。学習された、トークン化を行うモデルをトークナイザと呼ぶ。  \n",
    "データセットに合ったトークン化が可能。また語彙数を指定することも可能。指定した語彙数に収まるまで細かく分割してくれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに、ChatGPTが使っているトークナイザはこれ: [OpenAI Platform](https://platform.openai.com/tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "少し余談。\n",
    "\n",
    "前章でトークン化について以下のように説明した。\n",
    "\n",
    "> トークンとはモデルが扱える最小単位のことで、例えば単語が該当する。\n",
    "\n",
    "ここで、そもそも言語モデルは単語の確率を扱うものなので、絶対にトークン=単語にならないとおかしいとも考えられる。  \n",
    "実は文章生成においては、単語を最小単位にしなければならない理由はない。極端な話、文字を最小単位として文章を文字の並びと見てもいい訳だ。なんらかのシーケンスとできればそれで充分なのだ。実際、現在有名な言語モデルの多くは単語を最小単位としていない。\n",
    "\n",
    "ただ、言語モデルは単語の並びに確率を割り当てるモデルだ。単語を最小単位とするモデルだ。  \n",
    "もしこの定義に厳格になるのであれば、現在有名な多くの言語モデルは言語モデルと呼べないのかもしれんね。しらんけど。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "閑話休題。実際にやってみよう。  \n",
    "sentencepieceというライブラリを用いる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n",
    "\n",
    "語彙数とデータセット（テキストファイル）を指定して、学習させる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_prefix = 'models/tokenizer_jawiki_1000' # トークナイザのモデル名\n",
    "vocab_size = 8000 # 語彙数\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile, # データセット\n",
    "    model_prefix=tokenizer_prefix,\n",
    "    vocab_size=vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークン化\n",
    "\n",
    "学習したモデルにテキストを突っ込むとトークン化してくれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 1993, 6, 2317, 681, 343, 263]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(f'{tokenizer_prefix}.model') # モデル読み込み\n",
    "text = '今日はいい天気だ'\n",
    "ids = sp.encode(text)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストがトークン化され、ID列として取得できる。  \n",
    "`out_type`を指定すると文字列のリストが取得できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '今日', 'は', 'いい', '天', '気', 'だ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.encode(text, out_type=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アンダーバーは空白を意味する。初めにアンダーバーが付くのは仕様。\n",
    "\n",
    "ID列を文字列に戻すこともできる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今日はいい天気だ'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全てのデータをトークン化（ID化）しよう。  \n",
    "文章のリスト: `List[str]`を`sp.encode()`に与えるとID列のリスト: `List[List[int]]`が返ってくる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary: 8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11, 18, 6254, 54, 1057, 58, 1685, 79, 122, 17]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ids = sp.encode(data)\n",
    "n_vocab = len(sp)\n",
    "print('num of vocabrary:', n_vocab)\n",
    "data_ids[0][:10] # example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習データ\n",
    "\n",
    "NNの学習を行うため、入力と出力のペアを作成する。  \n",
    "今回はある単語から次に続く単語を予測するモデルを作成するので、ある単語IDとその次の単語IDがペアとなったデータを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 243959\n",
      "tensor([6863,    5,  289, 1301,  100,   21, 1825,   14, 4691, 5159, 2842,   87,\n",
      "         112,  262,  153,  593,  488,  652, 7911,  522,    3,    7,  496,  640,\n",
      "        1079,  119,    4, 2066, 2876,  449,    8,    5,  891,  266,   10,  476,\n",
      "         867, 1507,  999,  370, 1941,    4,   37,   86,   68, 1785,    4, 7985,\n",
      "        2285,  189,   19,    4, 1748, 2425,    3,    8,  730,  840,  603,  382,\n",
      "        1678,    7,    4, 5694])\n",
      "tensor([3395,  644, 3340,   54,  112,   12,   10, 2317, 4939,    7,  592, 1597,\n",
      "           5,    6,   26,   54,   12,   17,  159,   32, 1664,    4,  256,    3,\n",
      "          90, 2771, 5458,  316,   12,  101, 4035,  665,   64,   25, 2907,   12,\n",
      "           4,  142,   12,  566,  205, 4138,    3,   93,    5,   18,  375,   37,\n",
      "         218,    8,    5,  316,   36,    4,  282,  540, 1789,    8, 6697, 1021,\n",
      "        1568,  355,  282, 1090])\n"
     ]
    }
   ],
   "source": [
    "Ids = List[int]\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids: List[Ids]):\n",
    "        self.x = [] # 入力\n",
    "        self.t = [] # 正解\n",
    "        for ids in data_ids:\n",
    "            self.x += ids[:-1]\n",
    "            self.t += ids[1:]\n",
    "        self.n_data = len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.t[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "batch_size = 64\n",
    "dataset = TextDataset(data_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "n_data = len(dataset)\n",
    "print('num of data:', n_data)\n",
    "\n",
    "# examples\n",
    "x, y = next(iter(dataloader))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築\n",
    "\n",
    "ある単語IDを入力に取り、次の単語IDを出力するモデルを作成する。\n",
    "\n",
    "単語IDは入力時にone-hotベクトルに変換する。出力は語彙数分の次元を持つベクトルとする。  \n",
    "つまりこのタスクは単語の分類問題とも言える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, n_vocab: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_vocab, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_vocab)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size,)\n",
    "        \"\"\"\n",
    "        x = F.one_hot(x, self.n_vocab).to(torch.float32)\n",
    "            # (batch_size, n_vocab)\n",
    "        y = self.net(x) # (batch_size, n_vocab)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習\n",
    "\n",
    "損失関数に交差エントロピーを設定し、通常の分類モデルと同じように学習する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    model.train()\n",
    "    prog.start(n_iter=len(dataloader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(1, n_epochs + 1):\n",
    "        for x, t in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device) # 入力\n",
    "            t = t.to(device) # 正解\n",
    "            y = model(x) # 出力\n",
    "            loss = criterion(y, t) # 損失\n",
    "            loss.backward() # 逆伝播\n",
    "            optimizer.step() # パラメータ更新\n",
    "            prog.update(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "model = LanguageModel(n_vocab, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1-2/20: ######################################## 100% [00:00:27.19] loss: 6.33308 \n",
      "  3-4/20: ######################################## 100% [00:00:26.03] loss: 5.00505 \n",
      "  5-6/20: ######################################## 100% [00:00:26.09] loss: 4.45241 \n",
      "  7-8/20: ######################################## 100% [00:00:26.87] loss: 4.27896 \n",
      " 9-10/20: ######################################## 100% [00:00:26.53] loss: 4.21971 \n",
      "11-12/20: ######################################## 100% [00:00:26.81] loss: 4.18958 \n",
      "13-14/20: ######################################## 100% [00:00:27.00] loss: 4.17075 \n",
      "15-16/20: ######################################## 100% [00:00:26.42] loss: 4.15720 \n",
      "17-18/20: ######################################## 100% [00:00:27.13] loss: 4.14668 \n",
      "19-20/20: ######################################## 100% [00:00:27.73] loss: 4.13833 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, n_epochs=20, prog_unit=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 文章生成\n",
    "\n",
    "学習したモデルを用いて文章を生成する。\n",
    "\n",
    "学習させたモデルは、ある単語から次の単語を予測するモデルである。厳密には、ある単語IDを入力に取り、次の単語IDを出力するモデルである。  \n",
    "さらに厳密に言うと、出力は単語IDではなく確率分布である。この確率分布から次の単語IDをサンプリングすることで、次の単語を生成する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に文章を生成させてみる。ある単語からの次の単語の予測を繰り返すことで文章を生成する。初めの単語だけはこちらで指定する。  \n",
    "以下の条件を満たすまで単語の生成を続ける。\n",
    "- 単語数が指定した限界に達する\n",
    "- 句点が出力される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_id = sp.piece_to_id('。') # 句点のID\n",
    "\n",
    "def token_sampling(y: List[float]) -> int:\n",
    "    \"\"\"モデルの出力から単語をサンプリングする\"\"\"\n",
    "    probs = F.softmax(y, dim=-1) # 確率分布に変換\n",
    "    token, = random.choices(range(n_vocab), weights=probs) # サンプリング\n",
    "    return token\n",
    "\n",
    "def generate_sentence(\n",
    "    model: nn.Module,\n",
    "    start_word: str,\n",
    "    max_len: int = 30\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    token_id = sp.piece_to_id(start_word)\n",
    "    token_ids = [token_id]\n",
    "\n",
    "    # 終了条件を満たすまで単語を生成\n",
    "    while len(token_ids) <= max_len and token_id != end_id:\n",
    "\n",
    "        # 入力する単語IDをtensorに変換\n",
    "        x = torch.tensor(token_id).unsqueeze(0).to(device)\n",
    "\n",
    "        # 次の単語の確率分布を予測\n",
    "        y = model(x)[0]\n",
    "        token_id = token_sampling(y) # サンプリング\n",
    "        token_ids.append(token_id)\n",
    "\n",
    "    sentence = sp.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日知られている。\n",
      "地球とて楽文化大に敗れ準優勝でミッキーの親王、ピエロは1908号を得ることができる。\n",
      "科学は、有していたが、コンクールである。\n",
      "人情報番組タイトルの都合上半の間のミュンヘン美術学校、また、KACO、ローマの初期段階のコンスタンティウス1年に亡くなった船に対して、\n"
     ]
    }
   ],
   "source": [
    "start_words = ['今日', '地球', '科学', '人']\n",
    "for start_word in start_words:\n",
    "    print(generate_sentence(model, start_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マルコフモデル同様、直前の単語のみを予測に用いているため、不自然な文章が多く生成される。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 埋め込み層\n",
    "\n",
    "*Embedding Layer*\n",
    "\n",
    "NN内部における、単語IDに対する「one-hotベクトル化→線形変換」を行う部分をまとめて**埋め込み層**と表す。  \n",
    "そもそも、これらの処理は単語IDを受け取って対応するベクトルを出力すること=**単語のベクトル化**と同義である。自然言語処理の世界では単語のベクトル化を**単語の埋め込み**と表現する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線形変換を行う全結合層は各単語の埋め込み表現（単語ベクトル）を保有していることになる。それらは重みから確認できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2905, -0.1799, -0.4095, -0.4943,  0.1511],\n",
      "        [ 0.1365,  0.4748, -0.2256,  0.0019, -0.0497],\n",
      "        [ 0.1742,  0.2400,  0.0601,  0.5173, -0.5256]], grad_fn=<MmBackward0>) \n",
      "\n",
      "tensor([[-0.2905, -0.1799, -0.4095, -0.4943,  0.1511],\n",
      "        [ 0.1365,  0.4748, -0.2256,  0.0019, -0.0497],\n",
      "        [ 0.1742,  0.2400,  0.0601,  0.5173, -0.5256]],\n",
      "       grad_fn=<PermuteBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, n_vocab: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.fc = nn.Linear(n_vocab, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.one_hot(x, self.n_vocab).to(torch.float32)\n",
    "        h = self.fc(x)\n",
    "        return h\n",
    "\n",
    "embedding = Embedding(n_vocab=3, embed_dim=5)\n",
    "\n",
    "x = torch.tensor([0, 1, 2])\n",
    "h = embedding(x)\n",
    "\n",
    "print(h, '\\n') # 埋め込み層からの出力\n",
    "print(embedding.fc.weight.T) # 全結合層の重み"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyTorch`には`torch.nn.Embedding`というクラスが実装されているので、それを使うと良い。  \n",
    "[Embedding — PyTorch 2.0 documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, n_vocab, hidden_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(n_vocab, hidden_size), # Embedding層\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_vocab)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に、言語モデルの学習によって得られる単語ベクトルは良い埋め込み表現として機能することが知られている。単語の埋め込み表現を得ることを目的として言語モデルを学習させることもある。word2vecなどがこれに該当し、これについてはおまけの章で取り上げる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 特殊トークン\n",
    "\n",
    "言語モデルを扱う際、データを扱いやすくするために特殊なトークンを考えることがある。  \n",
    "以下に例を示す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Unknown*\n",
    "\n",
    "未知語を意味するトークン。  \n",
    "推論時に学習データに含まれなかった単語に出会ったときに対応できるようになる。学習データ内での出現回数が少ない単語も未知語として扱うことがある。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Begin of Sentence*\n",
    "\n",
    "文章の先頭を意味するトークン。  \n",
    "先程の例では初めの単語を与える必要があったが、このトークンを作ることで、モデルに「文章の初め」を伝えられるようになる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *End of Sentence*\n",
    "\n",
    "文章の終わりを意味するトークン。  \n",
    "先程の例では句点が出た場合に生成を止めたが、本来句点は文の終わりであって文章の終わりではない。このトークンを作ることでモデルが文章の終わりを伝えられるようになる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データの全ての文章の該当箇所（BOSであれば文章の初め、EOSであれば文章の終わり）にこれらのトークンを入れてから学習させることで、そのトークンの意味をモデルは理解する。  \n",
    "またモデルが直接触れるのはトークンではなくトークンIDなので、特殊トークンの名前は何でもいい。他のトークンと重複しないように括弧を付けたりすることが多い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentencepieceでは上記3つのトークンがID0, 1, 2にデフォルトで設定されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID0: <unk>\n",
      "ID1: <s>\n",
      "ID2: </s>\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(sp)\n",
    "print('ID0:', sp.id_to_piece(0)) # Unknown\n",
    "print('ID1:', sp.id_to_piece(1)) # Begin of Sentence\n",
    "print('ID2:', sp.id_to_piece(2)) # End of Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、トークン列にBOSとEOSを追加して学習させてみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_id = sp.bos_id() # BOSのID\n",
    "eos_id = sp.eos_id() # EOSのID\n",
    "for ids in data_ids:\n",
    "    ids.insert(0, bos_id) # 先頭にBOSを追加\n",
    "    ids.append(eos_id) # 末尾にEOSを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(data_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model = LanguageModel(n_vocab, 512).to(device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/10:                                            0% [00:00:00.12] loss: 8.24262 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/10: ######################################## 100% [00:00:13.23] loss: 7.01097 \n",
      " 2/10: ######################################## 100% [00:00:13.06] loss: 5.28967 \n",
      " 3/10: ######################################## 100% [00:00:12.94] loss: 4.94011 \n",
      " 4/10: ######################################## 100% [00:00:12.90] loss: 4.79164 \n",
      " 5/10: ######################################## 100% [00:00:13.05] loss: 4.71006 \n",
      " 6/10: ######################################## 100% [00:00:13.30] loss: 4.65820 \n",
      " 7/10: ######################################## 100% [00:00:13.09] loss: 4.61932 \n",
      " 8/10: ######################################## 100% [00:00:12.93] loss: 4.59368 \n",
      " 9/10: ######################################## 100% [00:00:12.90] loss: 4.57238 \n",
      "10/10: ######################################## 100% [00:00:12.94] loss: 4.55414 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, n_epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では文章を生成する。初めのトークンはBOSとし、EOSが出力されるまで生成を続ける。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_id = sp.unk_id() # UNKのID\n",
    "def token_sampling(y: List[float]) -> int:\n",
    "    y[unk_id] = -torch.inf # UNKがサンプリングされる確率を0にする\n",
    "    probs = F.softmax(y, dim=-1)\n",
    "    token, = random.choices(range(n_vocab), weights=probs)\n",
    "    return token\n",
    "\n",
    "def generate_sentence(model, max_len=50):\n",
    "    model.eval()\n",
    "    token_id = bos_id\n",
    "    token_ids = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        x = torch.tensor(token_id).unsqueeze(0).to(device)\n",
    "        y = model(x)[0]\n",
    "        token_id = token_sampling(y)\n",
    "        if token_id == eos_id: # EOSが生成されたら終了\n",
    "            break\n",
    "        token_ids.append(token_id)\n",
    "\n",
    "    sentence = sp.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "次々、県安芸吉田道場に入時だけのデモ・フーヒレンジャク出が作曲に各種手演技を世に晒された際は、歴史的な、過去の文字最も職した。そんな中郷ICを除いた\n",
      "EGKOblege)化した。20人の被害補償額の称しなが、韓国や、郊の難しい表現したほか、リーグ2連覇を果たし優勝を飾ったことも明日、単根を採択\n",
      "主翼前後年)の仕方のフェス・チャートでイギリス、10日刊ド Baby Day」と、fr)、亜鉛(Lstimm高山書に送船は掲載誌が、この庁舎や、代わりに\n",
      "現在の主要イベントの路線に転々とで3560mboard を用いてアウキャロバージアプリマラー「新しいスタイルし、1世はこの戦となり、「ビザンツ帝国の公共建造物から、「ドー講じ\n",
      ": 遊園設置され、現在は廃止時はこれまでに移動していた。現在は、それらの代表的な表現は積極的に描いている人により先行しやケードラボアレイノ岳ニ長ければるため、流階級を構成する\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(generate_sentence(model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
