{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習\n",
    "\n",
    "*Deep Learning*\n",
    "\n",
    "深層学習を用いて言語モデルを作成する。  \n",
    "ニューラルネットワークでマルコフモデルを作成し、深層学習における言語モデル実装の基礎を学ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import MeCab\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import vocab, build_vocab_from_iterator\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## データセット\n",
    "\n",
    "wiki40b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    ds = tfds.load('wiki40b/ja', split='test')\n",
    "ds = list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 89698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。',\n",
       " 'ライブドア社員であった初代代表取締役社長の山名真由によって企業内起業の形で創業。2005年に株式会社ライブドアから分割されて設立。かつてはライブドアホールディングス（現・LDH）の子会社であったが、ノンコア事業の整理にともない、株式会社ゲオ（現：株式会社ゲオホールディングス）に所有する全株式を譲渡し、同社の完全子会社となった。「ぽすれん」「ゲオ宅配レンタル」のオンラインDVD・CD・コミックレンタルサービス及び「GEO Online」と「ゲオアプリ」のアプリ・ウェブサイト運営の大きく分けて2事業を展開している。以前はDVD販売等のEコマースサービス「ぽすれんストア」、動画配信コンテンツ「ぽすれんBB」や電子書籍配信サービスの「GEO☆Books」事業も行っていた。オンラインDVDレンタル事業では会員数は10万人（2005年9月時点）。2006年5月よりCDレンタルを開始。同業他社には、カルチュア・コンビニエンス・クラブが運営する『TSUTAYA DISCAS』のほか、DMM.comが運営する『DMM.com オンラインDVDレンタル』がある。過去には「Yahoo!レンタルDVD」と「楽天レンタル」の運営を受託していた。',\n",
       " '2005年の一時期、東京のラジオ局、InterFMで、「堀江社長も使っているライブドアのぽすれん」というキャッチコピーでラジオCMを頻繁に行っていたことがあった。',\n",
       " '香川県内の農業協同組合の信用事業を統括する県域農協系金融機関であり、県内農業協同組合を会員とする。香川県は全県単一農協の香川県農業協同組合となったが、先に単一農協となった奈良県や沖縄県のケースと異なり、信連の統合は行われなかった。通称は「JA香川信連」または「JAバンク香川」。統一金融機関コードは3037。主に法人顧客を中心としており、個人取引は殆どない。県内の大型商業施設にある、他金融機関管理の共同ATMには香川信連の管轄のものがある。',\n",
       " '534年（永熙3年）、独孤信の子として生まれた。独孤信が父母妻子を捨てて長安に入ったため、独孤羅は東魏に取り残されて高氏の虜囚となった。独孤信が宇文護により処刑されると、ようやく釈放されて、中山に寓居した。北斉の独孤永業の一族として田宅を与えられた。北斉が滅亡し、楊堅が定州総管となると、楊堅の妻の独孤伽羅が兄の行方を捜索させて独孤羅を見つけ出し、初めて対面した。579年（大象元年）、功臣の子として楚安郡太守に任じられた。まもなく病のため辞職し、長安に帰った。580年（大象2年）、楊堅が北周の丞相となると、独孤羅は儀同大将軍の位を受け、楊堅の側近に仕えた。581年（開皇元年）、隋が建国されると、使持節・上開府・儀同大将軍の位を受けた。11月、右武衛将軍に転じた。582年（開皇2年）、父の趙国公の爵位を嗣いだ。592年（開皇12年）、大将軍・太子右衛率となった。593年（開皇13年）、涼州刺史に任じられた。597年（開皇17年）、涼州総管に任じられた。599年（開皇19年）2月6日、死去した。諡は徳といった。']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for sample in ds:\n",
    "    text = sample['text'].decode()\n",
    "    sections = text.split('_START_SECTION_')\n",
    "    for section in sections[1:]:\n",
    "        sentence = section.split('_START_PARAGRAPH_')[1]\n",
    "        sentence = sentence.replace('_NEWLINE_', '')\n",
    "        sentence = sentence.replace('\\n', '')\n",
    "        data.append(sentence)\n",
    "\n",
    "print('num of data:', len(data))\n",
    "data[:5] # examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多すぎるので減らす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 前処理\n",
    "\n",
    "テキストをNNで扱える形に変換する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークン化\n",
    "\n",
    "文章をトークンごとに分割する。トークンとは文章を構成する最小の単位で、単語や句読点などが該当する。  \n",
    "トークン化は分かち書きと似た意味であるが、言語モデルの領域で最小単位をトークンと呼ぶことが一般的なことや、単純にトークン化と呼ぶことが多いことから、本節以降トークン化と呼ぶ。\n",
    "\n",
    "トークン化には、分かち書き同様形態素解析を用いる。各文章をトークンのリストとして格納する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['「', '教科', '書', 'に', 'は', '決して', '載ら', 'ない', '」', '日本']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokens = []\n",
    "tagger = MeCab.Tagger('-Owakati')\n",
    "for sentence in data:\n",
    "    tokens = tagger.parse(sentence).strip().split(' ') # トークンのリスト\n",
    "    data_tokens.append(tokens)\n",
    "\n",
    "data_tokens[0][:10] # example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID化\n",
    "\n",
    "NNでは文字列を扱えないので、トークン1つ1つにIDを割り当てる。\n",
    "\n",
    "<br>\n",
    "\n",
    "torchtextの`vocab`モジュールを使用する。\n",
    "\n",
    "[torchtext.vocab — Torchtext 0.15.0 documentation](https://pytorch.org/text/stable/vocab.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab, build_vocab_from_iterator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語とその出現頻度を表した辞書を入力し、`Vocab`オブジェクトを作成する。`min_freq`で指定した出現頻度以下の単語は無視される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = vocab({'私': 4, 'りんご': 1, '食べる': 2, '好き': 3}, min_freq=1)\n",
    "v(['私', 'りんご', '好き'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_vocab_from_iterator()`でトークンのリスト（のリスト）から`vocab`オブジェクトを作成できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['は', '私', 'が', 'です', 'りんご', 'を', 'バナナ', '今日', '好き', '晴れ', '食べる']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = build_vocab_from_iterator([\n",
    "    ['私', 'は', 'りんご', 'が', '好き'],\n",
    "    ['私', 'は', 'バナナ', 'を', '食べる'],\n",
    "    ['今日', 'は', '晴れ', 'です']\n",
    "])\n",
    "v.get_itos() # vocabrary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを使ってトークン列をID列に変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary: 4602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[17, 3681, 3742, 2, 4, 3873, 4415, 47, 19, 70]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ids = []\n",
    "v = build_vocab_from_iterator(data_tokens)\n",
    "for tokens in data_tokens:\n",
    "    data_ids.append(v(tokens))\n",
    "\n",
    "print('num of vocabrary:', len(v))\n",
    "data_ids[0][:10] # example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習データ\n",
    "\n",
    "NNの学習を行うため、入力と出力のペアを作成する。  \n",
    "今回はある単語から次に続く単語を予測するモデルを作成するので、ある単語IDとその次の単語IDがペアとなったデータを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 21128\n",
      "tensor([  18, 1062,   22, 1061,  243, 2584,  216,   23,  137,   64,   18, 4524,\n",
      "         933, 4482, 1559,  912, 2110, 3530, 1130, 1377,   14,    0, 3499, 1982,\n",
      "         639,    7, 1159,  513,  352,   19,    4,   57,  827, 3659,   10, 3084,\n",
      "         199,   23,    4,    4,    0,   11,  759,    1,   19, 1942, 1609,    1,\n",
      "           1,   16,    0,    2,   10,  271,   18, 1023,  138, 2630,  511,   13,\n",
      "        1317, 3191,   19, 1604])\n",
      "tensor([   3,   12, 2623,   12, 2127,   22,  567,  513, 3125,    8,    3,   10,\n",
      "           8,    0, 1452,  794,  279,    2,   20,    9,   13,  127,    6,   28,\n",
      "          90, 3894,   35,  712,  256,    7,    1,  145,    5,   14, 1441,   11,\n",
      "           9,  883,    1,   39, 3399,    6,    4, 2624,    7,  134,  139, 2290,\n",
      "          39,    4,   35,  347, 3548,  299,    3,    0,    0,   15,    4,    6,\n",
      "           2,    3,    7,    3])\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids):\n",
    "        x, y = [], []\n",
    "        for ids in data_ids:\n",
    "            for id1, id2 in zip(ids[:-1], ids[1:]):\n",
    "                x.append(id1)\n",
    "                y.append(id2)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.n_data = len(x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "batch_size = 64\n",
    "dataset = TextDataset(data_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "n_data = len(dataset)\n",
    "print('num of data:', n_data)\n",
    "\n",
    "# examples\n",
    "x, y = next(iter(dataloader))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "## モデル構築\n",
    "\n",
    "マルコフモデルはある状態から次の状態を予測する。言語モデルに当てはめると、ある単語から次の単語を予測する。  \n",
    "これをNNで実装するので、ある単語IDを入力に取り、次の単語IDを出力するモデルを作成する。\n",
    "\n",
    "単語IDはカテゴリ変数なので、入力時はone-hotベクトルに変換する。出力は語彙数分の次元を持つベクトルを出力し、単語の分類問題として扱う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(vocab_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.one_hot(x, self.vocab_size).to(torch.float32)\n",
    "        y = self.net(x)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習\n",
    "\n",
    "通常の分類モデルと同じように学習する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "def train(model, optimizer, dataloader, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_epoch = 0\n",
    "        for x, t in tqdm(dataloader, desc=f'{epoch}/{n_epochs}', leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = model(x)\n",
    "            loss = criterion(y, t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_epoch += loss.item()\n",
    "        loss = loss_epoch/len(dataloader)\n",
    "        print(f'{epoch}/{n_epochs} loss: {loss}', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(len(v), 512).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20 loss: 6.617839218266421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/20 loss: 5.613264075218731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/20 loss: 5.059253672458614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/20 loss: 4.548879970596636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/20 loss: 4.112352208428512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/20 loss: 3.788919505035769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/20 loss: 3.5320494059709624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/20 loss: 3.3323206123628646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/20 loss: 3.170776387355839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/20 loss: 3.0450277141213777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/20 loss: 2.942641295695233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/20 loss: 2.863284314867233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/20 loss: 2.805458122149695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/20 loss: 2.7600872217708483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/20 loss: 2.734091659687077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/20 loss: 2.7113829447783733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/20 loss: 2.702974404093002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/20 loss: 2.698480599236272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/20 loss: 2.6884260206424218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 loss: 2.686243463859097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, dataloader, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 文章生成\n",
    "\n",
    "学習したモデルを用いて文章を生成する。\n",
    "\n",
    "学習させたモデルは、ある単語から次の単語を予測するモデルである。厳密には、ある単語IDを入力に取り、次の単語IDを出力するモデルである。  \n",
    "さらに厳密に言うと、出力は単語IDではなく確率分布である。この確率分布から次の単語IDをサンプリングすることで、次の単語を生成する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に文章を生成させてみる。ある単語からの次の単語の予測を繰り返すことで文章を生成する。初めの単語だけはこちらで指定する。  \n",
    "以下の条件を満たすまで単語の生成を続ける。\n",
    "- 単語数が指定した限界に達する\n",
    "- 読点が出力される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token_id = v.get_stoi()['。'] # 読点のID\n",
    "\n",
    "def generate_sentence(model, start_word, max_len=30):\n",
    "    model.eval()\n",
    "    token_id = v.get_stoi()[start_word]\n",
    "    token_ids = [token_id]\n",
    "\n",
    "    # 終了条件を満たすまで単語を生成\n",
    "    while len(token_ids) > max_len and token_id != end_token_id:\n",
    "\n",
    "        # 入力する単語IDをtensorに変換\n",
    "        x = torch.tensor(token_id).unsqueeze(0).to(device)\n",
    "\n",
    "        # 次の単語の確率分布を予測\n",
    "        y = model(x)[0]\n",
    "        y = F.softmax(y, dim=0)\n",
    "\n",
    "        # 分布に基づいたサンプリングを行う\n",
    "        token_id = random.choices(range(len(y)), weights=y)[0]\n",
    "        token_ids.append(token_id)\n",
    "\n",
    "    tokens = [v.get_itos()[i] for i in token_ids]\n",
    "    sentence = ''.join(tokens)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日\n",
      "地球\n",
      "科学\n",
      "人\n"
     ]
    }
   ],
   "source": [
    "start_words = ['今日', '地球', '科学', '人']\n",
    "for start_word in start_words:\n",
    "    print(generate_sentence(model, start_word))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 特殊トークン\n",
    "\n",
    "言語モデルを扱う際、データを扱いやすくするために特殊なトークンを考えることがある。  \n",
    "以下に例を示す。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOS\n",
    "\n",
    "*Begin of Sentence*\n",
    "\n",
    "文章の先頭を意味するトークン。  \n",
    "先程の例では初めの単語を与える必要があったが、このトークンを作ることで、モデルに「文章の初め」を伝えられるようになる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOS\n",
    "\n",
    "*End of Sentence*\n",
    "\n",
    "文章の終わりを意味するトークン。  \n",
    "先程の例では読点が出た場合に生成を止めたが、本来読点は文の終わりであって文章の終わりではない。このトークンを作ることでモデルが文章の終わりを伝えられるようになる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データの全ての文章の該当箇所（BOSであれば文章の初め、EOSであれば文章の終わり）にこれらのトークンを入れてから学習させることで、そのトークンの意味をモデルは理解する。  \n",
    "またモデルが触れるのはトークンではなくトークンIDなので、特殊トークンの名前は何でもいい。\n",
    "\n",
    "<br>\n",
    "\n",
    "では特殊トークンを使って学習させてみよう。使う特殊トークン上の二つ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['「', '教科', '書', 'に', 'は', '決して', '載ら', 'ない', '」', '日本']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokens[0][:10] # example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全ての文章にBOSとEOSを追加する。  \n",
    "他のトークンと重複しないような文字列を指定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS>', '「', '教科', '書', 'に', 'は', '決して', '載ら', 'ない', '」']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOS, EOS = '<BOS>', '<EOS>'\n",
    "for tokens in data_tokens:\n",
    "    tokens.insert(0, BOS)\n",
    "    tokens.append(EOS)\n",
    "\n",
    "data_tokens[0][:10] # example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "残りの学習は同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids = []\n",
    "v = build_vocab_from_iterator(data_tokens)\n",
    "for tokens in data_tokens:\n",
    "    data_ids.append(v(tokens))\n",
    "\n",
    "dataset = TextDataset(data_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(len(v), 512).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20 loss: 6.617031986841899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/20 loss: 5.612874150990012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/20 loss: 5.077422079211938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/20 loss: 4.557763280982742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/20 loss: 4.119269445270835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/20 loss: 3.787524982840715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/20 loss: 3.5315082401572586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/20 loss: 3.329807532761625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/20 loss: 3.177276020278474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/20 loss: 3.0457069613262564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/20 loss: 2.946605648823127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/20 loss: 2.8705885563781877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/20 loss: 2.816201899579899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/20 loss: 2.771974564669375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/20 loss: 2.741800141191768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/20 loss: 2.722364569495538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/20 loss: 2.711752150230065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/20 loss: 2.7037228617125644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/20 loss: 2.6991396296524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 loss: 2.6947849710544425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, dataloader, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では文章を生成する。初めのトークンはBOSとし、EOSが出力されるまで生成を続ける。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token_id = v.get_stoi()[EOS]\n",
    "\n",
    "def generate_sentence(model, max_len=50):\n",
    "    model.eval()\n",
    "    token_id = v.get_stoi()[BOS]\n",
    "    token_ids = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        x = torch.tensor(token_id).unsqueeze(0).to(device)\n",
    "        y = model(x)[0]\n",
    "        y = F.softmax(y, dim=0)\n",
    "        token_id = random.choices(range(len(y)), weights=y)[0]\n",
    "        if token_id == end_token_id:\n",
    "            break\n",
    "        token_ids.append(token_id)\n",
    "\n",
    "    tokens = [v.get_itos()[i] for i in token_ids]\n",
    "    sentence = ''.join(tokens)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "艦サーモンはVSO型である。同じでは殆どの行方は、父は仕事。13時間は「ニュースでは5回（桃井はるこ、キャラクター・LDH）が発生するとなって挙げられることとして\n",
      "養成所付近に葬られた4人以外では次々という位置づけられた（1885年大会である。2002年から2015年による演奏にはチトラドゥルガ陥落のサム・サン＝までがみー艦隊司令官\n",
      "マサイ語に移ったために住み、2000億円→同系統の廃止されていた。また、月曜の高氏のホイサラとしていた。これらの主砲を行なったブラウンシュヴァイク＝ピエール・育成\n",
      "武蔵野美術教師を周知させてサービス開始。だ。カストゥーリ・ランガッパ・ナーヤカ2世（大舘はPS版と再会しており、トライアウトに配置された。1990年）、全国少年少女戦士たちに就けられた改修\n",
      "メイン練習時間は優柔不断で表された料が花束嬢の歌手で、2010KZ₃₉のユーロビジョンになったが育成に使うことが、ISO3105、2009年と楽曲の記録した。『さらば』の\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(generate_sentence(model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
