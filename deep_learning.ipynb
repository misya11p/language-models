{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習\n",
    "\n",
    "*Deep Learning*\n",
    "\n",
    "深層学習を用いて言語モデルを作成する。  \n",
    "ニューラルネットワークでマルコフモデルを作成し、深層学習における言語モデル実装の基礎を学ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## データセット\n",
    "\n",
    "Wikipediaの記事を集めた**wiki40b**というデータセットを使用する。  \n",
    "Tensorflow Datasetsに収録されているものを使用する。\n",
    "\n",
    "[wiki40b  |  TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/wiki40b?hl=en#wiki40bja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "ds = tfds.load('wiki40b/ja', split='test')\n",
    "ds = list(ds.as_numpy_iterator())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の様な辞書がまとまっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': b'\\n_START_ARTICLE_\\n\\xe3\\x83\\x93\\xe3\\x83\\xbc\\xe3\\x83\\x88\\xe3\\x81\\x9f\\xe3\\x81\\x91\\xe3\\x81\\x97\\xe3\\x81\\xae\\xe6\\x95\\x99\\xe7\\xa7\\x91\\xe6\\x9b\\xb8\\xe3\\x81\\xab\\xe8\\xbc\\x89\\xe3\\x82\\x89\\xe3\\x81\\xaa\\xe3\\x81\\x84\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe4\\xba\\xba\\xe3\\x81\\xae\\xe8\\xac\\x8e\\n_START_SECTION_\\n\\xe6\\xa6\\x82\\xe8\\xa6\\x81\\n_START_PARAGRAPH_\\n\\xe3\\x80\\x8c\\xe6\\x95\\x99\\xe7\\xa7\\x91\\xe6\\x9b\\xb8\\xe3\\x81\\xab\\xe3\\x81\\xaf\\xe6\\xb1\\xba\\xe3\\x81\\x97\\xe3\\x81\\xa6\\xe8\\xbc\\x89\\xe3\\x82\\x89\\xe3\\x81\\xaa\\xe3\\x81\\x84\\xe3\\x80\\x8d\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe4\\xba\\xba\\xe3\\x81\\xae\\xe8\\xac\\x8e\\xe3\\x82\\x84\\xe3\\x81\\x97\\xe3\\x81\\x8d\\xe3\\x81\\x9f\\xe3\\x82\\x8a\\xe3\\x82\\x92\\xe5\\xa4\\x9a\\xe8\\xa7\\x92\\xe7\\x9a\\x84\\xe3\\x81\\xab\\xe6\\xa4\\x9c\\xe8\\xa8\\xbc\\xe3\\x81\\x97\\xe3\\x80\\x81\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe4\\xba\\xba\\xe3\\x81\\xaeDNA\\xe3\\x82\\x92\\xe8\\xa7\\xa3\\xe6\\x98\\x8e\\xe3\\x81\\x99\\xe3\\x82\\x8b\\xe3\\x80\\x82_NEWLINE_\\xe6\\x96\\xb0\\xe6\\x98\\xa5\\xe7\\x95\\xaa\\xe7\\xb5\\x84\\xe3\\x81\\xa8\\xe3\\x81\\x97\\xe3\\x81\\xa6\\xe5\\xae\\x9a\\xe6\\x9c\\x9f\\xe7\\x9a\\x84\\xe3\\x81\\xab\\xe6\\x94\\xbe\\xe9\\x80\\x81\\xe3\\x81\\x95\\xe3\\x82\\x8c\\xe3\\x81\\xa6\\xe3\\x81\\x8a\\xe3\\x82\\x8a\\xe3\\x80\\x81\\xe5\\xb9\\xb4\\xe6\\x9c\\xab\\xe3\\x81\\xae\\xe5\\x8d\\x88\\xe5\\x89\\x8d\\xe4\\xb8\\xad\\xe3\\x81\\xab\\xe5\\x86\\x8d\\xe6\\x94\\xbe\\xe9\\x80\\x81\\xe3\\x81\\x95\\xe3\\x82\\x8c\\xe3\\x82\\x8b\\xe3\\x81\\xae\\xe3\\x81\\x8c\\xe6\\x81\\x92\\xe4\\xbe\\x8b\\xe3\\x81\\xa8\\xe3\\x81\\xaa\\xe3\\x81\\xa3\\xe3\\x81\\xa6\\xe3\\x81\\x84\\xe3\\x82\\x8b\\xe3\\x80\\x82',\n",
       " 'version_id': b'1848243370795951995',\n",
       " 'wikidata_id': b'Q11331136'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = ds[0]\n",
    "ex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストの中身はこんな感じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_START_ARTICLE_\n",
      "ビートたけしの教科書に載らない日本人の謎\n",
      "_START_SECTION_\n",
      "概要\n",
      "_START_PARAGRAPH_\n",
      "「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。_NEWLINE_新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。\n"
     ]
    }
   ],
   "source": [
    "print(ex['text'].decode())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "セクションごとにまとめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 89698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。',\n",
       " 'ライブドア社員であった初代代表取締役社長の山名真由によって企業内起業の形で創業。2005年に株式会社ライブドアから分割されて設立。かつてはライブドアホールディングス（現・LDH）の子会社であったが、ノンコア事業の整理にともない、株式会社ゲオ（現：株式会社ゲオホールディングス）に所有する全株式を譲渡し、同社の完全子会社となった。「ぽすれん」「ゲオ宅配レンタル」のオンラインDVD・CD・コミックレンタルサービス及び「GEO Online」と「ゲオアプリ」のアプリ・ウェブサイト運営の大きく分けて2事業を展開している。以前はDVD販売等のEコマースサービス「ぽすれんストア」、動画配信コンテンツ「ぽすれんBB」や電子書籍配信サービスの「GEO☆Books」事業も行っていた。オンラインDVDレンタル事業では会員数は10万人（2005年9月時点）。2006年5月よりCDレンタルを開始。同業他社には、カルチュア・コンビニエンス・クラブが運営する『TSUTAYA DISCAS』のほか、DMM.comが運営する『DMM.com オンラインDVDレンタル』がある。過去には「Yahoo!レンタルDVD」と「楽天レンタル」の運営を受託していた。',\n",
       " '2005年の一時期、東京のラジオ局、InterFMで、「堀江社長も使っているライブドアのぽすれん」というキャッチコピーでラジオCMを頻繁に行っていたことがあった。',\n",
       " '香川県内の農業協同組合の信用事業を統括する県域農協系金融機関であり、県内農業協同組合を会員とする。香川県は全県単一農協の香川県農業協同組合となったが、先に単一農協となった奈良県や沖縄県のケースと異なり、信連の統合は行われなかった。通称は「JA香川信連」または「JAバンク香川」。統一金融機関コードは3037。主に法人顧客を中心としており、個人取引は殆どない。県内の大型商業施設にある、他金融機関管理の共同ATMには香川信連の管轄のものがある。',\n",
       " '534年（永熙3年）、独孤信の子として生まれた。独孤信が父母妻子を捨てて長安に入ったため、独孤羅は東魏に取り残されて高氏の虜囚となった。独孤信が宇文護により処刑されると、ようやく釈放されて、中山に寓居した。北斉の独孤永業の一族として田宅を与えられた。北斉が滅亡し、楊堅が定州総管となると、楊堅の妻の独孤伽羅が兄の行方を捜索させて独孤羅を見つけ出し、初めて対面した。579年（大象元年）、功臣の子として楚安郡太守に任じられた。まもなく病のため辞職し、長安に帰った。580年（大象2年）、楊堅が北周の丞相となると、独孤羅は儀同大将軍の位を受け、楊堅の側近に仕えた。581年（開皇元年）、隋が建国されると、使持節・上開府・儀同大将軍の位を受けた。11月、右武衛将軍に転じた。582年（開皇2年）、父の趙国公の爵位を嗣いだ。592年（開皇12年）、大将軍・太子右衛率となった。593年（開皇13年）、涼州刺史に任じられた。597年（開皇17年）、涼州総管に任じられた。599年（開皇19年）2月6日、死去した。諡は徳といった。']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for sample in ds:\n",
    "    text = sample['text'].decode()\n",
    "    sections = text.split('_START_SECTION_')\n",
    "    for section in sections[1:]:\n",
    "        sentence = section.split('_START_PARAGRAPH_')[1]\n",
    "        sentence = sentence.replace('_NEWLINE_', '')\n",
    "        sentence = sentence.replace('\\n', '')\n",
    "        data.append(sentence)\n",
    "\n",
    "print('num of data:', len(data))\n",
    "data[:5] # examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 前処理\n",
    "\n",
    "テキストをNNで扱える形に変換する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分かち書き\n",
    "\n",
    "テキストを単語ごとに分割する。英語は元から単語ごとに分割されているが、日本語はそうではないので別途行う必要がある。  \n",
    "分かち書きには形態素解析器を使用する。色々な種類があるが、ここでは**MeCab**を使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'私 は 猫 が 好き です 。 \\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = MeCab.Tagger('-Owakati') # 出力形式を分かち書きに指定\n",
    "result = tagger.parse('私は猫が好きです。')\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを使って、学習データの全てを単語ごとに分割する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['「', '教科', '書', 'に', 'は', '決して', '載ら', 'ない', '」', '日本']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def separation(data):\n",
    "    new_data = []\n",
    "    for sentence in data:\n",
    "        words = tagger.parse(sentence).strip().split()\n",
    "        new_data.append(words)\n",
    "    return new_data\n",
    "\n",
    "data = separation(data)\n",
    "data[0][:10] # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
