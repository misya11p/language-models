{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習を用いた言語モデル\n",
    "\n",
    "*Deep Learning*\n",
    "\n",
    "深層学習を用いて言語モデルを作成する。  \n",
    "ニューラルネットワークを用いて、ある単語から次の単語を予測するモデルを作成し、深層学習を活用した言語モデル実装の基礎を学ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import MeCab\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import vocab, build_vocab_from_iterator\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## データセット\n",
    "\n",
    "wiki40b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    ds = tfds.load('wiki40b/ja', split='test')\n",
    "ds = list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 89698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。',\n",
       " 'ライブドア社員であった初代代表取締役社長の山名真由によって企業内起業の形で創業。2005年に株式会社ライブドアから分割されて設立。かつてはライブドアホールディングス（現・LDH）の子会社であったが、ノンコア事業の整理にともない、株式会社ゲオ（現：株式会社ゲオホールディングス）に所有する全株式を譲渡し、同社の完全子会社となった。「ぽすれん」「ゲオ宅配レンタル」のオンラインDVD・CD・コミックレンタルサービス及び「GEO Online」と「ゲオアプリ」のアプリ・ウェブサイト運営の大きく分けて2事業を展開している。以前はDVD販売等のEコマースサービス「ぽすれんストア」、動画配信コンテンツ「ぽすれんBB」や電子書籍配信サービスの「GEO☆Books」事業も行っていた。オンラインDVDレンタル事業では会員数は10万人（2005年9月時点）。2006年5月よりCDレンタルを開始。同業他社には、カルチュア・コンビニエンス・クラブが運営する『TSUTAYA DISCAS』のほか、DMM.comが運営する『DMM.com オンラインDVDレンタル』がある。過去には「Yahoo!レンタルDVD」と「楽天レンタル」の運営を受託していた。',\n",
       " '2005年の一時期、東京のラジオ局、InterFMで、「堀江社長も使っているライブドアのぽすれん」というキャッチコピーでラジオCMを頻繁に行っていたことがあった。',\n",
       " '香川県内の農業協同組合の信用事業を統括する県域農協系金融機関であり、県内農業協同組合を会員とする。香川県は全県単一農協の香川県農業協同組合となったが、先に単一農協となった奈良県や沖縄県のケースと異なり、信連の統合は行われなかった。通称は「JA香川信連」または「JAバンク香川」。統一金融機関コードは3037。主に法人顧客を中心としており、個人取引は殆どない。県内の大型商業施設にある、他金融機関管理の共同ATMには香川信連の管轄のものがある。',\n",
       " '534年（永熙3年）、独孤信の子として生まれた。独孤信が父母妻子を捨てて長安に入ったため、独孤羅は東魏に取り残されて高氏の虜囚となった。独孤信が宇文護により処刑されると、ようやく釈放されて、中山に寓居した。北斉の独孤永業の一族として田宅を与えられた。北斉が滅亡し、楊堅が定州総管となると、楊堅の妻の独孤伽羅が兄の行方を捜索させて独孤羅を見つけ出し、初めて対面した。579年（大象元年）、功臣の子として楚安郡太守に任じられた。まもなく病のため辞職し、長安に帰った。580年（大象2年）、楊堅が北周の丞相となると、独孤羅は儀同大将軍の位を受け、楊堅の側近に仕えた。581年（開皇元年）、隋が建国されると、使持節・上開府・儀同大将軍の位を受けた。11月、右武衛将軍に転じた。582年（開皇2年）、父の趙国公の爵位を嗣いだ。592年（開皇12年）、大将軍・太子右衛率となった。593年（開皇13年）、涼州刺史に任じられた。597年（開皇17年）、涼州総管に任じられた。599年（開皇19年）2月6日、死去した。諡は徳といった。']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for sample in ds:\n",
    "    text = sample['text'].decode()\n",
    "    sections = text.split('_START_SECTION_')\n",
    "    for section in sections[1:]:\n",
    "        sentence = section.split('_START_PARAGRAPH_')[1]\n",
    "        sentence = sentence.replace('_NEWLINE_', '')\n",
    "        sentence = sentence.replace('\\n', '')\n",
    "        data.append(sentence)\n",
    "\n",
    "print('num of data:', len(data))\n",
    "data[:5] # examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多すぎるので減らす。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらを一つの.txtファイルに書き出す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = 'data/jawiki_small.txt'\n",
    "with open(textfile, 'w') as f:\n",
    "    for sentence in data:\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 前処理\n",
    "\n",
    "テキストをNNで扱える形に変換する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークン化\n",
    "\n",
    "文章をトークンごとに分割する。トークンとは文章を構成する最小の単位で、単語や句読点などが該当する。  \n",
    "トークン化は分かち書きと似た意味であるが、言語モデルの領域で最小単位をトークンと呼ぶことが一般的なことや、単純にトークン化と呼ぶことが多いことから、トークン化と呼ぶ。\n",
    "\n",
    "トークン化にはいくつかのやり方がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 簡単な手法\n",
    "\n",
    "いくつかの簡単な手法を紹介する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えば英語の場合、スペースで区切ればトークン化が完了する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'a', 'student']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I am a student'\n",
    "tokens = text.split(' ')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語の場合は文章がスペースで区切られていないので、別の手法が必要。\n",
    "\n",
    "日本語の場合、形態素解析器を用いることでトークン化が可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['今日', 'は', 'いい', '天気', 'だ', '\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '今日はいい天気だ'\n",
    "tagger = MeCab.Tagger('-Owakati')\n",
    "tokens = tagger.parse(text).split(' ')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### サブワード分割\n",
    "\n",
    "データセットから頻出する文字の並びを学習し、その並びをトークンとして分割する。\n",
    "\n",
    "形態素解析器を用いたやり方は、辞書が必要なことやトークン数が多くなるなどの問題がある。  \n",
    "サブワード分割はデータセットに合ったトークン化を実現する。語彙数を指定することも可能。指定した語彙数に収まるまで細かく分割する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command:     --input=data/jawiki_small.txt     --model_prefix=models/jawiki_tokenizer     --vocab_size=8000 \n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/jawiki_small.txt\n",
      "  input_format: \n",
      "  model_prefix: models/jawiki_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: data/jawiki_small.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (4385 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 962 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 38 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=336845\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=2732\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 962 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=108829\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 69425 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 962\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 2495\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 2495 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=35379 obj=459.93 num_tokens=135966 num_tokens/piece=3.84313\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=32482 obj=424.671 num_tokens=136673 num_tokens/piece=4.20765\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=24298 obj=435.558 num_tokens=145057 num_tokens/piece=5.96992\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=24250 obj=430.808 num_tokens=145341 num_tokens/piece=5.99344\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=18181 obj=452.067 num_tokens=156285 num_tokens/piece=8.59606\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=18174 obj=446.56 num_tokens=156454 num_tokens/piece=8.60867\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=13630 obj=471.789 num_tokens=168144 num_tokens/piece=12.3363\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=13627 obj=466.562 num_tokens=168279 num_tokens/piece=12.3489\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=10220 obj=494.222 num_tokens=181529 num_tokens/piece=17.7621\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=10220 obj=489.188 num_tokens=181530 num_tokens/piece=17.7622\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=503.294 num_tokens=188309 num_tokens/piece=21.3987\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=501.059 num_tokens=188309 num_tokens/piece=21.3987\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: models/jawiki_tokenizer.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: models/jawiki_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "model_prefix = 'models/jawiki_tokenizer' # モデルファイルpath\n",
    "vocab_size = 8000 # 語彙数\n",
    "spm.SentencePieceTrainer.Train(f'''\\\n",
    "    --input={textfile} \\\n",
    "    --model_prefix={model_prefix} \\\n",
    "    --vocab_size={vocab_size} \\\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '今日', 'は', 'いい', '天', '気', 'だ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.Load(f'{model_prefix}.model') # モデル読み込み\n",
    "\n",
    "text = '今日はいい天気だ'\n",
    "tokens = s.EncodeAsPieces(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '「', '教科書', 'には', '決', 'して', '載', 'ら', 'ない', '」']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokens = []\n",
    "for sentence in data:\n",
    "    tokens = s.EncodeAsPieces(sentence)\n",
    "    data_tokens.append(tokens)\n",
    "data_tokens[0][:10] # example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID化\n",
    "\n",
    "NNでは文字列を扱えないので、各トークンにIDとして整数値を割り当てる。\n",
    "\n",
    "torchtextの`vocab`モジュールを使用する。  \n",
    "[torchtext.vocab — Torchtext 0.15.0 documentation](https://pytorch.org/text/stable/vocab.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab, build_vocab_from_iterator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語とその出現頻度を表した辞書を入力し、`Vocab`オブジェクトを作成する。`min_freq`で指定した出現頻度以下の単語は無視される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = vocab({'私': 4, 'りんご': 1, '食べる': 2, '好き': 3}, min_freq=1)\n",
    "v(['私', 'りんご', '好き'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_vocab_from_iterator()`でトークンのリスト（のリスト）から`vocab`オブジェクトを作成できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['は', '私', 'が', 'です', 'りんご', 'を', 'バナナ', '今日', '好き', '晴れ', '食べる']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = build_vocab_from_iterator([\n",
    "    ['私', 'は', 'りんご', 'が', '好き'],\n",
    "    ['私', 'は', 'バナナ', 'を', '食べる'],\n",
    "    ['今日', 'は', '晴れ', 'です']\n",
    "])\n",
    "v.get_itos() # vocabrary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを使ってトークン列をID列に変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary: 8216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8, 16, 6920, 49, 734, 60, 1721, 74, 125, 14]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ids = []\n",
    "v = build_vocab_from_iterator(data_tokens)\n",
    "for tokens in data_tokens:\n",
    "    data_ids.append(v(tokens))\n",
    "\n",
    "n_vocab = len(v)\n",
    "print('num of vocabrary:', n_vocab)\n",
    "data_ids[0][:10] # example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習データ\n",
    "\n",
    "NNの学習を行うため、入力と出力のペアを作成する。  \n",
    "今回はある単語から次に続く単語を予測するモデルを作成するので、ある単語IDとその次の単語IDがペアとなったデータを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 243959\n",
      "tensor([  94,   32,  473, 7946,    2,  604,  581, 8074,   51,   12,   46,    0,\n",
      "          30,  315,    9, 3213,  369, 6540,    3, 6367,  299,  789,  373,  307,\n",
      "        1276, 1096,  140,  616,    0,  274, 1429,   24,  343,   13, 1321, 1521,\n",
      "          85,   37,   12,  501, 1278,  913,  986,    2, 4215,   29,   59,  103,\n",
      "        1992,   42,    3,  321,    2, 3107, 2466, 3923,    7,  346,    3,  779,\n",
      "           1, 1061,  970,  120])\n",
      "tensor([ 403,    0,   74, 1299,   37, 2024, 6075,   13,   10,  100,  139,  504,\n",
      "           2,    3, 1730,   29,  523,   65,  389,  129,    0, 1737,   33,    2,\n",
      "           1, 3242,    6,  625,  363,    2,    6, 2371,  160,  182,  440, 1724,\n",
      "           4, 1944, 4663,    6,  104, 4041,  747, 5013,    0, 2493,  578,    1,\n",
      "        2637,  833,    0, 4403,  314, 1785, 3118, 2994,  147,  820, 4471,    5,\n",
      "         847,   60, 4509,  260])\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids):\n",
    "        x, y = [], []\n",
    "        for ids in data_ids:\n",
    "            for in_id, out_id in zip(ids[:-1], ids[1:]):\n",
    "                x.append(in_id)\n",
    "                y.append(out_id)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.n_data = len(x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "batch_size = 64\n",
    "dataset = TextDataset(data_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "n_data = len(dataset)\n",
    "print('num of data:', n_data)\n",
    "\n",
    "# examples\n",
    "x, y = next(iter(dataloader))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築\n",
    "\n",
    "ある単語IDを入力に取り、次の単語IDを出力するモデルを作成する。\n",
    "\n",
    "単語IDはカテゴリ変数なので、入力時はone-hotベクトルに変換する。出力は語彙数分の次元を持つベクトルとする。つまりこのタスクは単語の分類問題となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, n_vocab, hidden_size):\n",
    "        super().__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_vocab, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_vocab)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.one_hot(x, self.n_vocab).to(torch.float32)\n",
    "        y = self.net(x)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習\n",
    "\n",
    "損失関数に交差エントロピーを設定し、通常の分類モデルと同じように学習する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    model.train()\n",
    "    prog.start(n_iter=len(dataloader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(1, n_epochs + 1):\n",
    "        for x, t in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = model(x)\n",
    "            loss = criterion(y, t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prog.update(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "model = LanguageModel(n_vocab, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1-5/50: ######################################## 100% [00:01:06.06] loss: 5.44917 \n",
      " 6-10/50: ######################################## 100% [00:01:03.77] loss: 4.26689 \n",
      "11-15/50: ######################################## 100% [00:01:05.23] loss: 4.16678 \n",
      "16-20/50: ######################################## 100% [00:01:04.70] loss: 4.13529 \n",
      "21-25/50: ######################################## 100% [00:01:06.06] loss: 4.11756 \n",
      "26-30/50: ######################################## 100% [00:01:06.80] loss: 4.10591 \n",
      "31-35/50: ######################################## 100% [00:01:04.76] loss: 4.09769 \n",
      "36-40/50: ######################################## 100% [00:01:05.08] loss: 4.09151 \n",
      "41-45/50: ######################################## 100% [00:01:08.01] loss: 4.08673 \n",
      "46-50/50: ######################################## 100% [00:01:06.93] loss: 4.08284 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, 50, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 文章生成\n",
    "\n",
    "学習したモデルを用いて文章を生成する。\n",
    "\n",
    "学習させたモデルは、ある単語から次の単語を予測するモデルである。厳密には、ある単語IDを入力に取り、次の単語IDを出力するモデルである。  \n",
    "さらに厳密に言うと、出力は単語IDではなく確率分布である。この確率分布から次の単語IDをサンプリングすることで、次の単語を生成する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に文章を生成させてみる。ある単語からの次の単語の予測を繰り返すことで文章を生成する。初めの単語だけはこちらで指定する。  \n",
    "以下の条件を満たすまで単語の生成を続ける。\n",
    "- 単語数が指定した限界に達する\n",
    "- 読点が出力される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token_id = v.get_stoi()['。'] # 読点のID\n",
    "\n",
    "def generate_sentence(model, start_word, max_len=30):\n",
    "    model.eval()\n",
    "    token_id = v.get_stoi()[start_word]\n",
    "    token_ids = [token_id]\n",
    "\n",
    "    # 終了条件を満たすまで単語を生成\n",
    "    while len(token_ids) <= max_len and token_id != end_token_id:\n",
    "\n",
    "        # 入力する単語IDをtensorに変換\n",
    "        x = torch.tensor(token_id).unsqueeze(0).to(device)\n",
    "\n",
    "        # 次の単語の確率分布を予測\n",
    "        y = model(x)[0]\n",
    "        y = F.softmax(y, dim=0)\n",
    "\n",
    "        # 分布に基づいたサンプリングを行う\n",
    "        token_id = random.choices(range(len(y)), weights=y)[0]\n",
    "        token_ids.append(token_id)\n",
    "\n",
    "    tokens = [v.get_itos()[i] for i in token_ids]\n",
    "    sentence = ''.join(tokens)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日の政治線にはバナジウム天然的に元の増えている。\n",
      "地球環境の諸侯は一面戦端の小さな農林歓の後、また君と掛けるものとなった。\n",
      "科学に対する空爆破産んだ林歓呼んでいるの娘役人・テカイ』(ファースト成功し、息子の綱吉川郷Bod\n",
      "人をハンドレスファッションブランド体制批判があることを対象にの立った。\n"
     ]
    }
   ],
   "source": [
    "start_words = ['今日', '地球', '科学', '人']\n",
    "for start_word in start_words:\n",
    "    print(generate_sentence(model, start_word))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 埋め込み層\n",
    "\n",
    "*Embedding Layer*\n",
    "\n",
    "NN内部における、単語IDに対する「one-hotベクトル化→線形変換」を行う部分をまとめて**埋め込み層**と表す。  \n",
    "そもそも、これらの処理は単語IDを受け取って対応するベクトルを出力すること=**単語のベクトル化**と同義である。自然言語処理の世界では単語のベクトル化を**単語の埋め込み**と表現する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線形変換を行う全結合層は各単語の埋め込み表現（単語ベクトル）を保有していることになる。それらは重みから確認できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0236, -0.4282,  0.4228, -0.1865, -0.2551],\n",
      "        [ 0.4069, -0.2835, -0.4091,  0.3555, -0.3168],\n",
      "        [ 0.4751, -0.4166,  0.2525,  0.2217,  0.0754]], grad_fn=<MmBackward0>) \n",
      "\n",
      "tensor([[-0.0236, -0.4282,  0.4228, -0.1865, -0.2551],\n",
      "        [ 0.4069, -0.2835, -0.4091,  0.3555, -0.3168],\n",
      "        [ 0.4751, -0.4166,  0.2525,  0.2217,  0.0754]],\n",
      "       grad_fn=<PermuteBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_dim):\n",
    "        super().__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.fc = nn.Linear(n_vocab, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.one_hot(x, self.n_vocab).to(torch.float32)\n",
    "        h = self.fc(x)\n",
    "        return h\n",
    "\n",
    "embedding = Embedding(n_vocab=3, embed_dim=5)\n",
    "\n",
    "x = torch.tensor([0, 1, 2])\n",
    "h = embedding(x)\n",
    "\n",
    "print(h, '\\n') # 埋め込み層からの出力\n",
    "print(embedding.fc.weight.T) # 全結合層の重み"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyTorch`には`torch.nn.Embedding`というクラスが実装されているので、それを使うと良い。  \n",
    "[Embedding — PyTorch 2.0 documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, n_vocab, hidden_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(n_vocab, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_vocab)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に、言語モデルの学習によって得られる単語ベクトルは良い埋め込み表現として機能することが知られている。単語の埋め込み表現を得ることを目的として言語モデルを学習させることもある。word2vecなどがこれに該当し、これについてはおまけの章で取り上げる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 特殊トークン\n",
    "\n",
    "言語モデルを扱う際、データを扱いやすくするために特殊なトークンを考えることがある。  \n",
    "以下に例を示す。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOS\n",
    "\n",
    "*Begin of Sentence*\n",
    "\n",
    "文章の先頭を意味するトークン。  \n",
    "先程の例では初めの単語を与える必要があったが、このトークンを作ることで、モデルに「文章の初め」を伝えられるようになる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOS\n",
    "\n",
    "*End of Sentence*\n",
    "\n",
    "文章の終わりを意味するトークン。  \n",
    "先程の例では読点が出た場合に生成を止めたが、本来読点は文の終わりであって文章の終わりではない。このトークンを作ることでモデルが文章の終わりを伝えられるようになる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNK\n",
    "\n",
    "*Unknown*\n",
    "\n",
    "未知語を意味するトークン。  \n",
    "推論時に学習データに含まれなかった単語に出会ったときに対応できるようになる。学習データ内での出現回数が少ない単語も未知語として扱うことがある。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データの全ての文章の該当箇所（BOSであれば文章の初め、EOSであれば文章の終わり）にこれらのトークンを入れてから学習させることで、そのトークンの意味をモデルは理解する。  \n",
    "またモデルが直接触れるのはトークンではなくトークンIDなので、特殊トークンの名前は何でもいい。他のトークンと重複しないように括弧を付けることが多い。\n",
    "\n",
    "<br>\n",
    "\n",
    "では特殊トークンを使って学習させてみよう。使う特殊トークンは上の三つ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['「', '教科', '書', 'に', 'は', '決して', '載ら', 'ない', '」', '日本']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokens[0][:10] # example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全ての文章にBOSとEOSを追加する。  \n",
    "他のトークンと重複しないような文字列を指定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS, EOS, UNK = '<BOS>', '<EOS>', '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<BOS>', '「', '教科', '書', 'に', 'は', '決して', '載ら', 'ない', '」']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tokens in data_tokens:\n",
    "    tokens.insert(0, BOS)\n",
    "    tokens.append(EOS)\n",
    "\n",
    "data_tokens[0][:10] # example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出現回数が少ない単語はUNKとして扱う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_freq=5: 出現回数が5回未満の単語は語彙に含まない\n",
    "v = build_vocab_from_iterator(data_tokens, min_freq=5, specials=[UNK])\n",
    "unk_id = v.get_stoi()[UNK] # UNKトークンのID\n",
    "v.set_default_index(unk_id) # 語彙に含まれない単語のIDを指定"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "残りの学習は同じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids = []\n",
    "\n",
    "for tokens in data_tokens:\n",
    "    data_ids.append(v(tokens))\n",
    "\n",
    "dataset = TextDataset(data_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(len(v), 512).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/20:                                            0% [00:00:00.03] loss: 6.10414 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/20: ######################################## 100% [00:00:00.79] loss: 3.54983 \n",
      " 2/20: ######################################## 100% [00:00:00.56] loss: 2.96415 \n",
      " 3/20: ######################################## 100% [00:00:00.57] loss: 2.84800 \n",
      " 4/20: ######################################## 100% [00:00:00.59] loss: 2.79078 \n",
      " 5/20: ######################################## 100% [00:00:00.62] loss: 2.76287 \n",
      " 6/20: ######################################## 100% [00:00:00.62] loss: 2.74396 \n",
      " 7/20: ######################################## 100% [00:00:00.63] loss: 2.73037 \n",
      " 8/20: ######################################## 100% [00:00:00.63] loss: 2.71907 \n",
      " 9/20: ######################################## 100% [00:00:00.63] loss: 2.71256 \n",
      "10/20: ######################################## 100% [00:00:00.64] loss: 2.70789 \n",
      "11/20: ######################################## 100% [00:00:00.65] loss: 2.70023 \n",
      "12/20: ######################################## 100% [00:00:00.66] loss: 2.69408 \n",
      "13/20: ######################################## 100% [00:00:00.69] loss: 2.69244 \n",
      "14/20: ######################################## 100% [00:00:00.67] loss: 2.68840 \n",
      "15/20: ######################################## 100% [00:00:00.67] loss: 2.68522 \n",
      "16/20: ######################################## 100% [00:00:00.68] loss: 2.68215 \n",
      "17/20: ######################################## 100% [00:00:00.68] loss: 2.68017 \n",
      "18/20: ######################################## 100% [00:00:00.68] loss: 2.68024 \n",
      "19/20: ######################################## 100% [00:00:00.70] loss: 2.67744 \n",
      "20/20: ######################################## 100% [00:00:00.69] loss: 2.67373 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, dataloader, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では文章を生成する。初めのトークンはBOSとし、EOSが出力されるまで生成を続ける。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token_id = v.get_stoi()[EOS]\n",
    "\n",
    "def generate_sentence(model, max_len=50):\n",
    "    model.eval()\n",
    "    token_id = v.get_stoi()[BOS]\n",
    "    token_ids = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        x = torch.tensor(token_id).unsqueeze(0).to(device)\n",
    "        y = model(x)[0]\n",
    "        y[unk_id] = -torch.inf # UNKがサンプリングされる確率を0にする\n",
    "        y = F.softmax(y, dim=0)\n",
    "        token_id = random.choices(range(len(y)), weights=y)[0]\n",
    "        if token_id == end_token_id:\n",
    "            break\n",
    "        token_ids.append(token_id)\n",
    "\n",
    "    tokens = [v.get_itos()[i] for i in token_ids]\n",
    "    sentence = ''.join(tokens)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005年1軍とのアカデミーによっている、それは名詞の中であり、ステッファーニは、メイン。6月に初と、ステッファーニは優勝者の美術院の中による。ハンガリー国際が富士山修験は\n",
      "PSO2作目を放送内容は、3の企画の番組『ヰタ。\n",
      "「コスプレコンテスト」と「村山浅間、大学を5月13年の長とはフェスティバルであった。\n",
      "分割(xが、その後のに放送が実施された情報番組『宇宙戦艦ヤマト22で、その他の制作（後（現在もある。『星に「君」のみ入場が台南線と8月\n",
      "1軍参謀長を記録（後に所蔵のほか、写真てはヤマト2放送終了となる。このため、その後、ステッファーニが、1月号の区別させ、管が、「僕」の『ファミ通に\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(generate_sentence(model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
