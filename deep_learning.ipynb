{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習を用いた言語モデル\n",
    "\n",
    "*Deep Learning*\n",
    "\n",
    "深層学習を用いて言語モデルを作成する。  \n",
    "ニューラルネットワークを用いて、ある単語から次の単語を予測するモデルを作成し、深層学習を活用した言語モデル実装の基礎を学ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import MeCab\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import vocab, build_vocab_from_iterator\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## データセット\n",
    "\n",
    "wiki40b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    ds = tfds.load('wiki40b/ja', split='test')\n",
    "ds = list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 89698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。',\n",
       " 'ライブドア社員であった初代代表取締役社長の山名真由によって企業内起業の形で創業。2005年に株式会社ライブドアから分割されて設立。かつてはライブドアホールディングス（現・LDH）の子会社であったが、ノンコア事業の整理にともない、株式会社ゲオ（現：株式会社ゲオホールディングス）に所有する全株式を譲渡し、同社の完全子会社となった。「ぽすれん」「ゲオ宅配レンタル」のオンラインDVD・CD・コミックレンタルサービス及び「GEO Online」と「ゲオアプリ」のアプリ・ウェブサイト運営の大きく分けて2事業を展開している。以前はDVD販売等のEコマースサービス「ぽすれんストア」、動画配信コンテンツ「ぽすれんBB」や電子書籍配信サービスの「GEO☆Books」事業も行っていた。オンラインDVDレンタル事業では会員数は10万人（2005年9月時点）。2006年5月よりCDレンタルを開始。同業他社には、カルチュア・コンビニエンス・クラブが運営する『TSUTAYA DISCAS』のほか、DMM.comが運営する『DMM.com オンラインDVDレンタル』がある。過去には「Yahoo!レンタルDVD」と「楽天レンタル」の運営を受託していた。',\n",
       " '2005年の一時期、東京のラジオ局、InterFMで、「堀江社長も使っているライブドアのぽすれん」というキャッチコピーでラジオCMを頻繁に行っていたことがあった。',\n",
       " '香川県内の農業協同組合の信用事業を統括する県域農協系金融機関であり、県内農業協同組合を会員とする。香川県は全県単一農協の香川県農業協同組合となったが、先に単一農協となった奈良県や沖縄県のケースと異なり、信連の統合は行われなかった。通称は「JA香川信連」または「JAバンク香川」。統一金融機関コードは3037。主に法人顧客を中心としており、個人取引は殆どない。県内の大型商業施設にある、他金融機関管理の共同ATMには香川信連の管轄のものがある。',\n",
       " '534年（永熙3年）、独孤信の子として生まれた。独孤信が父母妻子を捨てて長安に入ったため、独孤羅は東魏に取り残されて高氏の虜囚となった。独孤信が宇文護により処刑されると、ようやく釈放されて、中山に寓居した。北斉の独孤永業の一族として田宅を与えられた。北斉が滅亡し、楊堅が定州総管となると、楊堅の妻の独孤伽羅が兄の行方を捜索させて独孤羅を見つけ出し、初めて対面した。579年（大象元年）、功臣の子として楚安郡太守に任じられた。まもなく病のため辞職し、長安に帰った。580年（大象2年）、楊堅が北周の丞相となると、独孤羅は儀同大将軍の位を受け、楊堅の側近に仕えた。581年（開皇元年）、隋が建国されると、使持節・上開府・儀同大将軍の位を受けた。11月、右武衛将軍に転じた。582年（開皇2年）、父の趙国公の爵位を嗣いだ。592年（開皇12年）、大将軍・太子右衛率となった。593年（開皇13年）、涼州刺史に任じられた。597年（開皇17年）、涼州総管に任じられた。599年（開皇19年）2月6日、死去した。諡は徳といった。']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for sample in ds:\n",
    "    text = sample['text'].decode()\n",
    "    sections = text.split('_START_SECTION_')\n",
    "    for section in sections[1:]:\n",
    "        sentence = section.split('_START_PARAGRAPH_')[1]\n",
    "        sentence = sentence.replace('_NEWLINE_', '')\n",
    "        sentence = sentence.replace('\\n', '')\n",
    "        data.append(sentence)\n",
    "\n",
    "print('num of data:', len(data))\n",
    "data[:5] # examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多すぎるので減らす。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらを一つの.txtファイルに書き出しておく。後で使う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = 'data/jawiki_1000.txt'\n",
    "with open(textfile, 'w') as f:\n",
    "    for sentence in data:\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 前処理\n",
    "\n",
    "テキストをNNで扱える形に変換する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークン化\n",
    "\n",
    "文章をトークンごとに分割する。トークンとは文章を構成する最小の単位で、単語や句読点などが該当する。  \n",
    "トークン化は分かち書きと似た意味であるが、言語モデルの領域で最小単位をトークンと呼ぶことが一般的なことや、単純にトークン化と呼ぶことが多いことから、トークン化と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 英語\n",
    "\n",
    "英語のトークン化は簡単で、スペースで区切ればいい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'a', 'student']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I am a student'\n",
    "tokens = text.split(' ')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 日本語\n",
    "\n",
    "日本語の場合は文章がスペースで区切られていないので、別の手法が必要。  \n",
    "形態素解析器を用いるのが一般的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['今日', 'は', 'いい', '天気', 'だ', '\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '今日はいい天気だ'\n",
    "tagger = MeCab.Tagger('-Owakati')\n",
    "tokens = tagger.parse(text).split(' ')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全ての文章をトークン化しよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['「', '教科', '書', 'に', 'は', '決して', '載ら', 'ない', '」', '日本']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokens = []\n",
    "for sentence in data:\n",
    "    tokens = tagger.parse(sentence).split()\n",
    "    data_tokens.append(tokens)\n",
    "\n",
    "data_tokens[0][:10] # example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID化\n",
    "\n",
    "NNでは文字列を扱えないので、各トークンにIDとして整数値を割り当てる。\n",
    "\n",
    "torchtextの`vocab`モジュールを使用する。  \n",
    "[torchtext.vocab — Torchtext 0.15.0 documentation](https://pytorch.org/text/stable/vocab.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab, build_vocab_from_iterator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語とその出現頻度を表した辞書を入力し、`Vocab`オブジェクトを作成する。`min_freq`で指定した出現頻度以下の単語は無視される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = vocab({'私': 4, 'りんご': 1, '食べる': 2, '好き': 3}, min_freq=1)\n",
    "v(['私', 'りんご', '好き'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_vocab_from_iterator()`でトークンのリスト（のリスト）から`vocab`オブジェクトを作成できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['は', '私', 'が', 'です', 'りんご', 'を', 'バナナ', '今日', '好き', '晴れ', '食べる']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = build_vocab_from_iterator([\n",
    "    ['私', 'は', 'りんご', 'が', '好き'],\n",
    "    ['私', 'は', 'バナナ', 'を', '食べる'],\n",
    "    ['今日', 'は', '晴れ', 'です']\n",
    "])\n",
    "v.get_itos() # vocabrary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを使ってトークン列をID列に変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary: 22427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[18, 7079, 583, 2, 6, 10352, 21655, 44, 19, 66]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ids = []\n",
    "v = build_vocab_from_iterator(data_tokens)\n",
    "for tokens in data_tokens:\n",
    "    data_ids.append(v(tokens))\n",
    "\n",
    "n_vocab = len(v)\n",
    "print('num of vocabrary:', n_vocab)\n",
    "data_ids[0][:10] # example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習データ\n",
    "\n",
    "NNの学習を行うため、入力と出力のペアを作成する。  \n",
    "今回はある単語から次に続く単語を予測するモデルを作成するので、ある単語IDとその次の単語IDがペアとなったデータを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 243396\n",
      "tensor([    3,    14,    48,  3214,   149,  1057,   362, 20574,    31,    17,\n",
      "            2,    40,   763,  3547,  9894,  5652,   104,    58,   743,   781,\n",
      "            8,     1,   315,    10,  1726,   489,    93, 20353,   212,    12,\n",
      "         2794,     3,    22,   324,   131,    27, 11079,   143,     9,   794,\n",
      "           13,     4,     3,  2433, 13563,     1,    20,   843,    35,  1053,\n",
      "           21,    36,    20,     2,    10,     8,   538,    91,  2480,     1,\n",
      "            1,  6758,     8,    19])\n",
      "tensor([   36, 15623,     2,   852,  5331,  3880,    72,     2,     2,  2358,\n",
      "           55,     3,     4,   130,   112,     0,    11,     1,     3,   190,\n",
      "         1042, 17008,   307,    23,     2,    72,     4,     2,   282,     1,\n",
      "         2525,    98,   394,    12,    25,  1163,     5,   136,     1,    21,\n",
      "            4,    18,  1403,  2622,   369,    88,  1716,    56,   869,     6,\n",
      "          604,   185,     1,    32,    45,  1396,     5,     2,    27,   113,\n",
      "           89,     0,  9617,     2])\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids):\n",
    "        x, y = [], []\n",
    "        for ids in data_ids:\n",
    "            for in_id, out_id in zip(ids[:-1], ids[1:]):\n",
    "                x.append(in_id)\n",
    "                y.append(out_id)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.n_data = len(x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "batch_size = 64\n",
    "dataset = TextDataset(data_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "n_data = len(dataset)\n",
    "print('num of data:', n_data)\n",
    "\n",
    "# examples\n",
    "x, y = next(iter(dataloader))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築\n",
    "\n",
    "ある単語IDを入力に取り、次の単語IDを出力するモデルを作成する。\n",
    "\n",
    "単語IDはカテゴリ変数なので、入力時はone-hotベクトルに変換する。出力は語彙数分の次元を持つベクトルとする。つまりこのタスクは単語の分類問題となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, n_vocab, hidden_size):\n",
    "        super().__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_vocab, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_vocab)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.one_hot(x, self.n_vocab).to(torch.float32)\n",
    "        y = self.net(x)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習\n",
    "\n",
    "損失関数に交差エントロピーを設定し、通常の分類モデルと同じように学習する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    model.train()\n",
    "    prog.start(n_iter=len(dataloader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(1, n_epochs + 1):\n",
    "        for x, t in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = model(x)\n",
    "            loss = criterion(y, t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prog.update(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "model = LanguageModel(n_vocab, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1-2/20: ######################################## 100% [00:00:53.69] loss: 5.59791  \n",
      "  3-4/20: ######################################## 100% [00:00:52.62] loss: 4.35564 \n",
      "  5-6/20: ######################################## 100% [00:00:52.50] loss: 3.89540 \n",
      "  7-8/20: ######################################## 100% [00:00:52.51] loss: 3.74547 \n",
      " 9-10/20: ######################################## 100% [00:00:52.48] loss: 3.69052 \n",
      "11-12/20: ######################################## 100% [00:00:52.49] loss: 3.66524 \n",
      "13-14/20: ######################################## 100% [00:00:52.42] loss: 3.65046 \n",
      "15-16/20: ######################################## 100% [00:00:52.44] loss: 3.64009 \n",
      "17-18/20: ######################################## 100% [00:00:52.65] loss: 3.63196 \n",
      "19-20/20: ######################################## 100% [00:00:52.57] loss: 3.62591 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, 20, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 文章生成\n",
    "\n",
    "学習したモデルを用いて文章を生成する。\n",
    "\n",
    "学習させたモデルは、ある単語から次の単語を予測するモデルである。厳密には、ある単語IDを入力に取り、次の単語IDを出力するモデルである。  \n",
    "さらに厳密に言うと、出力は単語IDではなく確率分布である。この確率分布から次の単語IDをサンプリングすることで、次の単語を生成する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に文章を生成させてみる。ある単語からの次の単語の予測を繰り返すことで文章を生成する。初めの単語だけはこちらで指定する。  \n",
    "以下の条件を満たすまで単語の生成を続ける。\n",
    "- 単語数が指定した限界に達する\n",
    "- 句点が出力される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token_id = v.get_stoi()['。'] # 読点のID\n",
    "\n",
    "def generate_sentence(model, start_word, max_len=30):\n",
    "    model.eval()\n",
    "    token_id = v.get_stoi()[start_word]\n",
    "    token_ids = [token_id]\n",
    "\n",
    "    # 終了条件を満たすまで単語を生成\n",
    "    while len(token_ids) <= max_len and token_id != end_token_id:\n",
    "\n",
    "        # 入力する単語IDをtensorに変換\n",
    "        x = torch.tensor(token_id).unsqueeze(0).to(device)\n",
    "\n",
    "        # 次の単語の確率分布を予測\n",
    "        y = model(x)[0]\n",
    "        y = F.softmax(y, dim=0)\n",
    "\n",
    "        # 分布に基づいたサンプリングを行う\n",
    "        token_id = random.choices(range(len(y)), weights=y)[0]\n",
    "        token_ids.append(token_id)\n",
    "\n",
    "    tokens = [v.get_itos()[i] for i in token_ids]\n",
    "    sentence = ''.join(tokens)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日、この軽巡洋艦歴はリトアニアの流動性K.5月からスタンウェイの姿勢は、誌面でマークして白石、国境\n",
      "地球温暖化、陽イオン自身の魅力的に記され、ヨネダは彼は1751年にはアジア北西のキリスト教徒という説明\n",
      "科学的に溝を超えた父の政策に覆うフェアリングの重要のカリフォルニア大学リーグだった要塞施設は知名度もはっきりと改めた\n",
      "人が随時公表を持つことにあたったことに子孫が、カナダを優先していく。\n"
     ]
    }
   ],
   "source": [
    "start_words = ['今日', '地球', '科学', '人']\n",
    "for start_word in start_words:\n",
    "    print(generate_sentence(model, start_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マルコフモデル同様、直前の単語のみを予測に用いているため、不自然な文章が多く生成される。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 埋め込み層\n",
    "\n",
    "*Embedding Layer*\n",
    "\n",
    "NN内部における、単語IDに対する「one-hotベクトル化→線形変換」を行う部分をまとめて**埋め込み層**と表す。  \n",
    "そもそも、これらの処理は単語IDを受け取って対応するベクトルを出力すること=**単語のベクトル化**と同義である。自然言語処理の世界では単語のベクトル化を**単語の埋め込み**と表現する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線形変換を行う全結合層は各単語の埋め込み表現（単語ベクトル）を保有していることになる。それらは重みから確認できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1121, -0.2213, -0.1408,  0.0932, -0.2149],\n",
      "        [-0.4840, -0.5677, -0.3134, -0.2998,  0.1772],\n",
      "        [-0.1979, -0.1080,  0.0571,  0.4933,  0.1174]], grad_fn=<MmBackward0>) \n",
      "\n",
      "tensor([[-0.1121, -0.2213, -0.1408,  0.0932, -0.2149],\n",
      "        [-0.4840, -0.5677, -0.3134, -0.2998,  0.1772],\n",
      "        [-0.1979, -0.1080,  0.0571,  0.4933,  0.1174]],\n",
      "       grad_fn=<PermuteBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_dim):\n",
    "        super().__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.fc = nn.Linear(n_vocab, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.one_hot(x, self.n_vocab).to(torch.float32)\n",
    "        h = self.fc(x)\n",
    "        return h\n",
    "\n",
    "embedding = Embedding(n_vocab=3, embed_dim=5)\n",
    "\n",
    "x = torch.tensor([0, 1, 2])\n",
    "h = embedding(x)\n",
    "\n",
    "print(h, '\\n') # 埋め込み層からの出力\n",
    "print(embedding.fc.weight.T) # 全結合層の重み"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyTorch`には`torch.nn.Embedding`というクラスが実装されているので、それを使うと良い。  \n",
    "[Embedding — PyTorch 2.0 documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, n_vocab, hidden_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(n_vocab, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_vocab)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に、言語モデルの学習によって得られる単語ベクトルは良い埋め込み表現として機能することが知られている。単語の埋め込み表現を得ることを目的として言語モデルを学習させることもある。word2vecなどがこれに該当し、これについてはおまけの章で取り上げる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## サブワード分割\n",
    "\n",
    "よりよいトークン化の手法。  \n",
    "データセットから頻出する文字の並びを学習し、その並びをトークンとして分割する。\n",
    "\n",
    "形態素解析器を用いたやり方は、辞書が必要なことやトークンの種類が多くなるなどの問題がある。トークンの種類が多くなると必要なパラメータが多くなるので、減らしていきたい。  \n",
    "サブワード分割はデータセットに合ったトークン化を実現する。語彙数を指定することも可能。指定した語彙数に収まるまで細かく分割する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_prefix = 'models/jawiki_1000_tokenizer'\n",
    "vocab_size = 8000 # 語彙数\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile,\n",
    "    model_prefix=tokenizer_prefix,\n",
    "    vocab_size=vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['▁', '今日', 'は', 'いい', '天', '気', 'だ']\n",
      "ids: [11, 1993, 6, 2317, 681, 343, 263]\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(f'{tokenizer_prefix}.model') # モデル読み込み\n",
    "text = '今日はいい天気だ'\n",
    "print('tokens:', sp.encode(text, out_type=str))\n",
    "print('ids:', sp.encode(text, out_type=int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アンダーバーは空白を意味する。初めにアンダーバーが付くのは仕様。\n",
    "\n",
    "全てのデータをトークン化しよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。',\n",
       " 'ライブドア社員であった初代代表取締役社長の山名真由によって企業内起業の形で創業。2005年に株式会社ライブドアから分割されて設立。かつてはライブドアホールディングス（現・LDH）の子会社であったが、ノンコア事業の整理にともない、株式会社ゲオ（現：株式会社ゲオホールディングス）に所有する全株式を譲渡し、同社の完全子会社となった。「ぽすれん」「ゲオ宅配レンタル」のオンラインDVD・CD・コミックレンタルサービス及び「GEO Online」と「ゲオアプリ」のアプリ・ウェブサイト運営の大きく分けて2事業を展開している。以前はDVD販売等のEコマースサービス「ぽすれんストア」、動画配信コンテンツ「ぽすれんBB」や電子書籍配信サービスの「GEO☆Books」事業も行っていた。オンラインDVDレンタル事業では会員数は10万人（2005年9月時点）。2006年5月よりCDレンタルを開始。同業他社には、カルチュア・コンビニエンス・クラブが運営する『TSUTAYA DISCAS』のほか、DMM.comが運営する『DMM.com オンラインDVDレンタル』がある。過去には「Yahoo!レンタルDVD」と「楽天レンタル」の運営を受託していた。',\n",
       " '2005年の一時期、東京のラジオ局、InterFMで、「堀江社長も使っているライブドアのぽすれん」というキャッチコピーでラジオCMを頻繁に行っていたことがあった。',\n",
       " '香川県内の農業協同組合の信用事業を統括する県域農協系金融機関であり、県内農業協同組合を会員とする。香川県は全県単一農協の香川県農業協同組合となったが、先に単一農協となった奈良県や沖縄県のケースと異なり、信連の統合は行われなかった。通称は「JA香川信連」または「JAバンク香川」。統一金融機関コードは3037。主に法人顧客を中心としており、個人取引は殆どない。県内の大型商業施設にある、他金融機関管理の共同ATMには香川信連の管轄のものがある。',\n",
       " '534年（永熙3年）、独孤信の子として生まれた。独孤信が父母妻子を捨てて長安に入ったため、独孤羅は東魏に取り残されて高氏の虜囚となった。独孤信が宇文護により処刑されると、ようやく釈放されて、中山に寓居した。北斉の独孤永業の一族として田宅を与えられた。北斉が滅亡し、楊堅が定州総管となると、楊堅の妻の独孤伽羅が兄の行方を捜索させて独孤羅を見つけ出し、初めて対面した。579年（大象元年）、功臣の子として楚安郡太守に任じられた。まもなく病のため辞職し、長安に帰った。580年（大象2年）、楊堅が北周の丞相となると、独孤羅は儀同大将軍の位を受け、楊堅の側近に仕えた。581年（開皇元年）、隋が建国されると、使持節・上開府・儀同大将軍の位を受けた。11月、右武衛将軍に転じた。582年（開皇2年）、父の趙国公の爵位を嗣いだ。592年（開皇12年）、大将軍・太子右衛率となった。593年（開皇13年）、涼州刺史に任じられた。597年（開皇17年）、涼州総管に任じられた。599年（開皇19年）2月6日、死去した。諡は徳といった。']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5] # 再掲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary: 8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11, 18, 6254, 54, 1057, 58, 1685, 79, 122, 17]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ids = sp.encode(data)\n",
    "n_vocab = len(sp)\n",
    "print('num of vocabrary:', n_vocab)\n",
    "data_ids[0][:10] # example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに、ChatGPTがどの様にトークン化しているかをこのサイトから試せる: [OpenAI Platform](https://platform.openai.com/tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 特殊トークン\n",
    "\n",
    "言語モデルを扱う際、データを扱いやすくするために特殊なトークンを考えることがある。  \n",
    "以下に例を示す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Unknown*\n",
    "\n",
    "未知語を意味するトークン。  \n",
    "推論時に学習データに含まれなかった単語に出会ったときに対応できるようになる。学習データ内での出現回数が少ない単語も未知語として扱うことがある。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Begin of Sentence*\n",
    "\n",
    "文章の先頭を意味するトークン。  \n",
    "先程の例では初めの単語を与える必要があったが、このトークンを作ることで、モデルに「文章の初め」を伝えられるようになる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *End of Sentence*\n",
    "\n",
    "文章の終わりを意味するトークン。  \n",
    "先程の例では句点が出た場合に生成を止めたが、本来句点は文の終わりであって文章の終わりではない。このトークンを作ることでモデルが文章の終わりを伝えられるようになる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データの全ての文章の該当箇所（BOSであれば文章の初め、EOSであれば文章の終わり）にこれらのトークンを入れてから学習させることで、そのトークンの意味をモデルは理解する。  \n",
    "またモデルが直接触れるのはトークンではなくトークンIDなので、特殊トークンの名前は何でもいい。他のトークンと重複しないように括弧を付けることが多い。\n",
    "\n",
    "<br>\n",
    "\n",
    "では特殊トークンを使って学習させてみよう。使う特殊トークンは上の三つ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = '[UNK]' # Unknown\n",
    "BOS = '[BOS]' # Begin of sentence\n",
    "EOS = '[EOS]' # End of sentence\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile,\n",
    "    model_prefix=tokenizer_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    unk_piece=UNK,\n",
    "    bos_piece=BOS,\n",
    "    eos_piece=EOS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお`sentencepiece`では上記3つのトークンがID0, 1, 2にデフォルトで設定されているので、本当は指定しなくてもよい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 何も指定せずに学習\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile,\n",
    "    model_prefix=tokenizer_prefix,\n",
    "    vocab_size=vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID0: <unk>\n",
      "ID1: <s>\n",
      "ID2: </s>\n"
     ]
    }
   ],
   "source": [
    "# ID0, 1, 2\n",
    "sp = spm.SentencePieceProcessor(f'{tokenizer_prefix}.model')\n",
    "for i in range(3):\n",
    "    print(f'ID{i}:', sp.id_to_piece(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、トークン列にBOSとEOSを追加しよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_id = sp.bos_id()\n",
    "eos_id = sp.eos_id()\n",
    "for ids in data_ids:\n",
    "    ids[0] = bos_id # 先頭にBOSを追加, アンダーバーは上書き\n",
    "    ids[-1] = eos_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "残りを学習させる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset(data_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model = LanguageModel(len(v), 512).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1-2/20: ######################################## 100% [00:00:52.00] loss: 6.30764  \n",
      "  3-4/20: ######################################## 100% [00:00:51.88] loss: 4.90478 \n",
      "  5-6/20: ######################################## 100% [00:00:51.82] loss: 4.71638 \n",
      "  7-8/20: ######################################## 100% [00:00:51.85] loss: 4.63881 \n",
      " 9-10/20: ######################################## 100% [00:00:51.78] loss: 4.59391 \n",
      "11-12/20: ######################################## 100% [00:00:51.59] loss: 4.56537 \n",
      "13-14/20: ######################################## 100% [00:00:50.83] loss: 4.54433 \n",
      "15-16/20: ######################################## 100% [00:00:49.23] loss: 4.52929 \n",
      "17-18/20: ######################################## 100% [00:00:49.13] loss: 4.51622 \n",
      "19-20/20: ######################################## 100% [00:00:49.11] loss: 4.50476 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, 20, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では文章を生成する。初めのトークンはBOSとし、EOSが出力されるまで生成を続ける。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_id = sp.unk_id()\n",
    "\n",
    "def generate_sentence(model, max_len=50):\n",
    "    model.eval()\n",
    "    token_id = bos_id\n",
    "    token_ids = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        x = torch.tensor(token_id).unsqueeze(0).to(device)\n",
    "        y = model(x)[0]\n",
    "        y[unk_id] = -torch.inf # UNKがサンプリングされる確率を0にする\n",
    "        y = F.softmax(y, dim=0)\n",
    "        token_id = random.choices(range(len(y)), weights=y)[0]\n",
    "        if token_id == eos_id:\n",
    "            break\n",
    "        token_ids.append(token_id)\n",
    "\n",
    "    sentence = sp.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "」(Budelep スケ、新において、後に銀塩田になり、ゴールへの危機をを行い、美少女たちを中心に、思い出部と例外見孝二駆ド州行について、小学校設置に結びつ\n",
      "立地盤木市方面軍司令官(後の207月3つのシングル4チーム名の実家からの中であった。校が、9月12位と司政時代へと関心のおかげでインターネット間書店 tocrolso\n",
      "2016年の設立以降、王潮は唐使うコンスタンティヌス1世は外が発行が総合優勝を飾る記事などに分類されているという階の日本選に到着点者役に前作『いコンクール」、「紅海、に終わった。一方、部屋号\n",
      "方面に向かう。コンスタンティヌス1面はポーズに移籍した。1961世は今も環境に対する権威の通信大学の問題などが1,0年代を施できる範囲は木・以上の論文が産社会主義共和国の辞任を要求する声明を実現と、テレビ\n",
      "木学識を得たほか乗車を支払いでチーム数向けの旧エルサレム)島(現状を示す著作『RL)などより、甲斐から良く守的であった山口組執り、これに並列では、実現する\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(generate_sentence(model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
