{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習を用いた言語モデル\n",
    "\n",
    "*Deep Learning*\n",
    "\n",
    "深層学習を用いて言語モデルを作成する。  \n",
    "前章で作成したマルコフモデルの様な「ある単語から次の単語を予測するモデル」をニューラルネットワークを用いて作成する。\n",
    "\n",
    "本章では、深層学習を活用した言語モデル実装の基礎を学ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress(with_test=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット\n",
    "\n",
    "wiki40b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 89698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。\\n',\n",
       " 'ライブドア社員であった初代代表取締役社長の山名真由によって企業内起業の形で創業。2005年に株式会社ライブドアから分割されて設立。かつてはライブドアホールディングス（現・LDH）の子会社であったが、ノンコア事業の整理にともない、株式会社ゲオ（現：株式会社ゲオホールディングス）に所有する全株式を譲渡し、同社の完全子会社となった。「ぽすれん」「ゲオ宅配レンタル」のオンラインDVD・CD・コミックレンタルサービス及び「GEO Online」と「ゲオアプリ」のアプリ・ウェブサイト運営の大きく分けて2事業を展開している。以前はDVD販売等のEコマースサービス「ぽすれんストア」、動画配信コンテンツ「ぽすれんBB」や電子書籍配信サービスの「GEO☆Books」事業も行っていた。オンラインDVDレンタル事業では会員数は10万人（2005年9月時点）。2006年5月よりCDレンタルを開始。同業他社には、カルチュア・コンビニエンス・クラブが運営する『TSUTAYA DISCAS』のほか、DMM.comが運営する『DMM.com オンラインDVDレンタル』がある。過去には「Yahoo!レンタルDVD」と「楽天レンタル」の運営を受託していた。\\n',\n",
       " '2005年の一時期、東京のラジオ局、InterFMで、「堀江社長も使っているライブドアのぽすれん」というキャッチコピーでラジオCMを頻繁に行っていたことがあった。\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textfile = 'data/jawiki.txt'\n",
    "with open(textfile) as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "print('num of data:', len(data))\n",
    "data[:3] # examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多すぎるので減らす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 2000\n",
    "data = data[:n_data]\n",
    "\n",
    "textfile = f'data/jawiki_{n_data}.txt'\n",
    "with open(textfile, 'w') as f:\n",
    "    for sentence in data:\n",
    "        f.write(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## サブワード分割\n",
    "\n",
    "トークン化の手法。\n",
    "\n",
    "NNでも、マルコフモデル同様、文章をトークン化して学習を行う。  \n",
    "NNでは文字列をそのまま扱えないので、各トークンにID=クラスラベルを割り当てて、そのシーケンスとして文章を扱う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前章とは異なり、サブワード分割という新たな手法を使う。ちゃんと理由もある。\n",
    "\n",
    "1トークンあたりの文字数が多いことは、いくつかのデメリットを生む。  \n",
    "例えば、トークンの種類が多くなること。深層学習を用いた言語モデルはトークンの種類に比例してパラメータの数が増え、学習が困難になる。  \n",
    "それから、未知語が増えること。長いトークンは、それだけ多くの情報を持った限定的な言葉ということとなり、これらで語彙が埋まると表現力が落ちる。またデータセットに存在しない言葉を扱うことが困難になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サブワード分割は、これらの問題を解決する。  \n",
    "サブワードと呼ばれる、単語よりも小さな単位に分割することで、1トークンあたりの文字数を減らす。\n",
    "\n",
    "この手法では、データセットから頻出する文字の並びを学習し、その並びをトークンとして分割する。学習された、トークン化を行うモデルをトークナイザと呼ぶ。  \n",
    "データセットに合ったトークン化が可能。また語彙数を指定することも可能。指定した語彙数に収まるまで細かく分割してくれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに、ChatGPTが使っているトークナイザはこれ: [OpenAI Platform](https://platform.openai.com/tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "少し余談。\n",
    "\n",
    "前章でトークン化について以下のように説明した。\n",
    "\n",
    "> トークンとはモデルが扱える最小単位のことで、例えば単語が該当する。\n",
    "\n",
    "ここで、そもそも言語モデルは単語の確率を扱うものなので、絶対にトークン=単語にならないとおかしいとも考えられる。  \n",
    "実は文章生成においては、単語を最小単位にしなければならない理由はない。極端な話、文字を最小単位として文章を文字の並びと見てもいい訳だ。なんらかのシーケンスとできればそれで充分なのだ。実際、現在有名な言語モデルの多くは単語を最小単位としていない。\n",
    "\n",
    "ただ、言語モデルは単語の並びに確率を割り当てるモデルだ。単語を最小単位とするモデルだ。  \n",
    "もしこの定義に厳格になるのであれば、現在有名な多くの言語モデルは言語モデルと呼べないのかもしれんね。しらんけど。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "閑話休題。実際にやってみよう。  \n",
    "sentencepieceというライブラリを用いる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n",
    "\n",
    "語彙数とデータセット（テキストファイル）を指定して、学習させる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_prefix = f'models/tokenizer_jawiki_{n_data}' # トークナイザのモデル名\n",
    "vocab_size = 8000 # 語彙数\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile, # データセット\n",
    "    model_prefix=tokenizer_prefix,\n",
    "    vocab_size=vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークン化\n",
    "\n",
    "学習したモデルにテキストを突っ込むとトークン化してくれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 1897, 6, 2436, 664, 287, 346]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(f'{tokenizer_prefix}.model') # モデル読み込み\n",
    "text = '今日はいい天気だ'\n",
    "ids = sp.encode(text)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストがトークン化され、ID列として取得できる。  \n",
    "`out_type`を指定すると文字列のリストが取得できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '今日', 'は', 'いい', '天', '気', 'だ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.encode(text, out_type=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アンダーバーは空白を意味する。初めにアンダーバーが付くのは仕様。\n",
    "\n",
    "ID列を文字列に戻すこともできる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今日はいい天気だ'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全てのデータをトークン化（ID化）しよう。  \n",
    "文章のリスト: `List[str]`を`sp.encode()`に与えるとID列のリスト: `List[List[int]]`が返ってくる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary: 8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11, 19, 477, 653, 323, 51, 570, 57, 3856, 1583]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ids = sp.encode(data)\n",
    "n_vocab = len(sp)\n",
    "print('num of vocabrary:', n_vocab)\n",
    "data_ids[0][:10] # example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 特殊トークン\n",
    "\n",
    "言語モデルを扱う際、データを扱いやすくするために特殊なトークンを考えることがある。  \n",
    "以下に例を示す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Unknown*\n",
    "\n",
    "未知語を意味するトークン。  \n",
    "推論時に学習データに含まれなかった単語に出会ったときに対応できるようになる。学習データ内での出現回数が少ない単語も未知語として扱うことがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Begin of Sentence*\n",
    "\n",
    "文章の先頭を意味するトークン。  \n",
    "先程の例では初めの単語を与える必要があったが、このトークンを作ることで、モデルに「文章の初め」を伝えられるようになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *End of Sentence*\n",
    "\n",
    "文章の終わりを意味するトークン。  \n",
    "先程の例では句点が出た場合に生成を止めたが、本来句点は文の終わりであって文章の終わりではない。このトークンを作ることでモデルが文章の終わりを伝えられるようになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データの全ての文章の該当箇所（BOSであれば文章の初め、EOSであれば文章の終わり）にこれらのトークンを入れてから学習させることで、そのトークンの意味をモデルは理解する。  \n",
    "またモデルが直接触れるのはトークンではなくトークンIDなので、特殊トークンの名前は何でもいい。他のトークンと重複しないように括弧を付けたりすることが多い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentencepieceでは上記3つのトークンがID0, 1, 2にデフォルトで設定されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID0: <unk>\n",
      "ID1: <s>\n",
      "ID2: </s>\n"
     ]
    }
   ],
   "source": [
    "print('ID0:', sp.id_to_piece(0)) # Unknown\n",
    "print('ID1:', sp.id_to_piece(1)) # Begin of Sentence\n",
    "print('ID2:', sp.id_to_piece(2)) # End of Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、トークン列にBOSとEOSを追加して学習させてみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_id = sp.bos_id() # BOSのID\n",
    "eos_id = sp.eos_id() # EOSのID\n",
    "for ids in data_ids:\n",
    "    ids.insert(0, bos_id) # 先頭にBOSを追加\n",
    "    ids.append(eos_id) # 末尾にEOSを追加"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習データの作成\n",
    "\n",
    "NNの学習を行うため、入力と出力のペアを作成する。  \n",
    "今回はある単語から次に続く単語を予測するモデルを作成するので、ある単語IDとその次の単語IDがペアとなったデータを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  73,   13,  154,  495,   11,   71, 1750, 5906,  794,   36, 1383,  104,\n",
       "         1916,   52, 2798,  209, 5429,   11, 4401,   31,    5, 1387, 1278,  240,\n",
       "         2610, 4529,    3, 2491,    1,  782, 2460, 2642,    3, 2249,  321,  357,\n",
       "           12, 5065,  428, 1306,    4,  399, 3134,  286,  213,   94,   11,  116,\n",
       "            5,   30,    8,   61,  841,  606,   53,   11, 2032,   14, 1501,  179,\n",
       "           36,  110,   34,    4]),\n",
       " tensor([  31,   40,    5,  240,  252,   68, 4520,    9, 2958,    5, 3080,    7,\n",
       "          583, 1643,    3,  188,    6, 1521,  827, 3813, 2503, 1378,  840,    7,\n",
       "           26,    3,   84,  493,   11, 2077,    5,  589, 4005, 1690, 1531,   38,\n",
       "          150,  829, 4262,  126, 4088,    3,   18,    9,   39,    4, 2506, 1535,\n",
       "         1428, 1863,  459,   23,    6,  125, 1901, 5116,    8,    3,    9, 2278,\n",
       "         1554, 1308,  215, 5676]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ids = List[int]\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids: List[Ids]):\n",
    "        self.x = [] # 入力\n",
    "        self.t = [] # 正解\n",
    "        for ids in data_ids:\n",
    "            self.x += ids[:-1]\n",
    "            self.t += ids[1:]\n",
    "        self.n_data = len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.t[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "dataset = TextDataset(data_ids)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "# examples\n",
    "x, y = next(iter(train_loader))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 埋め込み層\n",
    "\n",
    "*Embedding Layer*\n",
    "\n",
    "指定したIDに対応するベクトルを出力する層。\n",
    "\n",
    "単語IDはクラスラベルなので、そのままNNに入力するのは適切ではない。そこで、埋め込み層と呼ばれる層を用いて単語IDを指定した次元のベクトルに変換する。単語をベクトル化することは埋め込みと呼ばれるので、埋め込み層と呼ばれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "埋め込み層は語彙の数だけベクトルを持っている。IDが入力されると、対応するベクトルが出力される。  \n",
    "実装してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4508,  2.4973,  0.0191, -0.4259,  0.7090],\n",
       "        [ 0.6346,  1.5298,  0.2450,  0.9936,  0.7267],\n",
       "        [-2.0840,  0.9692,  0.8315,  0.0060, -0.0687]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, n_vocab: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.vectors = torch.nn.Parameter(torch.randn(n_vocab, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.vectors[x]\n",
    "        return h\n",
    "\n",
    "embedding = Embedding(3, 5)\n",
    "\n",
    "x = torch.tensor([0, 1, 2])\n",
    "h = embedding(x)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyTorch`には`torch.nn.Embedding`というクラスが実装されているので、それを使うと良い。  \n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8123,  0.9160,  0.2652, -0.3974, -0.2158],\n",
       "        [-0.4996, -2.3171,  1.9133,  0.9837, -0.8494],\n",
       "        [ 1.3092,  0.8357, -1.4483,  0.1485,  0.4845]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = torch.nn.Embedding(3, 5)\n",
    "h = embedding(x)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "埋め込み層が持っているベクトルは学習可能なパラメータである。  \n",
    "埋め込み層がone-hot化+線形変換と同じことを行っていると見ればパラメータであることが理解しやすい。\n",
    "\n",
    "言語モデルの学習によって得られる単語ベクトルは良い埋め込み表現として機能することが知られている。単語の埋め込み表現を得ることを目的として言語モデルを学習させる試みもあり、word2vecなどがこれに該当する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築\n",
    "\n",
    "ある単語IDを入力に取り、次の単語IDを出力するモデルを作成する。\n",
    "\n",
    "実際の出力は語彙数分の次元を持つベクトルとなる。  \n",
    "つまりこのタスクは単語の分類問題とも言える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, n_vocab: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Embedding(n_vocab, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_vocab)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size,)\n",
    "        \"\"\"\n",
    "        y = self.net(x) # (batch_size, n_vocab)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習\n",
    "\n",
    "損失関数に交差エントロピーを設定し、通常の分類モデルと同じように学習する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = model(x)\n",
    "            loss = loss_fn(y, t)\n",
    "            total_loss += loss.item()\n",
    "    loss = total_loss / len(test_loader)\n",
    "    return loss\n",
    "\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    prog.start(n_iter=len(train_loader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for x, t in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device) # 入力\n",
    "            t = t.to(device) # 正解\n",
    "            y = model(x) # 出力\n",
    "            loss = loss_fn(y, t) # 損失\n",
    "            loss.backward() # 逆伝播\n",
    "            optimizer.step() # パラメータ更新\n",
    "            prog.update(loss.item())\n",
    "        test_loss = eval_model(model)\n",
    "        prog.memo(f'test: {test_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(sp)\n",
    "hidden_size = 512\n",
    "model = LanguageModel(n_vocab, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1-2/20: ############################## 100% [00:00:49.24] loss train: 6.58968, test: 6.42639 \n",
      "  3-4/20: ############################## 100% [00:00:45.75] loss train: 5.68028, test: 6.26719 \n",
      "  5-6/20: ############################## 100% [00:00:45.46] loss train: 5.29898, test: 6.22287 \n",
      "  7-8/20: ############################## 100% [00:00:44.78] loss train: 5.07391, test: 6.21966 \n",
      " 9-10/20: ############################## 100% [00:00:43.56] loss train: 4.92347, test: 6.23964 \n",
      "11-12/20: ############################## 100% [00:00:44.44] loss train: 4.81598, test: 6.26655 \n",
      "13-14/20: ############################## 100% [00:00:43.97] loss train: 4.73577, test: 6.29763 \n",
      "15-16/20: ############################## 100% [00:00:45.18] loss train: 4.67358, test: 6.33519 \n",
      "17-18/20: ############################## 100% [00:00:44.53] loss train: 4.62425, test: 6.37206 \n",
      "19-20/20: ############################## 100% [00:00:44.48] loss train: 4.58395, test: 6.41051 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, n_epochs=20, prog_unit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータでの精度は低くなるのはしょうがないかな。  \n",
    "直前の単語のみを見て次の単語を予測しているので、初見の文章を正確に予測するのは無理。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 文章生成\n",
    "\n",
    "学習したモデルを用いて文章を生成する。\n",
    "\n",
    "学習させたモデルは、ある単語から次の単語を予測するモデルである。厳密には、ある単語IDを入力に取り、次の単語IDを出力するモデルである。  \n",
    "さらに厳密に言うと、出力は単語IDではなく確率分布である。この確率分布から次の単語IDをサンプリングすることで、次の単語を生成する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に文章を生成させてみる。マルコフモデル同様、あるトークンからの次のトークンの予測を繰り返すことで文章を生成する。\n",
    "\n",
    "初めのトークンはBOSとし、以下の条件を満たすまで単語の生成を続ける。\n",
    "- EOSが生成される\n",
    "- 単語数が指定した限度に達する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_id = sp.unk_id() # UNKのID\n",
    "\n",
    "# モデルの出力から単語をサンプリングする関数\n",
    "def token_sampling(y: List[float]) -> int:\n",
    "    y[unk_id] = -torch.inf # UNKがサンプリングされる確率を0にする\n",
    "    probs = F.softmax(y, dim=-1) # 確率分布に変換\n",
    "    token, = random.choices(range(n_vocab), weights=probs) # サンプリング\n",
    "    return token\n",
    "\n",
    "# 文章を生成する関数\n",
    "def generate_sentence(model, max_len=50):\n",
    "    model.eval()\n",
    "    token_id = bos_id\n",
    "    token_ids = []\n",
    "\n",
    "    # 終了条件を満たすまで単語を生成\n",
    "    while len(token_ids) <= max_len and token_id != eos_id:\n",
    "\n",
    "        # 入力する単語IDをtensorに変換\n",
    "        x = torch.tensor(token_id).unsqueeze(0).to(device)\n",
    "\n",
    "        # 次の単語の確率分布を予測\n",
    "        y = model(x)[0]\n",
    "        token_id = token_sampling(y) # サンプリング\n",
    "        token_ids.append(token_id)\n",
    "\n",
    "    sentence = sp.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ゲーリングのVSEa」とりにも、これにわずか10軍のパラは御山の内容にせ、また、ブルックリンやを終え、死因果たの中間されることが多い。170合作者を付け頭髪などの出ていた。\n",
      "祭りたて屋根付き広場大学院侯の微博としいぶ。司法永14:)。\n",
      "8%の英雄は、甥、1961世のア(Sを発行に輝いた。これについてはメディデラーとともに「チンランドと初の6月22年ロンドンは「PRed Hopず、2010年7\n",
      "木造に向か、アシスタントコーチを務めている牛の息子として中納する「STSF1世率いるデンマーク王寺であり、ナチ党の未知れられている。ハウスのギッキー・形で田小学校司教職業、さを装\n",
      ":-音頭痛はラーシュテープ - 1つの大型点から収録されてしまったの中等のコンクリートの初めにしたサルチョの助手く、多摩市立て、草飛行士から正明和也\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(generate_sentence(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マルコフモデル同様、直前の単語のみを予測に用いているため、不自然な文章が多く生成される。\n",
    "\n",
    "出来上がったモデルは優れたものではないが、深層学習を用いた言語モデル実装の基礎を学ぶことができた。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
