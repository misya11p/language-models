{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習を用いた言語モデル\n",
    "\n",
    "*Deep Learning*\n",
    "\n",
    "深層学習を用いて言語モデルを作成する。  \n",
    "前章で作成したマルコフモデルの様な「ある単語から次の単語を予測するモデル」をニューラルネットワークを用いて作成する。\n",
    "\n",
    "本章では、深層学習を活用した言語モデル実装の基礎を学ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress(with_test=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット\n",
    "\n",
    "wiki40b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 89699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。',\n",
       " 'ライブドア社員であった初代代表取締役社長の山名真由によって企業内起業の形で創業。2005年に株式会社ライブドアから分割されて設立。かつてはライブドアホールディングス（現・LDH）の子会社であったが、ノンコア事業の整理にともない、株式会社ゲオ（現：株式会社ゲオホールディングス）に所有する全株式を譲渡し、同社の完全子会社となった。「ぽすれん」「ゲオ宅配レンタル」のオンラインDVD・CD・コミックレンタルサービス及び「GEO Online」と「ゲオアプリ」のアプリ・ウェブサイト運営の大きく分けて2事業を展開している。以前はDVD販売等のEコマースサービス「ぽすれんストア」、動画配信コンテンツ「ぽすれんBB」や電子書籍配信サービスの「GEO☆Books」事業も行っていた。オンラインDVDレンタル事業では会員数は10万人（2005年9月時点）。2006年5月よりCDレンタルを開始。同業他社には、カルチュア・コンビニエンス・クラブが運営する『TSUTAYA DISCAS』のほか、DMM.comが運営する『DMM.com オンラインDVDレンタル』がある。過去には「Yahoo!レンタルDVD」と「楽天レンタル」の運営を受託していた。',\n",
       " '2005年の一時期、東京のラジオ局、InterFMで、「堀江社長も使っているライブドアのぽすれん」というキャッチコピーでラジオCMを頻繁に行っていたことがあった。']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_path = 'data/jawiki.txt'\n",
    "with open(text_path) as f:\n",
    "    data = f.read().splitlines()\n",
    "\n",
    "print('num of data:', len(data))\n",
    "data[:3] # examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多すぎるので減らす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 2000\n",
    "data = data[:n_data]\n",
    "\n",
    "text_path = f'data/jawiki_{n_data}.txt'\n",
    "with open(text_path, 'w') as f:\n",
    "    f.write('\\n'.join(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## サブワード分割\n",
    "\n",
    "トークン化の手法。\n",
    "\n",
    "NNでも、マルコフモデル同様、文章をトークン化して学習を行う。  \n",
    "NNでは文字列をそのまま扱えないので、各トークンにID=クラスラベルを割り当てて、そのシーケンスとして文章を扱う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前章とは異なり、サブワード分割という新たな手法を使う。ちゃんと理由もある。\n",
    "\n",
    "1トークンあたりの文字数が多いことは、いくつかのデメリットを生む。  \n",
    "例えば、トークンの種類が多くなること。深層学習を用いた言語モデルはトークンの種類に比例してパラメータの数が増え、学習が困難になる。  \n",
    "それから、未知語が増えること。長いトークンは、それだけ多くの情報を持った限定的な言葉ということとなり、これらで語彙が埋まると表現力が落ちる。またデータセットに存在しない言葉を扱うことが困難になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サブワード分割は、これらの問題を解決する。  \n",
    "サブワードと呼ばれる、単語よりも小さな単位に分割することで、1トークンあたりの文字数を減らす。\n",
    "\n",
    "この手法では、データセットから頻出する文字の並びを学習し、その並びをトークンとして分割する。学習された、トークン化を行うモデルをトークナイザと呼ぶ。  \n",
    "データセットに合ったトークン化が可能。また語彙数を指定することも可能。指定した語彙数に収まるまで細かく分割してくれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに、ChatGPTが使っているトークナイザはこれ: [OpenAI Platform](https://platform.openai.com/tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "少し余談。\n",
    "\n",
    "前章でトークン化について以下のように説明した。\n",
    "\n",
    "> トークンとはモデルが扱える最小単位のことで、例えば単語が該当する。\n",
    "\n",
    "ここで、そもそも言語モデルは単語の確率を扱うものなので、絶対にトークン=単語にならないとおかしいとも考えられる。  \n",
    "実は文章生成においては、単語を最小単位にしなければならない理由はない。極端な話、文字を最小単位として文章を文字の並びと見てもいい訳だ。なんらかのシーケンスとできればそれで充分なのだ。実際、現在有名な言語モデルの多くは単語を最小単位としていない。\n",
    "\n",
    "ただ、言語モデルは単語の並びに確率を割り当てるモデルだ。単語を最小単位とするモデルだ。  \n",
    "もしこの定義に厳格になるのであれば、現在有名な多くの言語モデルは言語モデルと呼べないのかもしれんね。しらんけど。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "閑話休題。実際にやってみよう。  \n",
    "sentencepieceというライブラリを用いる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n",
    "\n",
    "語彙数とデータセット（テキストファイル）を指定して、学習させる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/jawiki_2000.txt\n",
      "  input_format: \n",
      "  model_prefix: models/tokenizer_jawiki_2000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: data/jawiki_2000.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (4385 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 1919 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 81 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=713740\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=3004\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 1919 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=230159\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 141046 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 1919\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 5176\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 5176 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=66805 obj=456.884 num_tokens=270647 num_tokens/piece=4.0513\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=60707 obj=419.484 num_tokens=272068 num_tokens/piece=4.48166\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=45413 obj=427.575 num_tokens=287377 num_tokens/piece=6.32808\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=45261 obj=423.344 num_tokens=288072 num_tokens/piece=6.36468\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=33922 obj=439.818 num_tokens=307120 num_tokens/piece=9.05371\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=33897 obj=435.299 num_tokens=307292 num_tokens/piece=9.06546\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=25420 obj=455.276 num_tokens=328225 num_tokens/piece=12.9121\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=25418 obj=450.566 num_tokens=328234 num_tokens/piece=12.9134\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=19063 obj=472.665 num_tokens=350332 num_tokens/piece=18.3776\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=19063 obj=468.504 num_tokens=350432 num_tokens/piece=18.3828\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=14297 obj=492.492 num_tokens=373201 num_tokens/piece=26.1034\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=14297 obj=488.72 num_tokens=373203 num_tokens/piece=26.1036\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=10722 obj=513.525 num_tokens=397129 num_tokens/piece=37.0387\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=10722 obj=509.875 num_tokens=397131 num_tokens/piece=37.0389\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=527.625 num_tokens=414459 num_tokens/piece=47.0976\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=525.166 num_tokens=414462 num_tokens/piece=47.098\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: models/tokenizer_jawiki_2000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: models/tokenizer_jawiki_2000.vocab\n"
     ]
    }
   ],
   "source": [
    "tokenizer_prefix = f'models/tokenizer_jawiki_{n_data}' # トークナイザのモデル名\n",
    "n_vocab = 8000 # 語彙数\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=text_path, # データセット\n",
    "    model_prefix=tokenizer_prefix,\n",
    "    vocab_size=n_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークン化\n",
    "\n",
    "学習したモデルにテキストを突っ込むとトークン化してくれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 1897, 6, 2436, 664, 287, 346]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(f'{tokenizer_prefix}.model') # モデル読み込み\n",
    "text = '今日はいい天気だ'\n",
    "ids = sp.encode(text)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストがトークン化され、ID列として取得できる。  \n",
    "`out_type`を指定すると文字列のリストが取得できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '今日', 'は', 'いい', '天', '気', 'だ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.encode(text, out_type=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アンダーバーは空白を意味する。初めにアンダーバーが付くのは仕様。\n",
    "\n",
    "ID列を文字列に戻すこともできる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今日はいい天気だ'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全てのデータをトークン化（ID化）しよう。  \n",
    "文章のリスト: `List[str]`を`sp.encode()`に与えるとID列のリスト: `List[List[int]]`が返ってくる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary: 8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11, 19, 477, 653, 323, 51, 570, 57, 3856, 1583]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ids = sp.encode(data)\n",
    "n_vocab = len(sp)\n",
    "print('num of vocabrary:', n_vocab)\n",
    "data_ids[0][:10] # example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 特殊トークン\n",
    "\n",
    "言語モデルを扱う際、データを扱いやすくするために特殊なトークンを考えることがある。  \n",
    "以下に例を示す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Unknown*\n",
    "\n",
    "未知語を意味するトークン。  \n",
    "推論時に学習データに含まれなかった単語に出会ったときに対応できるようになる。学習データ内での出現回数が少ない単語も未知語として扱うことがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Begin of Sentence*\n",
    "\n",
    "文章の先頭を意味するトークン。  \n",
    "先程の例では初めの単語を与える必要があったが、このトークンを作ることで、モデルに「文章の初め」を伝えられるようになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *End of Sentence*\n",
    "\n",
    "文章の終わりを意味するトークン。  \n",
    "先程の例では句点が出た場合に生成を止めたが、本来句点は文の終わりであって文章の終わりではない。このトークンを作ることでモデルが文章の終わりを伝えられるようになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データの全ての文章の該当箇所（BOSであれば文章の初め、EOSであれば文章の終わり）にこれらのトークンを入れてから学習させることで、そのトークンの意味をモデルは理解する。  \n",
    "またモデルが直接触れるのはトークンではなくトークンIDなので、特殊トークンの名前は何でもいい。他のトークンと重複しないように括弧を付けたりすることが多い。`[BOS]`とか`<EOS>`とか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentencepieceでは上記3つのトークンがID0, 1, 2にデフォルトで設定されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_id = sp.unk_id()\n",
    "bos_id = sp.bos_id()\n",
    "eos_id = sp.eos_id()\n",
    "unk_id, bos_id, eos_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bosとeosをデータに追加しておこう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in data_ids:\n",
    "    ids.insert(0, bos_id) # 先頭にBOSを追加\n",
    "    ids.append(eos_id) # 末尾にEOSを追加"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習データの作成\n",
    "\n",
    "NNの学習を行うため、入力と出力のペアを作成する。  \n",
    "今回はある単語から次に続く単語を予測するモデルを作成するので、ある単語IDとその次の単語IDがペアとなったデータを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 930, 2904, 4035,   11, 2999,    8,  313,  451,    5, 1376,  102,  762,\n",
       "           35,  374, 3384,   53,  570, 1223,  187,   91,  685, 5366, 3511,    3,\n",
       "         7136, 3689, 1575, 4541,   11, 1289, 1434, 3582]),\n",
       " tensor([   6,   10,    4,  191, 5184,  805, 3316,    7, 2119, 6650,  284,   17,\n",
       "           37, 4162,   17,   12,   39,    3,  135,   34,   32,   28,    5,   66,\n",
       "          488, 1800,  115,    5,   62,  508,   15,  115]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ids = List[int]\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids: List[Ids]):\n",
    "        self.x = [] # 入力\n",
    "        self.t = [] # 正解\n",
    "        for ids in data_ids:\n",
    "            self.x += ids[:-1] # 単語\n",
    "            self.t += ids[1:] # 次の単語\n",
    "        self.n_data = len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.t[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "# Dataset作成, 訓練データ:テストデータ = 8:2\n",
    "dataset = TextDataset(data_ids)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "# DataLoader作成\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# examples\n",
    "x, y = next(iter(train_loader))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 埋め込み層\n",
    "\n",
    "*Embedding Layer*\n",
    "\n",
    "指定したIDに対応するベクトルを出力する層。\n",
    "\n",
    "単語IDはクラスラベルなので、そのままNNに入力するのは適切ではない。そこで、埋め込み層と呼ばれる層を用いて単語IDを指定した次元のベクトルに変換する。単語をベクトル化することは埋め込みと呼ばれるので、埋め込み層と呼ばれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "埋め込み層は語彙の数だけベクトルを持っている。IDが入力されると、対応するベクトルが出力される。  \n",
    "実装してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3393, -1.0283, -0.2885,  0.0278,  0.1828],\n",
       "        [-0.7195,  0.6724, -0.7675, -0.4354,  0.3122],\n",
       "        [-0.2893, -2.1155,  0.3440, -0.5355, -0.7502]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, n_vocab: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.vectors = torch.nn.Parameter(torch.randn(n_vocab, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.vectors[x]\n",
    "        return h\n",
    "\n",
    "embedding = Embedding(3, 5)\n",
    "\n",
    "x = torch.tensor([0, 1, 2])\n",
    "h = embedding(x)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyTorch`には`torch.nn.Embedding`というクラスが実装されているので、それを使うと良い。  \n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1750, -0.9761,  0.2558, -1.1941,  1.0711],\n",
       "        [-0.9422, -0.1731,  0.0815, -0.0630, -0.3444],\n",
       "        [-1.6119,  0.0363,  0.6885, -0.9079,  0.9272]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = torch.nn.Embedding(3, 5)\n",
    "h = embedding(x)\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "埋め込み層が持っているベクトルは学習可能なパラメータである。  \n",
    "埋め込み層が「one-hot化+線形変換」を行っていると見ればパラメータであることが理解しやすい。\n",
    "\n",
    "また、言語モデルの学習によって得られる単語ベクトルは良い埋め込み表現として機能することが知られている。単語の埋め込み表現を得ることを目的として言語モデルを学習させる試みもあり、word2vecなどがこれに該当する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築\n",
    "\n",
    "ある単語IDを入力に取り、次の単語IDを出力するモデルを作成する。\n",
    "\n",
    "実際の出力は語彙数分の次元を持つベクトルとなる。  \n",
    "つまりこのタスクは単語の分類問題とも言える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルは埋め込み層と線形層で作る。最もシンプルな構成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, n_vocab: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size,)\n",
    "        \"\"\"\n",
    "        h = self.embedding(x) # (batch_size, hidden_size)\n",
    "        h = self.relu(h)\n",
    "        y = self.fc(h) # (batch_size, n_vocab)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(sp)\n",
    "hidden_size = 512\n",
    "model = LanguageModel(n_vocab, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータ数はこんな感じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of parameters: 8,200,000\n"
     ]
    }
   ],
   "source": [
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"num of parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## パープレキシティ\n",
    "\n",
    "*Perplexity*\n",
    "\n",
    "言語モデルの評価指標。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "言語モデルは文脈から次の単語を予測する。単語はカテゴリ変数なので、言語モデルは単語に関する分類モデルとなる。\n",
    "\n",
    "分類モデルでは、正解ラベルと予測ラベルの差を評価する指標として、交差エントロピーがよく用いられる。交差エントロピーは二つの確率分布の差を表す指標。\n",
    "\n",
    "$$\n",
    "H(p, q) = - \\sum_{x} p(x) \\log q(x)\n",
    "$$\n",
    "\n",
    "損失関数を交差エントロピーとしてNNを学習させることはよくある。予測された分布と正解の分布を与え、損失を計算する。\n",
    "\n",
    "$$\n",
    "L = H(t, y) = - \\sum_i t_i \\log y_i\n",
    "$$\n",
    "\n",
    "正解ラベルは基本的にone-hotベクトルなので、対応する部分だけ見れば良く、正解ラベルを$k$とすると以下のように表せる。\n",
    "\n",
    "$$\n",
    "L = - \\log y_k\n",
    "$$\n",
    "\n",
    "負の対数尤度とも見られるね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この交差エントロピーは当然言語モデルの学習でも使うことができる。実際に学習時の損失関数は交差エントロピーを使う。  \n",
    "しかし、言語モデルを評価する際は、交差エントロピーによって求めた損失をそのまま見るのではなく、それを少し変形したパープレキシティを用いる。\n",
    "\n",
    "$$\n",
    "\\text{ppl} = \\exp L = \\exp (- \\log y_k) = \\frac{1}{y_k}\n",
    "$$\n",
    "\n",
    "expoentialを取っただけ。結果的に「予測された正解単語の確率の逆数」になる。この値は**分岐数**とも呼ばれる。  \n",
    "例えば、予測された正解単語の確率が1/2の場合、パープレキシティは2となる。これはモデルが単語を2択まで絞れているように解釈できる。分岐数が少なければ、モデル自信をもって正解単語を予測できていると解釈できる。  \n",
    "このような解釈性が理由で、評価指標としてパープレキシティが用いられる。\n",
    "\n",
    "ちなみに複数データの場合はこう。交差エントロピー部分が平均になる。\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{ppl}\n",
    "    &= \\exp \\left( - \\frac{1}{N} \\sum_{i=1}^N \\log y_k^{(i)} \\right) \\\\\n",
    "    &= \\prod_{i=1}^N \\sqrt[N]{\\frac{1}{y_k^{(i)}}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "厳密には平均ではないが、各パープレキシティの平均として解釈しちゃってよい。相乗平均のようなイメージかな（相乗平均ともちょっと違うけど）。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習\n",
    "\n",
    "ではいよいよ学習させていく。  \n",
    "損失関数に交差エントロピーを設定し、通常の分類モデルと同じように学習する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = model(x)\n",
    "            loss = loss_fn(y, t)\n",
    "            losses.append(loss.item())\n",
    "    loss = sum(losses) / len(losses)\n",
    "    ppl = torch.exp(torch.tensor(loss)).item()\n",
    "    return ppl\n",
    "\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    prog.start(\n",
    "        n_iter=len(train_loader),\n",
    "        n_epochs=n_epochs,\n",
    "        unit=prog_unit,\n",
    "        label=\"ppl train\"\n",
    "    )\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for x, t in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device) # 入力\n",
    "            t = t.to(device) # 正解\n",
    "            y = model(x) # 出力\n",
    "            loss = loss_fn(y, t) # 損失\n",
    "            loss.backward() # 逆伝播\n",
    "            optimizer.step() # パラメータ更新\n",
    "            ppl = torch.exp(loss).item() # パープレキシティ\n",
    "            prog.update(ppl) # 進捗バー更新\n",
    "\n",
    "        if prog.now_epoch % prog_unit == 0:\n",
    "            test_ppl = eval_model(model)\n",
    "            prog.memo(f'test: {test_ppl:.5f}', no_step=True)\n",
    "        prog.memo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1-2/20: ############################## 100% [00:01:37.75] ppl train: 876.91764, test: 634.09088 \n",
      "  3-4/20: ############################## 100% [00:01:35.72] ppl train: 270.34231, test: 601.96765 \n",
      "  5-6/20: ############################## 100% [00:01:36.44] ppl train: 192.06430, test: 630.06244 \n",
      "  7-8/20: ############################## 100% [00:01:35.40] ppl train: 159.88602, test: 685.09576 \n",
      " 9-10/20: ############################## 100% [00:01:36.31] ppl train: 142.92731, test: 764.88733 \n",
      "11-12/20: ############################## 100% [00:01:34.97] ppl train: 132.34475, test: 861.72229 \n",
      "13-14/20: ############################## 100% [00:01:32.03] ppl train: 125.50464, test: 983.70300 \n",
      "15-16/20: ############################## 100% [00:01:32.25] ppl train: 120.48983, test: 1129.80676 \n",
      "17-18/20: ############################## 100% [00:01:32.10] ppl train: 116.86110, test: 1310.66492 \n",
      "19-20/20: ############################## 100% [00:01:32.40] ppl train: 113.93259, test: 1517.63538 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, n_epochs=20, prog_unit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語をピンポイントで予測するのは難しいので、テストデータに対する精度は低い様子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/nn.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 文章生成\n",
    "\n",
    "学習したモデルを用いて文章を生成する。\n",
    "\n",
    "学習させたモデルは、ある単語から次の単語を予測するモデルである。厳密には、ある単語IDを入力に取り、次の単語IDを出力するモデルである。  \n",
    "さらに厳密に言うと、出力は単語IDではなく確率分布である。この確率分布から次の単語IDをサンプリングすることで、次の単語を生成する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に文章を生成させてみる。マルコフモデル同様、あるトークンからの次のトークンの予測を繰り返すことで文章を生成する。\n",
    "\n",
    "初めのトークンはBOSとし、以下の条件を満たすまで単語の生成を続ける。\n",
    "- EOSが生成される\n",
    "- 単語数が指定した限度に達する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの出力から単語をサンプリングする関数\n",
    "def token_sampling(y):\n",
    "    \"\"\"\n",
    "    y: (1, n_vocab)\n",
    "    \"\"\"\n",
    "    y.squeeze_(0) # (n_vocab,)\n",
    "    y[unk_id] = -torch.inf # UNKがサンプリングされる確率を0にする\n",
    "    probs = F.softmax(y, dim=-1) # 確率分布に変換\n",
    "    token, = random.choices(range(n_vocab), weights=probs) # サンプリング\n",
    "    return token\n",
    "\n",
    "# 文章を生成する関数\n",
    "def generate_sentence(model, max_len=50):\n",
    "    model.eval()\n",
    "    token_id = bos_id\n",
    "    token_ids = []\n",
    "\n",
    "    # 終了条件を満たすまで単語を生成\n",
    "    while len(token_ids) <= max_len and token_id != eos_id:\n",
    "\n",
    "        # 入力する単語IDをtensorに変換\n",
    "        x = torch.tensor([token_id]).to(device)\n",
    "\n",
    "        # 次の単語の確率分布を予測\n",
    "        y = model(x)\n",
    "        token_id = token_sampling(y) # サンプリング\n",
    "        token_ids.append(token_id)\n",
    "\n",
    "    sentence = sp.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LanguageModel(n_vocab, hidden_size).to(device)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "『ドラマが見つかったりだすごし原景法を禁止する地主成熟、ゲーリングにとって内最大、学園新田次郎は大きな損害するに残存がジョナ (ものではない。ive経由を開\n",
      "195月の間にの犯を激怒り、マイケル・ラー的な物資である。産みにした「これらの複数のメール、マロリーは合衆国憲法違反(八幡宮原始末社できることが特筆」を意味する。\n",
      "and theyd.)と戦オランダ・カトリック教会の同一家の統合戦線戦へのしょう。\n",
      "19世紀末にまでに敗北の病院に日にませきた。王潮ら手打ちが始まる王セルティック・ブなさせており、正化等は様?』(監督・カリフ・テレビ・DVD販売店、4\n",
      "は社は主にホイナ=カストリ湾では、UShe 2に電話である(全国の選手で修正式の11月6日)に伝えられている。また、さし、ウルデ=ル人\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(generate_sentence(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マルコフモデル同様、直前の単語のみを予測に用いているため、不自然な文章が多く生成される。\n",
    "\n",
    "出来上がったモデルは優れたものではないが、深層学習を用いた言語モデル実装の基礎を学ぶことができた。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
