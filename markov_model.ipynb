{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# マルコフモデル\n",
    "\n",
    "*Markov Model*\n",
    "\n",
    "マルコフ過程に従う確率モデル。マルコフ連鎖とも。  \n",
    "マルコフ過程とは、ある状態から次の状態へ遷移する確率が、その状態のみに依存する確率過程のこと。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 文章生成\n",
    "\n",
    "マルコフモデルを用いて文章を生成する。状態を単語とし、ある単語の次に続く単語の確率を予測する。\n",
    "\n",
    "学習データとして、いくつかの単語列（=文章）を用意する。学習データに出現する全ての単語を対象に、その次に続く単語とその確率を記録することで、マルコフモデルが完成する。確率は単語の出現頻度から定める。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例として以下の単語列を考えてみよう。\n",
    "\n",
    "- 今日 は いい 天気 です\n",
    "- 私 は 今日 カレー を 食べ ました\n",
    "- 私 は カレー が 好き です\n",
    "\n",
    "この3つの単語列を学習させてマルコフモデルを作成する。\n",
    "\n",
    "まず、1つ目に選ばれる可能性のある単語として「今日」と「私」がある。どちらが選ばれるかはランダムであるが、「今日」は1回、「私」は2回出現しているので、「今日」が選ばれる確率は1/3、「私」が選ばれる確率は2/3と定義する。  \n",
    "次に、この次に選ばれる可能性のある単語と確率を定義する。「今日」の次に来る可能性がある単語は「は」と「カレー」で、出現頻度は同じであるため確率は一様となる。  \n",
    "この流れで全ての単語について次に続く単語と確率を定義するとこうなる。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'今日': {'は': 0.5, 'カレー': 0.5},\n",
       " 'は': {'いい': 0.3333333333333333,\n",
       "  '今日': 0.3333333333333333,\n",
       "  'カレー': 0.3333333333333333},\n",
       " 'いい': {'天気': 1.0},\n",
       " '天気': {'です': 1.0},\n",
       " '私': {'は': 1.0},\n",
       " 'カレー': {'を': 0.5, 'が': 0.5},\n",
       " 'を': {'食べ': 1.0},\n",
       " '食べ': {'ました': 1.0},\n",
       " 'が': {'好き': 1.0},\n",
       " '好き': {'です': 1.0}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    \"今日 は いい 天気 です\",\n",
    "    \"私 は 今日 カレー を 食べ ました\",\n",
    "    \"私 は カレー が 好き です\"\n",
    "]\n",
    "\n",
    "words = {}\n",
    "for sent in data:\n",
    "    sent = sent.split()\n",
    "    for w1, w2 in zip(sent[:-1], sent[1:]):\n",
    "        if w1 not in words:\n",
    "            words[w1] = {}\n",
    "        if w2 not in words[w1]:\n",
    "            words[w1][w2] = 0\n",
    "        words[w1][w2] += 1\n",
    "\n",
    "for w1 in words:\n",
    "    total = sum(words[w1].values())\n",
    "    for w2 in words[w1]:\n",
    "        words[w1][w2] /= total\n",
    "\n",
    "words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「今日」の次は「は」と「カレー」が$\\frac{1}{2}$ずつ、「は」の次は「いい」「今日」「カレー」が$\\frac{1}{3}$ずつという風に確率を定義することが出来た。\n",
    "\n",
    "では、この確率を用いて文章を生成してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私 は 今日 カレー を 食べ ました\n",
      "今日 カレー が 好き です\n",
      "今日 は いい 天気 です\n",
      "今日 カレー が 好き です\n",
      "私 は いい 天気 です\n",
      "私 は 今日 カレー を 食べ ました\n",
      "今日 は 今日 は 今日 カレー を 食べ ました\n",
      "私 は いい 天気 です\n",
      "今日 カレー が 好き です\n",
      "私 は いい 天気 です\n"
     ]
    }
   ],
   "source": [
    "def generate(words, start_words):\n",
    "    word, = random.choices(list(start_words), weights=start_words.values())\n",
    "    sentence = [word]\n",
    "    while word in words:\n",
    "        next_words = words[word]\n",
    "        word, = random.choices(list(next_words), weights=next_words.values())\n",
    "        sentence.append(word)\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "for _ in range(10):\n",
    "    sentence = generate(words, {\"今日\": 1/3, \"私\": 2/3})\n",
    "    print(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然な文章もあれば、意味不明な文章もあるだろう。単語を予測する際に直前の単語のみを参照しているため、自然な文章を生成することは難しい。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### markovify\n",
    "\n",
    "マルコフモデルを用いて文章を生成するためのライブラリ。\n",
    "\n",
    "- [jsvine/markovify: A simple, extensible Markov chain generator.](https://github.com/jsvine/markovify)\n",
    "\n",
    "これを使うと簡単にマルコフモデルを用いた言語モデルを実装できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"今日 は いい 天気 です\",\n",
    "    \"私 は 今日 カレー を 食べ ました\",\n",
    "    \"私 は カレー が 好き です\"\n",
    "]\n",
    "model = markovify.Text(data, state_size=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで学習が完了した。文章を生成してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私 は カレー を 食べ ました\n",
      "私 は カレー を 食べ ました\n",
      "私 は カレー を 食べ ました\n",
      "私 は いい 天気 です\n",
      "今日 カレー が 好き です\n",
      "私 は カレー を 食べ ました\n",
      "私 は カレー を 食べ ました\n",
      "私 は 今日 カレー が 好き です\n",
      "私 は いい 天気 です\n",
      "今日 カレー が 好き です\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    sentence = model.make_sentence()\n",
    "    print(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 一般的なデータセットの活用\n",
    "\n",
    "ここまで、私が用意した3つの短文を学習データとして言語モデルを作成した。本節ではもう少し大きなデータセットを用いて言語モデルを作成する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wiki40b\n",
    "\n",
    "Wikipediaの記事を集めたデータセット。  \n",
    "Tensorflow Datasetsに収録されているものを使用する。\n",
    "\n",
    "[wiki40b  |  TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/wiki40b?hl=en#wiki40bja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-18 23:52:59.472848: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-18 23:52:59.943937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-06-18 23:53:00.444905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-18 23:53:00.465202: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-06-18 23:53:00.502251: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_2}}]]\n",
      "2023-06-18 23:53:00.502495: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_4}}]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "ds = tfds.load('wiki40b/ja', split='test')\n",
    "ds = list(ds.as_numpy_iterator())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の様な辞書がまとまっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': b'\\n_START_ARTICLE_\\n\\xe3\\x83\\x93\\xe3\\x83\\xbc\\xe3\\x83\\x88\\xe3\\x81\\x9f\\xe3\\x81\\x91\\xe3\\x81\\x97\\xe3\\x81\\xae\\xe6\\x95\\x99\\xe7\\xa7\\x91\\xe6\\x9b\\xb8\\xe3\\x81\\xab\\xe8\\xbc\\x89\\xe3\\x82\\x89\\xe3\\x81\\xaa\\xe3\\x81\\x84\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe4\\xba\\xba\\xe3\\x81\\xae\\xe8\\xac\\x8e\\n_START_SECTION_\\n\\xe6\\xa6\\x82\\xe8\\xa6\\x81\\n_START_PARAGRAPH_\\n\\xe3\\x80\\x8c\\xe6\\x95\\x99\\xe7\\xa7\\x91\\xe6\\x9b\\xb8\\xe3\\x81\\xab\\xe3\\x81\\xaf\\xe6\\xb1\\xba\\xe3\\x81\\x97\\xe3\\x81\\xa6\\xe8\\xbc\\x89\\xe3\\x82\\x89\\xe3\\x81\\xaa\\xe3\\x81\\x84\\xe3\\x80\\x8d\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe4\\xba\\xba\\xe3\\x81\\xae\\xe8\\xac\\x8e\\xe3\\x82\\x84\\xe3\\x81\\x97\\xe3\\x81\\x8d\\xe3\\x81\\x9f\\xe3\\x82\\x8a\\xe3\\x82\\x92\\xe5\\xa4\\x9a\\xe8\\xa7\\x92\\xe7\\x9a\\x84\\xe3\\x81\\xab\\xe6\\xa4\\x9c\\xe8\\xa8\\xbc\\xe3\\x81\\x97\\xe3\\x80\\x81\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe4\\xba\\xba\\xe3\\x81\\xaeDNA\\xe3\\x82\\x92\\xe8\\xa7\\xa3\\xe6\\x98\\x8e\\xe3\\x81\\x99\\xe3\\x82\\x8b\\xe3\\x80\\x82_NEWLINE_\\xe6\\x96\\xb0\\xe6\\x98\\xa5\\xe7\\x95\\xaa\\xe7\\xb5\\x84\\xe3\\x81\\xa8\\xe3\\x81\\x97\\xe3\\x81\\xa6\\xe5\\xae\\x9a\\xe6\\x9c\\x9f\\xe7\\x9a\\x84\\xe3\\x81\\xab\\xe6\\x94\\xbe\\xe9\\x80\\x81\\xe3\\x81\\x95\\xe3\\x82\\x8c\\xe3\\x81\\xa6\\xe3\\x81\\x8a\\xe3\\x82\\x8a\\xe3\\x80\\x81\\xe5\\xb9\\xb4\\xe6\\x9c\\xab\\xe3\\x81\\xae\\xe5\\x8d\\x88\\xe5\\x89\\x8d\\xe4\\xb8\\xad\\xe3\\x81\\xab\\xe5\\x86\\x8d\\xe6\\x94\\xbe\\xe9\\x80\\x81\\xe3\\x81\\x95\\xe3\\x82\\x8c\\xe3\\x82\\x8b\\xe3\\x81\\xae\\xe3\\x81\\x8c\\xe6\\x81\\x92\\xe4\\xbe\\x8b\\xe3\\x81\\xa8\\xe3\\x81\\xaa\\xe3\\x81\\xa3\\xe3\\x81\\xa6\\xe3\\x81\\x84\\xe3\\x82\\x8b\\xe3\\x80\\x82',\n",
       " 'version_id': b'1848243370795951995',\n",
       " 'wikidata_id': b'Q11331136'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = ds[0]\n",
    "ex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストの中身はこんな感じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_START_ARTICLE_\n",
      "ビートたけしの教科書に載らない日本人の謎\n",
      "_START_SECTION_\n",
      "概要\n",
      "_START_PARAGRAPH_\n",
      "「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。_NEWLINE_新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。\n"
     ]
    }
   ],
   "source": [
    "print(ex['text'].decode())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "セクションごとにまとめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 89698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。',\n",
       " 'ライブドア社員であった初代代表取締役社長の山名真由によって企業内起業の形で創業。2005年に株式会社ライブドアから分割されて設立。かつてはライブドアホールディングス（現・LDH）の子会社であったが、ノンコア事業の整理にともない、株式会社ゲオ（現：株式会社ゲオホールディングス）に所有する全株式を譲渡し、同社の完全子会社となった。「ぽすれん」「ゲオ宅配レンタル」のオンラインDVD・CD・コミックレンタルサービス及び「GEO Online」と「ゲオアプリ」のアプリ・ウェブサイト運営の大きく分けて2事業を展開している。以前はDVD販売等のEコマースサービス「ぽすれんストア」、動画配信コンテンツ「ぽすれんBB」や電子書籍配信サービスの「GEO☆Books」事業も行っていた。オンラインDVDレンタル事業では会員数は10万人（2005年9月時点）。2006年5月よりCDレンタルを開始。同業他社には、カルチュア・コンビニエンス・クラブが運営する『TSUTAYA DISCAS』のほか、DMM.comが運営する『DMM.com オンラインDVDレンタル』がある。過去には「Yahoo!レンタルDVD」と「楽天レンタル」の運営を受託していた。',\n",
       " '2005年の一時期、東京のラジオ局、InterFMで、「堀江社長も使っているライブドアのぽすれん」というキャッチコピーでラジオCMを頻繁に行っていたことがあった。',\n",
       " '香川県内の農業協同組合の信用事業を統括する県域農協系金融機関であり、県内農業協同組合を会員とする。香川県は全県単一農協の香川県農業協同組合となったが、先に単一農協となった奈良県や沖縄県のケースと異なり、信連の統合は行われなかった。通称は「JA香川信連」または「JAバンク香川」。統一金融機関コードは3037。主に法人顧客を中心としており、個人取引は殆どない。県内の大型商業施設にある、他金融機関管理の共同ATMには香川信連の管轄のものがある。',\n",
       " '534年（永熙3年）、独孤信の子として生まれた。独孤信が父母妻子を捨てて長安に入ったため、独孤羅は東魏に取り残されて高氏の虜囚となった。独孤信が宇文護により処刑されると、ようやく釈放されて、中山に寓居した。北斉の独孤永業の一族として田宅を与えられた。北斉が滅亡し、楊堅が定州総管となると、楊堅の妻の独孤伽羅が兄の行方を捜索させて独孤羅を見つけ出し、初めて対面した。579年（大象元年）、功臣の子として楚安郡太守に任じられた。まもなく病のため辞職し、長安に帰った。580年（大象2年）、楊堅が北周の丞相となると、独孤羅は儀同大将軍の位を受け、楊堅の側近に仕えた。581年（開皇元年）、隋が建国されると、使持節・上開府・儀同大将軍の位を受けた。11月、右武衛将軍に転じた。582年（開皇2年）、父の趙国公の爵位を嗣いだ。592年（開皇12年）、大将軍・太子右衛率となった。593年（開皇13年）、涼州刺史に任じられた。597年（開皇17年）、涼州総管に任じられた。599年（開皇19年）2月6日、死去した。諡は徳といった。']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for sample in ds:\n",
    "    text = sample['text'].decode()\n",
    "    sections = text.split('_START_SECTION_')\n",
    "    for section in sections[1:]:\n",
    "        sentence = section.split('_START_PARAGRAPH_')[1]\n",
    "        sentence = sentence.replace('_NEWLINE_', '')\n",
    "        sentence = sentence.replace('\\n', '')\n",
    "        data.append(sentence)\n",
    "\n",
    "print('num of data:', len(data))\n",
    "data[:5] # examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分かち書き\n",
    "\n",
    "テキストを単語ごとに分割する。英語は元から単語ごとに分割されているが、日本語はそうではないので別途行う必要がある。  \n",
    "分かち書きには形態素解析器を使用する。色々な種類があるが、ここでは**MeCab**を使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'私 は 猫 が 好き です 。 \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = MeCab.Tagger('-Owakati') # 出力形式を分かち書きに指定\n",
    "result = tagger.parse('私は猫が好きです。')\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを使って、学習データの全てを単語ごとに分割する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['「 教科 書 に は 決して 載ら ない 」 日本 人 の 謎 や しきたり を 多角 的 に 検証 し 、 日本 人 の DNA を 解明 する 。 新春 番組 と し て 定期 的 に 放送 さ れ て おり 、 年末 の 午前 中 に 再 放送 さ れる の が 恒例 と なっ て いる 。',\n",
       " 'ライブ ドア 社員 で あっ た 初代 代表 取締 役 社長 の 山名 真由 に よっ て 企業 内 起業 の 形 で 創業 。 2005 年 に 株式 会社 ライブ ドア から 分割 さ れ て 設立 。 かつて は ライブ ドア ホールディングス （ 現 ・ LDH ） の 子 会社 で あっ た が 、 ノン コア 事業 の 整理 に ともない 、 株式 会社 ゲオ （ 現 ： 株式 会社 ゲオ ホールディングス ） に 所有 する 全 株式 を 譲渡 し 、 同社 の 完全 子 会社 と なっ た 。 「 ぽす れん 」 「 ゲオ 宅配 レンタル 」 の オン ライン DVD ・ CD ・ コミック レンタル サービス 及び 「 GEO Online 」 と 「 ゲオ アプリ 」 の アプリ ・ ウェブサイト 運営 の 大きく 分け て 2 事業 を 展開 し て いる 。 以前 は DVD 販売 等 の E コマース サービス 「 ぽす れん ストア 」 、 動画 配信 コンテンツ 「 ぽす れん BB 」 や 電子 書籍 配信 サービス の 「 GEO ☆ Books 」 事業 も 行っ て い た 。 オン ライン DVD レンタル 事業 で は 会員 数 は 10 万 人 （ 2005 年 9 月 時点 ） 。 2006 年 5 月 より CD レンタル を 開始 。 同業 他社 に は 、 カルチュア ・ コンビニエンス ・ クラブ が 運営 する 『 TSUTAYA DISCAS 』 の ほか 、 DMM . com が 運営 する 『 DMM . com オン ライン DVD レンタル 』 が ある 。 過去 に は 「 Yahoo ! レンタル DVD 」 と 「 楽天 レンタル 」 の 運営 を 受託 し て い た 。',\n",
       " '2005 年 の 一 時期 、 東京 の ラジオ 局 、 InterFM で 、 「 堀江 社長 も 使っ て いる ライブ ドア の ぽす れん 」 と いう キャッチ コピー で ラジオ CM を 頻繁 に 行っ て い た こと が あっ た 。',\n",
       " '香川 県 内 の 農業 協同 組合 の 信用 事業 を 統括 する 県域 農協 系 金融 機関 で あり 、 県内 農業 協同 組合 を 会員 と する 。 香川 県 は 全県 単一 農協 の 香川 県 農業 協同 組合 と なっ た が 、 先 に 単一 農協 と なっ た 奈良 県 や 沖縄 県 の ケース と 異なり 、 信連 の 統合 は 行わ れ なかっ た 。 通称 は 「 JA 香川 信連 」 また は 「 JA バンク 香川 」 。 統一 金融 機関 コード は 3037 。 主に 法人 顧客 を 中心 と し て おり 、 個人 取引 は 殆ど ない 。 県 内 の 大型 商業 施設 に ある 、 他 金融 機関 管理 の 共同 ATM に は 香川 信連 の 管轄 の もの が ある 。',\n",
       " '534 年 （ 永 熙 3 年 ） 、 独 孤 信 の 子 と し て 生まれ た 。 独 孤 信 が 父母 妻子 を 捨て て 長安 に 入っ た ため 、 独 孤 羅 は 東魏 に 取り残さ れ て 高 氏 の 虜囚 と なっ た 。 独 孤 信 が 宇文 護 に より 処刑 さ れる と 、 ようやく 釈放 さ れ て 、 中山 に 寓居 し た 。 北斉 の 独 孤 永業 の 一族 と し て 田宅 を 与え られ た 。 北斉 が 滅亡 し 、 楊堅 が 定 州 総管 と なる と 、 楊堅 の 妻 の 独 孤 伽羅 が 兄 の 行方 を 捜索 さ せ て 独 孤 羅 を 見つけ 出し 、 初めて 対面 し た 。 579 年 （ 大 象 元年 ） 、 功臣 の 子 と し て 楚 安 郡 太守 に 任じ られ た 。 ま も なく 病 の ため 辞職 し 、 長安 に 帰っ た 。 580 年 （ 大 象 2 年 ） 、 楊堅 が 北周 の 丞相 と なる と 、 独 孤 羅 は 儀同 大将軍 の 位 を 受け 、 楊堅 の 側近 に 仕え た 。 581 年 （ 開皇 元年 ） 、 隋 が 建国 さ れる と 、 使 持節 ・ 上 開府 ・ 儀同 大将軍 の 位 を 受け た 。 11 月 、 右 武衛 将軍 に 転じ た 。 582 年 （ 開皇 2 年 ） 、 父 の 趙 国公 の 爵位 を 嗣い だ 。 592 年 （ 開皇 12 年 ） 、 大将軍 ・ 太子 右 衛 率 と なっ た 。 593 年 （ 開皇 13 年 ） 、 涼州 刺史 に 任じ られ た 。 597 年 （ 開皇 17 年 ） 、 涼州 総管 に 任じ られ た 。 599 年 （ 開皇 19 年 ） 2 月 6 日 、 死去 し た 。 諡 は 徳 と いっ た 。']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wakati = []\n",
    "for sentence in data:\n",
    "    data_wakati.append(tagger.parse(sentence).strip())\n",
    "\n",
    "data_wakati[:5] # examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n",
    "\n",
    "markovifyを使って学習を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = markovify.Text(data_wakati, state_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高から楕円銀河群は山陽本線・小原とのみだりなスプリング・ハイデンについていたの大きな花弁の、ライジング福岡藩の評議員となる。総攻撃に完全になっても、1990年に定年、『プロフェッショナル向けて、彼女は大ファンから、国内オーケストラは広窓を結んだレースウェイブスへの輸送のカロッツェリア・ビデオにて内閣が小腸は57打点と隣接町、歯を獲得にそれぞれ接する。\n",
      "\n",
      "西安市内の冬季のがエスペランス地区をつなぐアメリカ軍と無線が存在し、一時的に、男性の解明を除いた。\n",
      "\n",
      "基本的な種\n",
      "\n",
      "『読売ジャイアンツと、中国仏教反対するために所属した。金はストレートで、名誉な資金をくす玉の中で開催、同じ錯視は捕虜との寿命は自身に始まりで不足につながる一〜ネイビーFCユトレヒト条約を防衛のみの下り降車専用実施。県都モンツァで一つの1次のボールを達成し、反攻するようにマップも存在と同じ3キーパーでは北京首都シュリーランガパトナを記録しながら、『食人とする機能が、ダーバンまでカリフォルニア州選手層のエミリー・ワトソンという。元に多大なる。2015年には同級生およびメゼールとなり鳥栖筑紫野市町教育学区の監督の高い住宅地の清瀧寺村という言葉を利用である。これをあげたが就任後、X線画像、ノースカロライナ大学文学院博士課程修了。デュボスのため、Java系統推定された。ゴールディングのめざましい工業、志願した。イギリス議会は父は河原崎の警戒本部長されることで『ジュノン・コズグリフリッチ・主砲ブラディミール・ゲレーロの舞曲レントラーが進んだが本作を終え、民衆暴動用にもなお車両でプロキックに足場また京滋学生でしか浮かばないとなっている。それ以前から途中でメキシコシティFCウニオン・コミュニケーションズ販売先に結婚しての中のプロダクトプレイスメントにパドレスと、予備予選のある限りで整備などベルイマンの構成を果たした。非常に上昇後、特に前頭10日。2010年6,14歳であり、平原の直前のほうが、ロタール山地はごく少ないらしい。ミジンコダグ・慶長の主張できる。大学出身の改築期間に積雪するにまでレギュラーシーズンあたり210ので負極側扁した。また、宮となる。一部となった。その先に中尉は、32号鴻巣市域直上の選定しての中心に鉄道輸送品が仮定しては街にいかに有用であると、複数の準優勝。サウスウエスト、これに面し、東に増やしつつも、4．……。またロシア、製作マイスターとした町東線のコースを修了、現世代における多くはアメリカ史上では、同年度のFIFAクラブと戦うが、溶接の皇子マヌエル・オブ・159では誰である。グウィンは全選手だった。城跡に入ってさまざまな構造解析しようと修士を犯したが発生しているがいる。よって公開されたと思いが必要とバスケットボール部タッグ王座決定戦にてシナリオ等の機体は民俗学の裸出し、7月初めの戦後の送電線を初めての各部フェルトの情報提供も存在してのヒットと数える指揮を結んだがあり、炭酸カルシウム血だらけの逐次重合開始して瞼も内側は海軍の最後のトリプルスレット戦15インチ砲の主要盆地のアパートであった。水が城を迎えるトークが、先端が検査する人物視点に乗っていることが、明治22日から引き続き副萼片より上43打点、金銭を取得して設立しており、この橋や鳴子温泉駅から見られているが魚の始末するため、ガストンの野党のチューニングの自動車になった。1852年4mm、1月に変動が監督に大別されている。ラゲールガウシアンビームを使用することを後方に4月、馬具、クライアントサイドの地区が代表となり、実際に入りの二相は少なく、東京都港へ移籍した。\n",
      "\n",
      "1954年5、特に左はハーパーズ・ブランドン・ショッピングエリアの言い、PCIExpressは2月28日に初ゴール、3月の都合のようになったとした航空機小屋にほか、常温で亡くなったが一定期間が確定診断科植物防疫給水していたことの天使となり、電子を含有量が10日、石材とした。2章は全般の徴発を行っている。縁の成績である大規模は教員サークル活動する下地幹郎が出走を持つ世界はモチツツジには関東地区は日本であった\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    sentence = model.make_sentence().replace(' ', '')\n",
    "    print(sentence, end='\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "語彙が増えたことで多様な文章が生成されるようになった。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
