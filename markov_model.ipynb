{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# マルコフモデル\n",
    "\n",
    "*Markov Model*\n",
    "\n",
    "マルコフ過程に従う確率モデル。マルコフ連鎖とも。  \n",
    "マルコフ過程とは、ある状態から次の状態へ遷移する確率が、その状態のみに依存する確率過程のこと。\n",
    "\n",
    "本章では、このマルコフモデルを用いて言語モデルを実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "import markovify\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## マルコフモデルを用いた文章生成\n",
    "\n",
    "マルコフモデルによる言語モデル（以後マルコフモデルと呼ぶ）では、状態を単語とし、ある単語の次に続く単語の確率を予測する。  \n",
    "\n",
    "自然な文章を生成するためには、前章で作ったようなランダムな出力を行うモデルではなく、入力された文脈を考慮したモデルが必要となる。  \n",
    "マルコフモデルはそのようなモデルの中で特に理解しやすいモデルである。このモデルは、与えられた文脈の中の最後の単語に着目し、次の単語を予測する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マルコフモデルでは、モデルが持っている全ての語彙に対して、その次に続く単語とその確率を記述する。  \n",
    "例えば、モデルが以下の語彙を持っているとする。\n",
    "\n",
    "- 今日\n",
    "- カレー\n",
    "- 天気\n",
    "- おいしい\n",
    "- は\n",
    "- いい\n",
    "- 。\n",
    "\n",
    "これらに対して、次に続く単語とその確率を記述する。  \n",
    "適当に、主観で決めてみよう。\n",
    "\n",
    "単語 | 次に続く単語(確率)\n",
    "--- | ---\n",
    "今日 | は(1.0)\n",
    "カレー | は(1.0)\n",
    "天気 | は(0.5), 。(0.5)\n",
    "おいしい | 。(0.5), カレー(0.5)\n",
    "は | 今日(0.25), カレー(0.25), おいしい(0.25), いい(0.25)\n",
    "いい | 天気(0.5), 。(0.5)\n",
    "\n",
    "句点は文の終わりを表すので、次に続く単語はないと考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マルコフモデルは、この確率に基づいて次の単語を予測する。それを繰り返して文章を生成する。\n",
    "\n",
    "やってみよう。まずは語彙とその次に続く単語の確率を記述する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"今日\": {\"は\": 1},\n",
    "    \"カレー\": {\"は\": 1},\n",
    "    \"天気\": {\"は\": 0.5, \"。\": 0.5},\n",
    "    \"おいしい\": {\"。\": 0.5, \"カレー\": 0.5},\n",
    "    \"は\": {\"今日\": 0.25, \"カレー\": 0.25, \"おいしい\": 0.25, \"いい\": 0.25},\n",
    "    \"いい\": {\"天気\": 0.5, \"。\": 0.5},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして、このような確率に基づいて文章を生成するモデルを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordProb = Dict[str, float]\n",
    "\n",
    "class MarkovModel:\n",
    "    def __init__(self, vocab: Dict[str, WordProb]):\n",
    "        self.vocab = vocab # 語彙とその次の単語の確率\n",
    "\n",
    "    def generate_sentence(self, start_word: str) -> str:\n",
    "        word = start_word\n",
    "        sentence = [word]\n",
    "\n",
    "        # 句点が出るまでサンプリング\n",
    "        while word != \"。\":\n",
    "            next_words = self.vocab[word]\n",
    "            word, = random.choices(list(next_words), weights=next_words.values())\n",
    "            sentence.append(word)\n",
    "\n",
    "        return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。\n",
    "\n",
    "初めの単語はこちらで指定する必要がある。  \n",
    "いくつかの単語で試してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日 は 今日 は カレー は 今日 は カレー は おいしい 。\n",
      "カレー は カレー は いい 天気 は カレー は いい 天気 は 今日 は おいしい 。\n",
      "天気 。\n"
     ]
    }
   ],
   "source": [
    "model = MarkovModel(vocab)\n",
    "start_words = [\"今日\", \"カレー\", \"天気\"] # 最初の単語\n",
    "for start_word in start_words:\n",
    "    sentence = model.generate_sentence(start_word)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文脈を考慮したモデルが作成できた。  \n",
    "次に続く単語に対してまあまあ妥当な確率を手動で設定したので、多少は自然さが見られるはず。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## マルコフモデルの学習\n",
    "\n",
    "語彙と次の単語の確率をデータセットから自動で決定する。\n",
    "\n",
    "語彙に紐づいている次の単語の確率が、マルコフモデルの全てを表していると見られる。  \n",
    "前節ではこの確率を手動で定めたが、データを元に自動で決定する事を考える。\n",
    "\n",
    "このような、モデルの動きを決める値（マルコフモデルの場合は次に続く単語の確率）はパラメータと呼ぶ。そして、適切なパラメータをデータセットから自動で求める事を学習と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ではやってみよう。\n",
    "\n",
    "データセットとして、いくつかの単語列を用意する。そして、データセットに出現する全ての単語を対象に、その次に続く単語とその確率を記録する。こうすることでマルコフモデルが完成する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確率は単語の出現頻度から定める。  \n",
    "例として以下の3つの単語列をデータセットとして考えてみよう。\n",
    "\n",
    "- 今日 は いい 天気 です\n",
    "- 私 は 今日 カレー を 食べ ました\n",
    "- 私 は カレー が 好き です\n",
    "\n",
    "これらから適切な確率を定め、マルコフモデルを作成する。\n",
    "\n",
    "まず、1つ目に選ばれる可能性のある単語として「今日」と「私」がある。どちらが選ばれるかはランダムであるが、「今日」は1回、「私」は2回出現しているので、「今日」が選ばれる確率は1/3、「私」が選ばれる確率は2/3と定義する。  \n",
    "次に、この次に選ばれる可能性のある単語と確率を定義する。例えば、「今日」の次に来る可能性がある単語は「は」と「カレー」で、出現頻度は同じであるため確率は一様となる。また「私」の後は確定で「は」が来ることになる。  \n",
    "この流れで全ての単語について次に続く単語と確率を定義するとこうなる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"今日 は いい 天気 です 。\",\n",
    "    \"私 は 今日 カレー を 食べ ました 。\",\n",
    "    \"私 は カレー が 好き です 。\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'今日': {'は': 0.5, 'カレー': 0.5},\n",
       " 'は': {'いい': 0.3333333333333333,\n",
       "  '今日': 0.3333333333333333,\n",
       "  'カレー': 0.3333333333333333},\n",
       " 'いい': {'天気': 1.0},\n",
       " '天気': {'です': 1.0},\n",
       " 'です': {'。': 1.0},\n",
       " '私': {'は': 1.0},\n",
       " 'カレー': {'を': 0.5, 'が': 0.5},\n",
       " 'を': {'食べ': 1.0},\n",
       " '食べ': {'ました': 1.0},\n",
       " 'ました': {'。': 1.0},\n",
       " 'が': {'好き': 1.0},\n",
       " '好き': {'です': 1.0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "for sent in data:\n",
    "    sent = sent.split()\n",
    "    for w1, w2 in zip(sent[:-1], sent[1:]):\n",
    "        if w1 not in vocab:\n",
    "            vocab[w1] = {}\n",
    "        if w2 not in vocab[w1]:\n",
    "            vocab[w1][w2] = 0\n",
    "        vocab[w1][w2] += 1\n",
    "\n",
    "for w1 in vocab:\n",
    "    total = sum(vocab[w1].values())\n",
    "    for w2 in vocab[w1]:\n",
    "        vocab[w1][w2] /= total\n",
    "\n",
    "vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「今日」の次は「は」と「カレー」が$\\frac{1}{2}$ずつ、「は」の次は「いい」「今日」「カレー」が$\\frac{1}{3}$ずつという風に確率を定義することが出来た。\n",
    "\n",
    "これで、マルコフモデルの学習が完了したことになる。  \n",
    "このモデルでいくつか文章を生成してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日 カレー を 食べ ました 。\n",
      "私 は 今日 カレー が 好き です 。\n",
      "私 は 今日 カレー を 食べ ました 。\n",
      "私 は いい 天気 です 。\n",
      "今日 は いい 天気 です 。\n",
      "今日 カレー が 好き です 。\n",
      "私 は カレー が 好き です 。\n",
      "今日 は いい 天気 です 。\n",
      "今日 カレー を 食べ ました 。\n",
      "今日 カレー が 好き です 。\n"
     ]
    }
   ],
   "source": [
    "model = MarkovModel(vocab)\n",
    "\n",
    "for _ in range(10):\n",
    "    start_word, = random.choices([\"今日\", \"私\"], weights=[1/3, 2/3]) # 最初の単語\n",
    "    sentence = model.generate_sentence(start_word)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ランダムなモデルよりは自然な文章が生成されているのではないだろうか。\n",
    "\n",
    "これで、データセットからマルコフモデルを学習し、文章を生成することが出来た。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### markovify\n",
    "\n",
    "マルコフモデルを用いて文章を生成するためのライブラリ。\n",
    "\n",
    "- [jsvine/markovify: A simple, extensible Markov chain generator.](https://github.com/jsvine/markovify)\n",
    "\n",
    "これを使うと簡単にマルコフモデルを用いた言語モデルを実装できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['今日 は いい 天気 です 。', '私 は 今日 カレー を 食べ ました 。', '私 は カレー が 好き です 。']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data # 再掲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = markovify.Text(data, state_size=1) # 学習"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで学習が完了した。文章を生成してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日 は カレー を 食べ ました 。\n",
      "今日 は カレー を 食べ ました 。\n",
      "今日 は 今日 カレー が 好き です 。\n",
      "私 は カレー を 食べ ました 。\n",
      "私 は 今日 カレー が 好き です 。\n",
      "私 は カレー を 食べ ました 。\n",
      "私 は 今日 カレー が 好き です 。\n",
      "私 は カレー を 食べ ました 。\n",
      "私 は 今日 は 今日 は カレー が 好き です 。\n",
      "私 は 今日 は カレー が 好き です 。\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    sentence = model.make_sentence()\n",
    "    print(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 一般的なデータセットの活用\n",
    "\n",
    "ここまで、私が用意した3つの短文をデータセットとして言語モデルを作成した。  \n",
    "本節ではもう少し大きなデータセットを用いて言語モデルを作成する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wiki40b\n",
    "\n",
    "Wikipediaの記事を集めたデータセット。  \n",
    "Tensorflow Datasetsに収録されているものを使用する。\n",
    "\n",
    "[wiki40b  |  TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/wiki40b?hl=en#wiki40bja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\" # tensorflowのログを非表示\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load(\"wiki40b/ja\", split=\"test\", data_dir=\"data\")\n",
    "ds = list(ds.as_numpy_iterator())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "辞書のリストが得られる。  \n",
    "`ds: List[Dict[str, bytes]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': b'\\n_START_ARTICLE_\\n\\xe3\\x83\\x93\\xe3\\x83\\xbc\\xe3\\x83\\x88\\xe3\\x81\\x9f\\xe3\\x81\\x91\\xe3\\x81\\x97\\xe3\\x81\\xae\\xe6\\x95\\x99\\xe7\\xa7\\x91\\xe6\\x9b\\xb8\\xe3\\x81\\xab\\xe8\\xbc\\x89\\xe3\\x82\\x89\\xe3\\x81\\xaa\\xe3\\x81\\x84\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe4\\xba\\xba\\xe3\\x81\\xae\\xe8\\xac\\x8e\\n_START_SECTION_\\n\\xe6\\xa6\\x82\\xe8\\xa6\\x81\\n_START_PARAGRAPH_\\n\\xe3\\x80\\x8c\\xe6\\x95\\x99\\xe7\\xa7\\x91\\xe6\\x9b\\xb8\\xe3\\x81\\xab\\xe3\\x81\\xaf\\xe6\\xb1\\xba\\xe3\\x81\\x97\\xe3\\x81\\xa6\\xe8\\xbc\\x89\\xe3\\x82\\x89\\xe3\\x81\\xaa\\xe3\\x81\\x84\\xe3\\x80\\x8d\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe4\\xba\\xba\\xe3\\x81\\xae\\xe8\\xac\\x8e\\xe3\\x82\\x84\\xe3\\x81\\x97\\xe3\\x81\\x8d\\xe3\\x81\\x9f\\xe3\\x82\\x8a\\xe3\\x82\\x92\\xe5\\xa4\\x9a\\xe8\\xa7\\x92\\xe7\\x9a\\x84\\xe3\\x81\\xab\\xe6\\xa4\\x9c\\xe8\\xa8\\xbc\\xe3\\x81\\x97\\xe3\\x80\\x81\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe4\\xba\\xba\\xe3\\x81\\xaeDNA\\xe3\\x82\\x92\\xe8\\xa7\\xa3\\xe6\\x98\\x8e\\xe3\\x81\\x99\\xe3\\x82\\x8b\\xe3\\x80\\x82_NEWLINE_\\xe6\\x96\\xb0\\xe6\\x98\\xa5\\xe7\\x95\\xaa\\xe7\\xb5\\x84\\xe3\\x81\\xa8\\xe3\\x81\\x97\\xe3\\x81\\xa6\\xe5\\xae\\x9a\\xe6\\x9c\\x9f\\xe7\\x9a\\x84\\xe3\\x81\\xab\\xe6\\x94\\xbe\\xe9\\x80\\x81\\xe3\\x81\\x95\\xe3\\x82\\x8c\\xe3\\x81\\xa6\\xe3\\x81\\x8a\\xe3\\x82\\x8a\\xe3\\x80\\x81\\xe5\\xb9\\xb4\\xe6\\x9c\\xab\\xe3\\x81\\xae\\xe5\\x8d\\x88\\xe5\\x89\\x8d\\xe4\\xb8\\xad\\xe3\\x81\\xab\\xe5\\x86\\x8d\\xe6\\x94\\xbe\\xe9\\x80\\x81\\xe3\\x81\\x95\\xe3\\x82\\x8c\\xe3\\x82\\x8b\\xe3\\x81\\xae\\xe3\\x81\\x8c\\xe6\\x81\\x92\\xe4\\xbe\\x8b\\xe3\\x81\\xa8\\xe3\\x81\\xaa\\xe3\\x81\\xa3\\xe3\\x81\\xa6\\xe3\\x81\\x84\\xe3\\x82\\x8b\\xe3\\x80\\x82',\n",
       " 'version_id': b'1848243370795951995',\n",
       " 'wikidata_id': b'Q11331136'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = ds[0]\n",
    "ex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストの中身はこんな感じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_START_ARTICLE_\n",
      "ビートたけしの教科書に載らない日本人の謎\n",
      "_START_SECTION_\n",
      "概要\n",
      "_START_PARAGRAPH_\n",
      "「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。_NEWLINE_新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。\n"
     ]
    }
   ],
   "source": [
    "print(ex[\"text\"].decode())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "セクションごとにまとめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 89698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。',\n",
       " 'ライブドア社員であった初代代表取締役社長の山名真由によって企業内起業の形で創業。2005年に株式会社ライブドアから分割されて設立。かつてはライブドアホールディングス（現・LDH）の子会社であったが、ノンコア事業の整理にともない、株式会社ゲオ（現：株式会社ゲオホールディングス）に所有する全株式を譲渡し、同社の完全子会社となった。「ぽすれん」「ゲオ宅配レンタル」のオンラインDVD・CD・コミックレンタルサービス及び「GEO Online」と「ゲオアプリ」のアプリ・ウェブサイト運営の大きく分けて2事業を展開している。以前はDVD販売等のEコマースサービス「ぽすれんストア」、動画配信コンテンツ「ぽすれんBB」や電子書籍配信サービスの「GEO☆Books」事業も行っていた。オンラインDVDレンタル事業では会員数は10万人（2005年9月時点）。2006年5月よりCDレンタルを開始。同業他社には、カルチュア・コンビニエンス・クラブが運営する『TSUTAYA DISCAS』のほか、DMM.comが運営する『DMM.com オンラインDVDレンタル』がある。過去には「Yahoo!レンタルDVD」と「楽天レンタル」の運営を受託していた。',\n",
       " '2005年の一時期、東京のラジオ局、InterFMで、「堀江社長も使っているライブドアのぽすれん」というキャッチコピーでラジオCMを頻繁に行っていたことがあった。']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for sample in ds:\n",
    "    text = sample[\"text\"].decode()\n",
    "    sections = text.split(\"_START_SECTION_\")\n",
    "    for section in sections[1:]:\n",
    "        sentence = section.split(\"_START_PARAGRAPH_\")[1]\n",
    "        sentence = sentence.replace(\"_NEWLINE_\", \"\")\n",
    "        sentence = sentence.replace(\"\\n\", \"\")\n",
    "        data.append(sentence)\n",
    "\n",
    "print(\"num of data:\", len(data))\n",
    "data[:3] # examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後の章でも使うので書き出しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = \"data/jawiki.txt\"\n",
    "with open(text_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークン化\n",
    "\n",
    "テキストをトークンごとに分割する。トークンとはモデルが扱える最小単位のことで、例えば単語が該当する。  \n",
    "本節でも単語を最小単位=トークンとして、文章を単語列に変換する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "英語の文章は単語間にスペースが入っているので、それで終わりである。特にこちらで行う処理はない。  \n",
    "一方で日本語の文章はそうではないので、こちらで分割する必要がある。\n",
    "\n",
    "日本語トークン化には形態素解析器を使用する。これは自然言語処理で形態素解析（品詞の分析）に使用するツール。  \n",
    "色々な種類があるが、ここではMeCabを使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'私 は 猫 が 好き です 。 \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = MeCab.Tagger(\"-Owakati\") # 出力形式を分かち書きに指定\n",
    "result = tagger.parse(\"私は猫が好きです。\")\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを使って、学習データの全てを単語ごとに分割する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['「 教科 書 に は 決して 載ら ない 」 日本 人 の 謎 や しきたり を 多角 的 に 検証 し 、 日本 人 の DNA を 解明 する 。 新春 番組 と し て 定期 的 に 放送 さ れ て おり 、 年末 の 午前 中 に 再 放送 さ れる の が 恒例 と なっ て いる 。',\n",
       " 'ライブ ドア 社員 で あっ た 初代 代表 取締 役 社長 の 山名 真由 に よっ て 企業 内 起業 の 形 で 創業 。 2005 年 に 株式 会社 ライブ ドア から 分割 さ れ て 設立 。 かつて は ライブ ドア ホールディングス （ 現 ・ LDH ） の 子 会社 で あっ た が 、 ノン コア 事業 の 整理 に ともない 、 株式 会社 ゲオ （ 現 ： 株式 会社 ゲオ ホールディングス ） に 所有 する 全 株式 を 譲渡 し 、 同社 の 完全 子 会社 と なっ た 。 「 ぽす れん 」 「 ゲオ 宅配 レンタル 」 の オン ライン DVD ・ CD ・ コミック レンタル サービス 及び 「 GEO Online 」 と 「 ゲオ アプリ 」 の アプリ ・ ウェブサイト 運営 の 大きく 分け て 2 事業 を 展開 し て いる 。 以前 は DVD 販売 等 の E コマース サービス 「 ぽす れん ストア 」 、 動画 配信 コンテンツ 「 ぽす れん BB 」 や 電子 書籍 配信 サービス の 「 GEO ☆ Books 」 事業 も 行っ て い た 。 オン ライン DVD レンタル 事業 で は 会員 数 は 10 万 人 （ 2005 年 9 月 時点 ） 。 2006 年 5 月 より CD レンタル を 開始 。 同業 他社 に は 、 カルチュア ・ コンビニエンス ・ クラブ が 運営 する 『 TSUTAYA DISCAS 』 の ほか 、 DMM . com が 運営 する 『 DMM . com オン ライン DVD レンタル 』 が ある 。 過去 に は 「 Yahoo ! レンタル DVD 」 と 「 楽天 レンタル 」 の 運営 を 受託 し て い た 。',\n",
       " '2005 年 の 一 時期 、 東京 の ラジオ 局 、 InterFM で 、 「 堀江 社長 も 使っ て いる ライブ ドア の ぽす れん 」 と いう キャッチ コピー で ラジオ CM を 頻繁 に 行っ て い た こと が あっ た 。']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wakati = []\n",
    "for sentence in data:\n",
    "    data_wakati.append(tagger.parse(sentence).strip())\n",
    "\n",
    "data_wakati[:3] # examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n",
    "\n",
    "markovifyを使ってマルコフモデルを学習させ、文章を生成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = markovify.Text(data_wakati, state_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沿海地方に位置し独房によりシニアチームが生まれ、国際タイトルを持つことからであったと同様に比例区を調べるようにブラジルで優位性から頻繁な飛行場線に清国である。\n",
      "1982年7月26日の間部・アランダ川を要する。\n",
      "大学招聘。\n",
      "ニトログリセリンのフェアリング装着方法を受け取ることが勃発する。\n",
      "2016年4得点も、概ね相当するものは、及び西側は新しい者はお告げがある。\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    sentence = model.make_sentence().replace(\" \", \"\")\n",
    "    print(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "語彙が増えたことで多様な文章が生成されるようになった。\n",
    "\n",
    "ただ、文脈の最後の単語しか考慮できないので、文章全体での自然さを作り出すことは難しい。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
