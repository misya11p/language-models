{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# マルコフモデル\n",
    "\n",
    "まずはシンプルなモデルで言語モデルを学ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os; os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "import markovify\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## マルコフモデル\n",
    "\n",
    "*Markov Model*\n",
    "\n",
    "マルコフ過程に従う確率モデル。マルコフ連鎖とも。マルコフ過程とは、ある状態から次の状態へ遷移する確率が、その状態のみに依存する確率過程のこと。\n",
    "\n",
    "$$\n",
    "p(x_t | x_{t-1}, x_{t-2}, \\ldots, x_1) = p(x_t | x_{t-1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## マルコフモデルを用いた言語モデル\n",
    "\n",
    "マルコフモデルによる言語モデル（以後マルコフモデルと呼ぶ）では、状態を単語とし、ある単語の次に続く単語の確率を記述する。\n",
    "\n",
    "自然な文章を生成するためには、前章で作ったようなランダムな出力を行うモデルではなく、入力された文脈を考慮したモデルが必要となる。マルコフモデルはそのようなモデルの中で特に理解しやすいシンプルなモデルである。このモデルは、与えられた文脈の中の最後の単語のみに着目し、次の単語を予測する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マルコフモデルでは、モデルが持っている全ての語彙に対して、その次に続く単語とその確率を記述する。\n",
    "\n",
    "例えば、モデルが以下の語彙を持っているとする。\n",
    "\n",
    "- 今日\n",
    "- カレー\n",
    "- 天気\n",
    "- おいしい\n",
    "- は\n",
    "- いい\n",
    "- 。\n",
    "\n",
    "これらに対して、次に続く単語とその確率を定める。適当に、主観で決めてみよう。\n",
    "\n",
    "単語 | 次に続く単語(確率)\n",
    "--- | ---\n",
    "今日 | は(1.0)\n",
    "カレー | は(1.0)\n",
    "天気 | は(0.5), 。(0.5)\n",
    "おいしい | 。(0.5), カレー(0.5)\n",
    "は | 今日(0.25), カレー(0.25), おいしい(0.25), いい(0.25)\n",
    "いい | 天気(0.5), 。(0.5)\n",
    "。 |\n",
    "\n",
    "句点は文の終わりを表すので、次に続く単語はないと考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "辞書として定義しておこう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"今日\": {\"は\": 1},\n",
    "    \"カレー\": {\"は\": 1},\n",
    "    \"天気\": {\"は\": 0.5, \"。\": 0.5},\n",
    "    \"おいしい\": {\"。\": 0.5, \"カレー\": 0.5},\n",
    "    \"は\": {\"今日\": 0.25, \"カレー\": 0.25, \"おいしい\": 0.25, \"いい\": 0.25},\n",
    "    \"いい\": {\"天気\": 0.5, \"。\": 0.5},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マルコフモデルは、この確率に基づいて次の単語を予測する。それを繰り返して文章を生成する。\n",
    "\n",
    "モデルを作ってみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordProb = Dict[str, float]\n",
    "\n",
    "class MarkovModel:\n",
    "    def __init__(self, vocab: Dict[str, WordProb]):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, context):\n",
    "        last_word = context[-1]\n",
    "        words, dist = zip(*self.vocab[last_word].items())\n",
    "        word, = random.choices(words, weights=dist)\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "できた。これで文章を生成してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model: MarkovModel, start_word: str):\n",
    "    context = [start_word]\n",
    "    next_word = start_word\n",
    "    while next_word != \"。\":\n",
    "        next_word = model(context)\n",
    "        context.append(next_word)\n",
    "    return \" \".join(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初めの単語はこちらで指定する必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日 は 今日 は おいしい 。\n",
      "カレー は 今日 は いい 天気 は 今日 は カレー は カレー は 今日 は 今日 は おいしい 。\n"
     ]
    }
   ],
   "source": [
    "model = MarkovModel(vocab)\n",
    "print(generate_sentence(model, \"今日\"))\n",
    "print(generate_sentence(model, \"カレー\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文脈を考慮したモデルが作成できた。次に続く単語に対してまあまあ妥当な確率を手動で設定したので、少しは自然さが見られるはず。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## マルコフモデルの学習\n",
    "\n",
    "語彙と次の単語の確率をデータセットから自動で決定する。\n",
    "\n",
    "語彙に紐づいている次の単語の確率が、マルコフモデルの全てを表していると見られる。  \n",
    "前節ではこの確率を手動で定めたが、データを元に自動で設定する事を考える。\n",
    "\n",
    "このような、モデルの動きを決める値（マルコフモデルの場合は次に続く単語の確率）はパラメータと呼ぶ。そして、適切なパラメータをデータセットから自動で求める事を**機械学習**（や学習）と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ではやってみよう。\n",
    "\n",
    "データセットとして、いくつかの単語列を用意する。そして、データセットに出現する全ての単語を対象に、その次に続く単語とその確率を記録する。こうすることでマルコフモデルが完成する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確率は単語の出現頻度から定める。例として以下の3つの単語列をデータセットとして考えてみよう。\n",
    "\n",
    "- 今日 は いい 天気 です 。\n",
    "- 今日 は カレー を 食べ ました 。\n",
    "- 私 は 今日 カレー を 食べ ました 。\n",
    "- 私 は カレー が 好き です 。\n",
    "\n",
    "このデータセットから、例えば、「今日」の次に来る可能性がある単語が「は」と「カレー」であると分かる。出現頻度を見ると、「は」が2回、「カレー」が1回出現しているため、「は」に2/3、「カレー」に1/3の確率を定義できる。このようにして、全ての単語について次に続く単語とその確率を定義する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際にやってみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"今日 は いい 天気 です 。\",\n",
    "    \"今日 は カレー を 食べ ました 。\",\n",
    "    \"私 は 今日 カレー を 食べ ました 。\",\n",
    "    \"私 は カレー が 好き です 。\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'今日': {'は': 0.6666666666666666, 'カレー': 0.3333333333333333},\n",
       " 'は': {'いい': 0.25, 'カレー': 0.5, '今日': 0.25},\n",
       " 'いい': {'天気': 1.0},\n",
       " '天気': {'です': 1.0},\n",
       " 'です': {'。': 1.0},\n",
       " 'カレー': {'を': 0.6666666666666666, 'が': 0.3333333333333333},\n",
       " 'を': {'食べ': 1.0},\n",
       " '食べ': {'ました': 1.0},\n",
       " 'ました': {'。': 1.0},\n",
       " '私': {'は': 1.0},\n",
       " 'が': {'好き': 1.0},\n",
       " '好き': {'です': 1.0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "for sent in data:\n",
    "    sent = sent.split()\n",
    "    for w1, w2 in zip(sent[:-1], sent[1:]):\n",
    "        if w1 not in vocab:\n",
    "            vocab[w1] = {}\n",
    "        if w2 not in vocab[w1]:\n",
    "            vocab[w1][w2] = 0\n",
    "        vocab[w1][w2] += 1\n",
    "\n",
    "for w1 in vocab:\n",
    "    total = sum(vocab[w1].values())\n",
    "    for w2 in vocab[w1]:\n",
    "        vocab[w1][w2] /= total\n",
    "\n",
    "vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、マルコフモデルの学習が完了したことになる。このモデルでいくつか文章を生成してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日 カレー を 食べ ました 。\n",
      "今日 カレー を 食べ ました 。\n",
      "今日 は カレー が 好き です 。\n",
      "今日 は 今日 は いい 天気 です 。\n",
      "今日 カレー を 食べ ました 。\n",
      "私 は いい 天気 です 。\n",
      "私 は 今日 は いい 天気 です 。\n",
      "私 は 今日 は カレー が 好き です 。\n",
      "私 は いい 天気 です 。\n",
      "私 は いい 天気 です 。\n"
     ]
    }
   ],
   "source": [
    "model = MarkovModel(vocab)\n",
    "\n",
    "for _ in range(10):\n",
    "    start_word, = random.choices([\"今日\", \"私\"]) # 最初の単語はランダム\n",
    "    print(generate_sentence(model, start_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ランダムなモデルよりは自然な文章が生成されているのではないだろうか。\n",
    "\n",
    "これで、データセットからマルコフモデルを学習し、文章を生成することが出来た。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### markovify\n",
    "\n",
    "マルコフモデルを用いて文章を生成するためのライブラリ。\n",
    "\n",
    "- [jsvine/markovify: A simple, extensible Markov chain generator.](https://github.com/jsvine/markovify)\n",
    "\n",
    "これを使うと簡単にマルコフモデルを用いた言語モデルを実装できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['今日 は いい 天気 です 。',\n",
       " '今日 は カレー を 食べ ました 。',\n",
       " '私 は 今日 カレー を 食べ ました 。',\n",
       " '私 は カレー が 好き です 。']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data # 再掲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = markovify.Text(data, state_size=1) # 学習"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで学習が完了した。文章を生成してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私 は 今日 は いい 天気 です 。\n",
      "私 は 今日 は カレー が 好き です 。\n",
      "None\n",
      "None\n",
      "私 は 今日 は 今日 カレー を 食べ ました 。\n",
      "私 は 今日 は いい 天気 です 。\n",
      "None\n",
      "今日 は 今日 は 今日 は 今日 カレー を 食べ ました 。\n",
      "今日 は 今日 は カレー が 好き です 。\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    sentence = model.make_sentence()\n",
    "    print(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## より大きなデータセットの活用\n",
    "\n",
    "ここまで、私が用意した3つの短文をデータセットとして言語モデルを作成した。本節ではもう少し大きなデータセットを用いて言語モデルを作成する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wiki40b\n",
    "\n",
    "Wikipediaの記事を集めたデータセット。Tensorflow Datasetsに収録されているものを使用する。\n",
    "\n",
    "[wiki40b  |  TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/wiki40b?hl=en#wiki40bja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 22:22:21.462195: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-09 22:22:21.464491: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-09 22:22:21.470701: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-09 22:22:21.481143: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-09 22:22:21.484081: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-09 22:22:21.492566: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-09 22:22:22.081362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# import os; os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\" # tensorflowのログを非表示\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 22:22:23.679943: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: 2.19 GiB, total: 2.19 GiB) to data/wiki40b/ja/1.3.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/komiya/works/language-models/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Dl Completed...:  10%|█         | 2/20 [00:00<00:05,  3.02 file/s]2024-10-09 22:22:37.420462: E external/local_tsl/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f22d4021a40 (URI: https://storage.googleapis.com/tfds-data/datasets%2Fwiki40b%2Fja%2F1.3.0%2Fwiki40b-train.tfrecord-00014-of-00016) has been stuck at 8534697 of 67108864 bytes for 18446744073709551614 seconds and will be aborted. CURL timing information: lookup time: 0.008875 (No error), connect time: 0.013485 (No error), pre-transfer time: 0.068009 (No error), start-transfer time: 0.444177 (No error)\n",
      "2024-10-09 22:22:37.420518: E external/local_tsl/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f2350021230 (URI: https://storage.googleapis.com/tfds-data/datasets%2Fwiki40b%2Fja%2F1.3.0%2Fwiki40b-train.tfrecord-00002-of-00016) has been stuck at 12057250 of 67108864 bytes for 18446744073709551614 seconds and will be aborted. CURL timing information: lookup time: 0.021915 (No error), connect time: 0.035114 (No error), pre-transfer time: 0.111577 (No error), start-transfer time: 0.504019 (No error)\n",
      "2024-10-09 22:22:37.421375: E external/local_tsl/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f22cc021310 (URI: https://storage.googleapis.com/tfds-data/datasets%2Fwiki40b%2Fja%2F1.3.0%2Fwiki40b-validation.tfrecord-00000-of-00001) has been stuck at 9190051 of 67108864 bytes for 18446744073709551614 seconds and will be aborted. CURL timing information: lookup time: 0.009467 (No error), connect time: 0.016718 (No error), pre-transfer time: 0.076438 (No error), start-transfer time: 0.458002 (No error)\n",
      "2024-10-09 22:22:37.422607: E external/local_tsl/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f2374021230 (URI: https://storage.googleapis.com/tfds-data/datasets%2Fwiki40b%2Fja%2F1.3.0%2Fwiki40b-test.tfrecord-00000-of-00001) has been stuck at 9271978 of 67108864 bytes for 18446744073709551614 seconds and will be aborted. CURL timing information: lookup time: 0.010361 (No error), connect time: 0.0177 (No error), pre-transfer time: 0.124508 (No error), start-transfer time: 0.480851 (No error)\n",
      "2024-10-09 22:22:37.422658: E external/local_tsl/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f22f0021690 (URI: https://storage.googleapis.com/tfds-data/datasets%2Fwiki40b%2Fja%2F1.3.0%2Fwiki40b-train.tfrecord-00010-of-00016) has been stuck at 6650530 of 67108864 bytes for 18446744073709551614 seconds and will be aborted. CURL timing information: lookup time: 0.01368 (No error), connect time: 0.024392 (No error), pre-transfer time: 0.090166 (No error), start-transfer time: 0.424787 (No error)\n",
      "2024-10-09 22:22:37.423014: E external/local_tsl/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f2320021230 (URI: https://storage.googleapis.com/tfds-data/datasets%2Fwiki40b%2Fja%2F1.3.0%2Fwiki40b-train.tfrecord-00006-of-00016) has been stuck at 15383200 of 67108864 bytes for 18446744073709551614 seconds and will be aborted. CURL timing information: lookup time: 0.022303 (No error), connect time: 0.032203 (No error), pre-transfer time: 0.180014 (No error), start-transfer time: 0.525785 (No error)\n",
      "2024-10-09 22:22:37.424875: E external/local_tsl/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f22fc021230 (URI: https://storage.googleapis.com/tfds-data/datasets%2Fwiki40b%2Fja%2F1.3.0%2Fwiki40b-train.tfrecord-00009-of-00016) has been stuck at 7060129 of 67108864 bytes for 18446744073709551614 seconds and will be aborted. CURL timing information: lookup time: 0.018766 (No error), connect time: 0.028306 (No error), pre-transfer time: 0.093821 (No error), start-transfer time: 0.508445 (No error)\n",
      "2024-10-09 22:22:37.425705: E external/local_tsl/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f22d8021230 (URI: https://storage.googleapis.com/tfds-data/datasets%2Fwiki40b%2Fja%2F1.3.0%2Fwiki40b-train.tfrecord-00012-of-00016) has been stuck at 10336937 of 67108864 bytes for 18446744073709551614 seconds and will be aborted. CURL timing information: lookup time: 0.009422 (No error), connect time: 0.016579 (No error), pre-transfer time: 0.076988 (No error), start-transfer time: 0.401267 (No error)\n",
      "2024-10-09 22:22:37.427430: E external/local_tsl/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f2308021230 (URI: https://storage.googleapis.com/tfds-data/datasets%2Fwiki40b%2Fja%2F1.3.0%2Fwiki40b-train.tfrecord-00008-of-00016) has been stuck at 6453922 of 67108864 bytes for 18446744073709551614 seconds and will be aborted. CURL timing information: lookup time: 0.014691 (No error), connect time: 0.024173 (No error), pre-transfer time: 0.090101 (No error), start-transfer time: 0.444036 (No error)\n",
      "2024-10-09 22:22:37.427849: E external/local_tsl/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f2344021230 (URI: https://storage.googleapis.com/tfds-data/datasets%2Fwiki40b%2Fja%2F1.3.0%2Fwiki40b-train.tfrecord-00003-of-00016) has been stuck at 20249257 of 67108864 bytes for 18446744073709551614 seconds and will be aborted. CURL timing information: lookup time: 0.022223 (No error), connect time: 0.035486 (No error), pre-transfer time: 0.112583 (No error), start-transfer time: 0.541891 (No error)\n",
      "2024-10-09 22:22:37.431433: E external/local_tsl/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f23140216d0 (URI: https://storage.googleapis.com/tfds-data/datasets%2Fwiki40b%2Fja%2F1.3.0%2Fwiki40b-train.tfrecord-00007-of-00016) has been stuck at 6306466 of 67108864 bytes for 18446744073709551614 seconds and will be aborted. CURL timing information: lookup time: 0.085446 (No error), connect time: 0.096914 (No error), pre-transfer time: 0.161375 (No error), start-transfer time: 0.59594 (No error)\n",
      "2024-10-09 22:22:37.442779: E external/local_tsl/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f22c8021230 (URI: https://storage.googleapis.com/tfds-data/datasets%2Fwiki40b%2Fja%2F1.3.0%2Fwiki40b-train.tfrecord-00015-of-00016) has been stuck at 6421160 of 67108864 bytes for 18446744073709551614 seconds and will be aborted. CURL timing information: lookup time: 0.008366 (No error), connect time: 0.013649 (No error), pre-transfer time: 0.067721 (No error), start-transfer time: 0.494485 (No error)\n",
      "2024-10-09 22:22:37.443178: E external/local_tsl/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f22d0021230 (URI: https://storage.googleapis.com/tfds-data/datasets%2Fwiki40b%2Fja%2F1.3.0%2Fwiki40b-train.tfrecord-00013-of-00016) has been stuck at 8075937 of 67108864 bytes for 18446744073709551614 seconds and will be aborted. CURL timing information: lookup time: 0.009313 (No error), connect time: 0.015923 (No error), pre-transfer time: 0.078407 (No error), start-transfer time: 0.535968 (No error)\n",
      "2024-10-09 22:22:37.447331: E external/local_tsl/tsl/platform/cloud/curl_http_request.cc:610] The transmission  of request 0x7f22e4021230 (URI: https://storage.googleapis.com/tfds-data/datasets%2Fwiki40b%2Fja%2F1.3.0%2Fwiki40b-train.tfrecord-00011-of-00016) has been stuck at 5454498 of 67108864 bytes for 18446744073709551614 seconds and will be aborted. CURL timing information: lookup time: 0.018765 (No error), connect time: 0.027655 (No error), pre-transfer time: 0.092178 (No error), start-transfer time: 0.520453 (No error)\n"
     ]
    }
   ],
   "source": [
    "ds = tfds.load(\"wiki40b/ja\", split=\"test\", data_dir=\"data\")\n",
    "ds = list(ds.as_numpy_iterator())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "辞書のリストが得られる。  \n",
    "`ds: List[Dict[str, bytes]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': b'\\n_START_ARTICLE_\\n\\xe3\\x83\\x93\\xe3\\x83\\xbc\\xe3\\x83\\x88\\xe3\\x81\\x9f\\xe3\\x81\\x91\\xe3\\x81\\x97\\xe3\\x81\\xae\\xe6\\x95\\x99\\xe7\\xa7\\x91\\xe6\\x9b\\xb8\\xe3\\x81\\xab\\xe8\\xbc\\x89\\xe3\\x82\\x89\\xe3\\x81\\xaa\\xe3\\x81\\x84\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe4\\xba\\xba\\xe3\\x81\\xae\\xe8\\xac\\x8e\\n_START_SECTION_\\n\\xe6\\xa6\\x82\\xe8\\xa6\\x81\\n_START_PARAGRAPH_\\n\\xe3\\x80\\x8c\\xe6\\x95\\x99\\xe7\\xa7\\x91\\xe6\\x9b\\xb8\\xe3\\x81\\xab\\xe3\\x81\\xaf\\xe6\\xb1\\xba\\xe3\\x81\\x97\\xe3\\x81\\xa6\\xe8\\xbc\\x89\\xe3\\x82\\x89\\xe3\\x81\\xaa\\xe3\\x81\\x84\\xe3\\x80\\x8d\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe4\\xba\\xba\\xe3\\x81\\xae\\xe8\\xac\\x8e\\xe3\\x82\\x84\\xe3\\x81\\x97\\xe3\\x81\\x8d\\xe3\\x81\\x9f\\xe3\\x82\\x8a\\xe3\\x82\\x92\\xe5\\xa4\\x9a\\xe8\\xa7\\x92\\xe7\\x9a\\x84\\xe3\\x81\\xab\\xe6\\xa4\\x9c\\xe8\\xa8\\xbc\\xe3\\x81\\x97\\xe3\\x80\\x81\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe4\\xba\\xba\\xe3\\x81\\xaeDNA\\xe3\\x82\\x92\\xe8\\xa7\\xa3\\xe6\\x98\\x8e\\xe3\\x81\\x99\\xe3\\x82\\x8b\\xe3\\x80\\x82_NEWLINE_\\xe6\\x96\\xb0\\xe6\\x98\\xa5\\xe7\\x95\\xaa\\xe7\\xb5\\x84\\xe3\\x81\\xa8\\xe3\\x81\\x97\\xe3\\x81\\xa6\\xe5\\xae\\x9a\\xe6\\x9c\\x9f\\xe7\\x9a\\x84\\xe3\\x81\\xab\\xe6\\x94\\xbe\\xe9\\x80\\x81\\xe3\\x81\\x95\\xe3\\x82\\x8c\\xe3\\x81\\xa6\\xe3\\x81\\x8a\\xe3\\x82\\x8a\\xe3\\x80\\x81\\xe5\\xb9\\xb4\\xe6\\x9c\\xab\\xe3\\x81\\xae\\xe5\\x8d\\x88\\xe5\\x89\\x8d\\xe4\\xb8\\xad\\xe3\\x81\\xab\\xe5\\x86\\x8d\\xe6\\x94\\xbe\\xe9\\x80\\x81\\xe3\\x81\\x95\\xe3\\x82\\x8c\\xe3\\x82\\x8b\\xe3\\x81\\xae\\xe3\\x81\\x8c\\xe6\\x81\\x92\\xe4\\xbe\\x8b\\xe3\\x81\\xa8\\xe3\\x81\\xaa\\xe3\\x81\\xa3\\xe3\\x81\\xa6\\xe3\\x81\\x84\\xe3\\x82\\x8b\\xe3\\x80\\x82',\n",
       " 'version_id': b'1848243370795951995',\n",
       " 'wikidata_id': b'Q11331136'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = ds[0]\n",
    "ex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストの中身はこんな感じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_START_ARTICLE_\n",
      "ビートたけしの教科書に載らない日本人の謎\n",
      "_START_SECTION_\n",
      "概要\n",
      "_START_PARAGRAPH_\n",
      "「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。_NEWLINE_新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。\n"
     ]
    }
   ],
   "source": [
    "print(ex[\"text\"].decode())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "セクションごとにまとめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 89698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['「教科書には決して載らない」日本人の謎やしきたりを多角的に検証し、日本人のDNAを解明する。新春番組として定期的に放送されており、年末の午前中に再放送されるのが恒例となっている。',\n",
       " 'ライブドア社員であった初代代表取締役社長の山名真由によって企業内起業の形で創業。2005年に株式会社ライブドアから分割されて設立。かつてはライブドアホールディングス（現・LDH）の子会社であったが、ノンコア事業の整理にともない、株式会社ゲオ（現：株式会社ゲオホールディングス）に所有する全株式を譲渡し、同社の完全子会社となった。「ぽすれん」「ゲオ宅配レンタル」のオンラインDVD・CD・コミックレンタルサービス及び「GEO Online」と「ゲオアプリ」のアプリ・ウェブサイト運営の大きく分けて2事業を展開している。以前はDVD販売等のEコマースサービス「ぽすれんストア」、動画配信コンテンツ「ぽすれんBB」や電子書籍配信サービスの「GEO☆Books」事業も行っていた。オンラインDVDレンタル事業では会員数は10万人（2005年9月時点）。2006年5月よりCDレンタルを開始。同業他社には、カルチュア・コンビニエンス・クラブが運営する『TSUTAYA DISCAS』のほか、DMM.comが運営する『DMM.com オンラインDVDレンタル』がある。過去には「Yahoo!レンタルDVD」と「楽天レンタル」の運営を受託していた。',\n",
       " '2005年の一時期、東京のラジオ局、InterFMで、「堀江社長も使っているライブドアのぽすれん」というキャッチコピーでラジオCMを頻繁に行っていたことがあった。']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for sample in ds:\n",
    "    text = sample[\"text\"].decode()\n",
    "    sections = text.split(\"_START_SECTION_\")\n",
    "    for section in sections[1:]:\n",
    "        sentence = section.split(\"_START_PARAGRAPH_\")[1]\n",
    "        sentence = sentence.replace(\"_NEWLINE_\", \"\")\n",
    "        sentence = sentence.replace(\"\\n\", \"\")\n",
    "        data.append(sentence)\n",
    "\n",
    "print(\"num of data:\", len(data))\n",
    "data[:3] # examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後の章でも使うので書き出しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = \"data/jawiki.txt\"\n",
    "with open(text_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークン化\n",
    "\n",
    "テキストをトークンごとに分割する。トークンとはモデルが扱える最小単位のことで、例えば単語が該当する。本節でも単語を最小単位=トークンとして、文章を単語列に変換する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "英語の文章は単語間にスペースが入っているので、それで終わりである。特にこちらで行う処理はない。一方で日本語の文章はそうではないので、こちらで分割する必要がある。\n",
    "\n",
    "日本語トークン化には形態素解析器を使用する。これは自然言語処理で形態素解析（品詞の分析）に使用するツール。色々な種類があるが、ここではMeCabを使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'私 は 猫 が 好き です 。 \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = MeCab.Tagger(\"-Owakati\") # 出力形式を分かち書きに指定\n",
    "result = tagger.parse(\"私は猫が好きです。\")\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを使って、学習データの全てを単語ごとに分割する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['「 教科 書 に は 決して 載ら ない 」 日本 人 の 謎 や しきたり を 多角 的 に 検証 し 、 日本 人 の DNA を 解明 する 。 新春 番組 と し て 定期 的 に 放送 さ れ て おり 、 年末 の 午前 中 に 再 放送 さ れる の が 恒例 と なっ て いる 。',\n",
       " 'ライブ ドア 社員 で あっ た 初代 代表 取締 役 社長 の 山名 真由 に よっ て 企業 内 起業 の 形 で 創業 。 2005 年 に 株式 会社 ライブ ドア から 分割 さ れ て 設立 。 かつて は ライブ ドア ホールディングス （ 現 ・ LDH ） の 子 会社 で あっ た が 、 ノン コア 事業 の 整理 に ともない 、 株式 会社 ゲオ （ 現 ： 株式 会社 ゲオ ホールディングス ） に 所有 する 全 株式 を 譲渡 し 、 同社 の 完全 子 会社 と なっ た 。 「 ぽす れん 」 「 ゲオ 宅配 レンタル 」 の オン ライン DVD ・ CD ・ コミック レンタル サービス 及び 「 GEO Online 」 と 「 ゲオ アプリ 」 の アプリ ・ ウェブサイト 運営 の 大きく 分け て 2 事業 を 展開 し て いる 。 以前 は DVD 販売 等 の E コマース サービス 「 ぽす れん ストア 」 、 動画 配信 コンテンツ 「 ぽす れん BB 」 や 電子 書籍 配信 サービス の 「 GEO ☆ Books 」 事業 も 行っ て い た 。 オン ライン DVD レンタル 事業 で は 会員 数 は 10 万 人 （ 2005 年 9 月 時点 ） 。 2006 年 5 月 より CD レンタル を 開始 。 同業 他社 に は 、 カルチュア ・ コンビニエンス ・ クラブ が 運営 する 『 TSUTAYA DISCAS 』 の ほか 、 DMM . com が 運営 する 『 DMM . com オン ライン DVD レンタル 』 が ある 。 過去 に は 「 Yahoo ! レンタル DVD 」 と 「 楽天 レンタル 」 の 運営 を 受託 し て い た 。',\n",
       " '2005 年 の 一 時期 、 東京 の ラジオ 局 、 InterFM で 、 「 堀江 社長 も 使っ て いる ライブ ドア の ぽす れん 」 と いう キャッチ コピー で ラジオ CM を 頻繁 に 行っ て い た こと が あっ た 。']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wakati = []\n",
    "for sentence in data:\n",
    "    data_wakati.append(tagger.parse(sentence).strip())\n",
    "\n",
    "data_wakati[:3] # examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実践\n",
    "\n",
    "markovifyを使ってマルコフモデルを学習させ、文章を生成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = markovify.Text(data_wakati, state_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "郡西部にまで同一呼称方法で4進出に斜上、賀蘭部制度にも含めて木製ニシュトレイスランが実現して使われることに用いられる。北にできる。表題曲とした。通行量以上あった形態変化したためであった。\n",
      "MCのポジションで割っては第48チームとし、実際による卵塊を受け、最新のノウハウを習得して大会を温存して色の築造当初は、そこである。容器に昇格とミュラ住民の掃討した。しかしブレスト間であった。特に2000年5試合に送電線が交わされたシーズン、なぜその日には肯定された以外ではテヘラン会談の活躍も可能に存在しているものの勢力がずきずきし、2013年8月ｰ10月18日の防塁に自由都市で展開させる。安井英雄伝説になった。2013年に引っ越している。\n",
      "アドベンチャーの秀句やイヴ・オタヴィオ・ヒベイロも運搬するなどに自動化係数γ-18世紀に選ばれて1990年、フィルングルント東端・ギャップを得た。\n",
      "ジェダイの評価を保ち、一方、メキシコへの悪性腫瘍の時期に高丸からなる。しかしながら身障者も大型種牡馬としたアクバルの対象地域で膝のスターターとしフレッシュオールアメリカンに囲まれ、UFCからかえった。配給をハザマには、リロングウェがその後、主力戦車駆逐艦は近江彦根東は可能としており、議会はラインの小説で御家にはロシア移民が低迷したものでは自身が使用時に、これは少ない。\n",
      "赤穂国際A得点5m以上の罪との全空気弁花LaLaonlineにて全国委員会を有する地下茎を軍規違反の家はいた。7月下旬には任意に存在している。\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    sentence = model.make_sentence().replace(\" \", \"\")\n",
    "    print(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "語彙が増えたことで多様な文章が生成されるようになった。\n",
    "\n",
    "ただ、文脈の最後の単語しか考慮できないので、文章全体での自然さを作り出すことは難しい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## N-gram\n",
    "\n",
    "単語の分割方法。N個の単語を1つの単位として文章を分割する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1単語ごとに分割するやり方は1-gramと呼べる。1-gramだと直前の1単語しか考慮できないので自然な文章を生成するのが難しい。N-gramではそれがN個に拡張されるので、N-gramを採用することでより広い文脈を考慮できるようになる。\n",
    "\n",
    "- 1-gram: 今日 / は / いい / 天気 / です / 。\n",
    "- 2-gram: 今日 は / は いい / いい 天気 / 天気 です / です 。\n",
    "- 3-gram: 今日 は いい / は いい 天気 / いい 天気 です / 天気 です 。\n",
    "\n",
    "ちなみに、1-gramは*uni-gram*、2-gramは*bi-gram*、3-gramは*tri-gram*と呼ばれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gramにおけるマルコフモデルの学習はこれまでと同様に行うことが出来る。N個の単語を1つの単位として次に続く単語の確率分布を求める。以下のデータセット:\n",
    "\n",
    "- 今日 は いい 天気 です 。\n",
    "- 今日 は カレー を 食べ ました 。\n",
    "- 私 は 今日 カレー を 食べ ました 。\n",
    "- 私 は カレー が 好き です 。\n",
    "\n",
    "から、2-gramの確率分布を求めると:\n",
    "\n",
    "- 「今日 は」の次は「いい」が0.5、「カレー」が0.5\n",
    "- 「は いい」の次は「天気」が1\n",
    "- 「いい 天気」の次は「です」が1\n",
    "- ...\n",
    "\n",
    "て感じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
