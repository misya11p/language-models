{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "\n",
    "*Sequence to Sequence*.  \n",
    "*Encoder-Decoder Model*とも。\n",
    "\n",
    "2つのモデルを用いて、入力されたシーケンスに基づいた別のシーケンスを出力するモデル。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "from glob import glob\n",
    "import os\n",
    "from typing import List\n",
    "import random\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress(with_test=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 発想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで、RNNを用いて、入力した単語列に続く単語を予測するモデルを作成し、単語の予測を繰り返すことで文章を生成した。\n",
    "\n",
    "RNNは時系列の情報を保持するために隠れ状態$h_t$を用いる。隠れ状態の初期値$h_0$は0ベクトルとしているが、ここで、何らかの入力データから生成したベクトルを$h_0$として用いることを考える。このとき、上手く学習させれば、その入力に基づいた文章を生成できそう。\n",
    "\n",
    "例えば、入力を画像をとし、CNNを用いて抽出した特徴量を$h_0$として用いるようにすれば、入力画像に基づいた文章が生成できる。画像のキャプションなどが例に挙げられる。  \n",
    "学習方法は簡単で、入力画像に対して適切な文章が出力されるように学習させるだけ。隠れ状態を通じてRNNからCNNまで逆伝播を繋げる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、入力に文章を用いることはできないだろうか。RNNに文章を入力し、最後に出力された隠れ状態を文章ベクトルとする。これを別のRNNへの入力$h_0$とすれば、入力文に基づいた文章生成が可能になる。  \n",
    "\n",
    "この発想は翻訳タスクに大きく役立つ。入力と出力に同じ意味を持った異なる言語の文章を設定すれば、入力文と同じ意味を持った文章生成が可能になる。  \n",
    "こういった、シーケンスをシーケンスに変換するモデルを *seq2seq (Sequence to Sequence)* と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章では、入力に日本語文、出力に入力と同じ意味を持つ英語文を設定し、日本語→英語の翻訳を行うモデルを作成する。\n",
    "\n",
    "このモデルは以下の二つのRNNから構成される。\n",
    "- ***Encoder*** : 文章ベクトルを生成するRNN\n",
    "- ***Decoder*** : 文章ベクトルを受け取って出力文を生成するRNN\n",
    "\n",
    "このことから***Encoder-Decoderモデル***とも呼ばれる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 対訳コーパス\n",
    "\n",
    "翻訳モデルを作るには、同じ意味を持つ文章が複数の言語でまとまっているデータが必要。このようなデータは対訳コーパスと呼んだりする。\n",
    "\n",
    "本章では以下のデータセットを使用する。\n",
    "- [Wikipedia日英京都関連文書対訳コーパス](https://alaginrc.nict.go.jp/WikiCorpus/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en: Therefore, it is considered that the 'being that caused bizarre things' came to be called yokai.\n",
      "ja: そのため「怪異を起こす存在」を妖怪と呼ぶようになったと考えられる。\n",
      "\n",
      "en: Yoshitsune Senbonzakura' (Yoshitsune and One Thousand Cherry Trees), another title of a Kabuki play that has Yoshitsune in the leading role, also has Benkei as one of the characters.\n",
      "ja: また、義経を主人公とした「義経千本桜」などの歌舞伎にも、主要人物の一人として登場する。\n",
      "\n",
      "en: Misao YASHIRO (the founder of Meiji Horitsu Gakko)\n",
      "ja: 矢代操（明治法律学校の創立者）\n",
      "\n",
      "en: In 785, the involvement of his father, Tsuguhito, in the assassination of FUJIWARA no Tanetsugu also implicated his involvement, and he was also deported to Sado Province.\n",
      "ja: 785年、父継人が藤原種継暗殺事件に関与したため、連座して佐渡国に流される。\n",
      "\n",
      "en: Festival in honor of Ebisu (October 19 and 20):\n",
      "ja: 恵比寿講（10月19日・20日）\n",
      "\n",
      "num of data: 443596\n"
     ]
    }
   ],
   "source": [
    "data_ja, data_en = [], []\n",
    "root_dir = 'data/jaen-kyoto/'\n",
    "for xml_file in glob(os.path.join(root_dir, '*/*.xml')):\n",
    "    try:\n",
    "        tree = ET.parse(xml_file)\n",
    "    except ET.ParseError:\n",
    "        continue\n",
    "    root = tree.getroot()\n",
    "    for sentence in root.iter('sen'):\n",
    "        ja = sentence.find('j').text\n",
    "        en = sentence.findall('e')[-1].text\n",
    "        if ja and en:\n",
    "            data_ja.append(ja)\n",
    "            data_en.append(en)\n",
    "\n",
    "# examples\n",
    "for _ in range(5):\n",
    "    i = random.randint(0, len(data_en))\n",
    "    print('en:', data_en[i])\n",
    "    print('ja:', data_ja[i])\n",
    "    print()\n",
    "\n",
    "n_data = len(data_en)\n",
    "print('num of data:', n_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "書き出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile_ja = 'data/kyoto_ja.txt'\n",
    "with open(textfile_ja, 'w') as f:\n",
    "    f.write('\\n'.join(data_ja))\n",
    "\n",
    "textfile_en = 'data/kyoto_en.txt'\n",
    "with open(textfile_en, 'w') as f:\n",
    "    f.write('\\n'.join(data_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークナイザの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/kyoto_ja.txt\n",
      "  input_format: \n",
      "  model_prefix: models/tokenizer_kyoto_ja\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 3\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: data/kyoto_ja.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 443596 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=17422239\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=3970\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 443596 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=5923332\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 1003970 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 443596\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 479757\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 479757 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=578562 obj=126.485 num_tokens=6361402 num_tokens/piece=10.9952\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=497066 obj=114.509 num_tokens=6411495 num_tokens/piece=12.8987\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=371925 obj=114.719 num_tokens=6573103 num_tokens/piece=17.6732\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=370110 obj=114.26 num_tokens=6582651 num_tokens/piece=17.7857\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=277426 obj=115.438 num_tokens=6767112 num_tokens/piece=24.3925\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=277263 obj=114.973 num_tokens=6770583 num_tokens/piece=24.4194\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=207932 obj=116.775 num_tokens=7002582 num_tokens/piece=33.6773\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=207916 obj=116.191 num_tokens=7006929 num_tokens/piece=33.7008\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=155936 obj=118.438 num_tokens=7261506 num_tokens/piece=46.5672\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=155935 obj=117.822 num_tokens=7265829 num_tokens/piece=46.5952\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=116951 obj=120.469 num_tokens=7533992 num_tokens/piece=64.4201\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=116950 obj=119.854 num_tokens=7535369 num_tokens/piece=64.4324\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=87712 obj=122.769 num_tokens=7813425 num_tokens/piece=89.0805\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=87712 obj=122.17 num_tokens=7815767 num_tokens/piece=89.1072\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=65784 obj=125.365 num_tokens=8111688 num_tokens/piece=123.308\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=65784 obj=124.751 num_tokens=8111762 num_tokens/piece=123.309\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49338 obj=128.143 num_tokens=8426558 num_tokens/piece=170.792\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49338 obj=127.503 num_tokens=8428527 num_tokens/piece=170.832\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37003 obj=131.118 num_tokens=8768474 num_tokens/piece=236.967\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37003 obj=130.417 num_tokens=8768939 num_tokens/piece=236.979\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27752 obj=134.292 num_tokens=9144269 num_tokens/piece=329.499\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27752 obj=133.513 num_tokens=9144365 num_tokens/piece=329.503\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20814 obj=137.694 num_tokens=9557483 num_tokens/piece=459.185\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20814 obj=136.847 num_tokens=9557577 num_tokens/piece=459.19\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15610 obj=141.502 num_tokens=10021205 num_tokens/piece=641.973\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15610 obj=140.552 num_tokens=10021338 num_tokens/piece=641.982\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11707 obj=145.821 num_tokens=10544268 num_tokens/piece=900.681\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11707 obj=144.746 num_tokens=10544224 num_tokens/piece=900.677\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=150.834 num_tokens=11158160 num_tokens/piece=1267.97\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=149.536 num_tokens=11158159 num_tokens/piece=1267.97\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: models/tokenizer_kyoto_ja.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: models/tokenizer_kyoto_ja.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/kyoto_en.txt\n",
      "  input_format: \n",
      "  model_prefix: models/tokenizer_kyoto_en\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 3\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: data/kyoto_en.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 443596 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=60215996\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=263\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 443596 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=31192236\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 407360 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 443596\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 456235\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 456235 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=171104 obj=11.7047 num_tokens=1095748 num_tokens/piece=6.40399\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=146850 obj=9.21024 num_tokens=1103705 num_tokens/piece=7.51587\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=110101 obj=9.17929 num_tokens=1141041 num_tokens/piece=10.3636\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=109980 obj=9.16834 num_tokens=1147254 num_tokens/piece=10.4315\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=82483 obj=9.21401 num_tokens=1202245 num_tokens/piece=14.5757\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=82481 obj=9.20459 num_tokens=1202228 num_tokens/piece=14.5758\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=61859 obj=9.27158 num_tokens=1265251 num_tokens/piece=20.4538\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=61859 obj=9.25807 num_tokens=1265208 num_tokens/piece=20.4531\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=46394 obj=9.35215 num_tokens=1332728 num_tokens/piece=28.7263\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=46394 obj=9.33473 num_tokens=1332717 num_tokens/piece=28.7261\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=34795 obj=9.45382 num_tokens=1401459 num_tokens/piece=40.2776\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=34795 obj=9.43153 num_tokens=1401477 num_tokens/piece=40.2781\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=26096 obj=9.58136 num_tokens=1471028 num_tokens/piece=56.3699\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=26096 obj=9.55347 num_tokens=1471032 num_tokens/piece=56.37\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=19572 obj=9.73407 num_tokens=1539931 num_tokens/piece=78.6803\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=19572 obj=9.70063 num_tokens=1539990 num_tokens/piece=78.6833\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=14679 obj=9.92108 num_tokens=1612027 num_tokens/piece=109.819\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=14679 obj=9.87847 num_tokens=1612059 num_tokens/piece=109.821\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11009 obj=10.1463 num_tokens=1686360 num_tokens/piece=153.18\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11009 obj=10.0964 num_tokens=1686340 num_tokens/piece=153.178\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=10.3284 num_tokens=1745352 num_tokens/piece=198.335\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=10.2867 num_tokens=1745408 num_tokens/piece=198.342\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: models/tokenizer_kyoto_en.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: models/tokenizer_kyoto_en.vocab\n"
     ]
    }
   ],
   "source": [
    "pad_id = 3\n",
    "\n",
    "vocab_size_ja = 8000\n",
    "tokenizer_prefix_ja = f'models/tokenizer_kyoto_ja'\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_ja, # データセット\n",
    "    model_prefix=tokenizer_prefix_ja,\n",
    "    vocab_size=vocab_size_ja,\n",
    "    pad_id=pad_id\n",
    ")\n",
    "\n",
    "vocab_size_en = 8000\n",
    "tokenizer_prefix_en = f'models/tokenizer_kyoto_en'\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_en, # データセット\n",
    "    model_prefix=tokenizer_prefix_en,\n",
    "    vocab_size=vocab_size_en,\n",
    "    pad_id=pad_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ数を減らしたものも作っておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/kyoto_ja_10000.txt\n",
      "  input_format: \n",
      "  model_prefix: models/tokenizer_kyoto_ja_10000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 3\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: data/kyoto_ja_10000.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 10000 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=373275\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=1917\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 10000 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=148581\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 62847 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 10000\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 11487\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 11487 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=28002 obj=79.0223 num_tokens=131320 num_tokens/piece=4.68967\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=25684 obj=70.5616 num_tokens=132234 num_tokens/piece=5.1485\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=19217 obj=72.4325 num_tokens=138894 num_tokens/piece=7.22766\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=19169 obj=71.7497 num_tokens=139061 num_tokens/piece=7.25447\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=14365 obj=75.1775 num_tokens=147992 num_tokens/piece=10.3023\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=14356 obj=74.4358 num_tokens=148065 num_tokens/piece=10.3138\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=10766 obj=78.2094 num_tokens=157695 num_tokens/piece=14.6475\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=10765 obj=77.5141 num_tokens=157698 num_tokens/piece=14.6491\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8799 obj=80.4521 num_tokens=165061 num_tokens/piece=18.7591\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8799 obj=79.9675 num_tokens=165156 num_tokens/piece=18.7699\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: models/tokenizer_kyoto_ja_10000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: models/tokenizer_kyoto_ja_10000.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/kyoto_en_10000.txt\n",
      "  input_format: \n",
      "  model_prefix: models/tokenizer_kyoto_en_10000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 3\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: data/kyoto_en_10000.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 10000 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=1243892\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.955% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=72\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.99955\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 10000 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=683744\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 27113 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 10000\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 18320\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 18320 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11265 obj=11.1276 num_tokens=41257 num_tokens/piece=3.66241\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=9729 obj=8.87514 num_tokens=41482 num_tokens/piece=4.26375\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8798 obj=8.82419 num_tokens=41941 num_tokens/piece=4.76711\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8756 obj=8.80599 num_tokens=41950 num_tokens/piece=4.791\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: models/tokenizer_kyoto_en_10000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: models/tokenizer_kyoto_en_10000.vocab\n"
     ]
    }
   ],
   "source": [
    "n_data = 10000\n",
    "data_ja = data_ja[:n_data]\n",
    "data_en = data_en[:n_data]\n",
    "\n",
    "textfile_ja = f'data/kyoto_ja_{n_data}.txt'\n",
    "with open(textfile_ja, 'w') as f:\n",
    "    f.write('\\n'.join(data_ja))\n",
    "\n",
    "textfile_en = f'data/kyoto_en_{n_data}.txt'\n",
    "with open(textfile_en, 'w') as f:\n",
    "    f.write('\\n'.join(data_en))\n",
    "\n",
    "vocab_size_ja = 8000\n",
    "tokenizer_prefix_ja = f'models/tokenizer_kyoto_ja_{n_data}'\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_ja, # データセット\n",
    "    model_prefix=tokenizer_prefix_ja,\n",
    "    vocab_size=vocab_size_ja,\n",
    "    pad_id=pad_id\n",
    ")\n",
    "\n",
    "vocab_size_en = 8000\n",
    "tokenizer_prefix_en = f'models/tokenizer_kyoto_en_{n_data}'\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_en, # データセット\n",
    "    model_prefix=tokenizer_prefix_en,\n",
    "    vocab_size=vocab_size_en,\n",
    "    pad_id=pad_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理\n",
    "\n",
    "データ数を減らした方を使う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークナイザ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary (ja): 8000\n",
      "num of vocabrary (en): 8000\n"
     ]
    }
   ],
   "source": [
    "sp_ja = spm.SentencePieceProcessor(f'{tokenizer_prefix_ja}.model')\n",
    "sp_en = spm.SentencePieceProcessor(f'{tokenizer_prefix_en}.model')\n",
    "\n",
    "unk_id = sp_ja.unk_id()\n",
    "bos_id = sp_ja.bos_id()\n",
    "eos_id = sp_ja.eos_id()\n",
    "pad_id = sp_ja.pad_id()\n",
    "n_vocab_ja = len(sp_ja)\n",
    "n_vocab_en = len(sp_en)\n",
    "print('num of vocabrary (ja):', n_vocab_ja)\n",
    "print('num of vocabrary (en):', n_vocab_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids_ja = sp_ja.encode(data_ja)\n",
    "data_ids_en = sp_en.encode(data_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOS, EOSの追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_id = sp_ja.bos_id()\n",
    "eos_id = sp_ja.eos_id()\n",
    "for ids_ja, ids_en in zip(data_ids_ja, data_ids_en):\n",
    "    ids_en.insert(0, bos_id)\n",
    "    ids_ja.append(eos_id)\n",
    "    ids_en.append(eos_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データの作成\n",
    "\n",
    "入力文と正解のペアを作成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoderへの入力（入力文）とDecoderの出力（正解）のペアを作成する。  \n",
    "また、Decoderへの入力を考える必要がある。今回は教師強制を採用し、出力文の頭に\\<BOS>を付与したものをDecoderへの入力とする。\n",
    "\n",
    "例）\n",
    "Encoderへの入力（入力文） | Decoderへの入力 | Eecoderの出力（出力文）\n",
    "--- | --- | ---\n",
    "夏 休み が 終わり ました 。 \\<EOS> | \\<BOS> Summer vacation is over . | Summer vacation is over . \\<EOS>\n",
    "ツイッター は 亡くなり ました 。 \\<EOS> | \\<BOS> Twitter is dead . | Twitter is dead . \\<EOS>\n",
    "今日 から X で 暮らし ましょう 。 \\<EOS> | \\<BOS> Let 's live in X from today . | Let 's live in X from today . \\<EOS>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaderの作成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 84]), torch.Size([32, 124]), torch.Size([32, 124]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids_ja, data_ids_en):\n",
    "        self.data_ja = [torch.tensor(ids) for ids in data_ids_ja]\n",
    "        self.data_en = [torch.tensor(ids) for ids in data_ids_en]\n",
    "        self.n_data = len(self.data_ja)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ja = self.data_ja[idx]\n",
    "        en = self.data_en[idx]\n",
    "        x_enc = ja # encoderへの入力\n",
    "        x_dec = en[:-1] # decoderへの入力\n",
    "        y_dec = en[1:] # decoderの出力\n",
    "        return x_enc, x_dec, y_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "def collate_fn(batch): # padding\n",
    "    x_enc, x_dec, y_dec= zip(*batch)\n",
    "    x_enc = pad_sequence(x_enc, batch_first=True, padding_value=pad_id)\n",
    "    x_dec = pad_sequence(x_dec, batch_first=True, padding_value=pad_id)\n",
    "    y_dec = pad_sequence(y_dec, batch_first=True, padding_value=pad_id)\n",
    "    return x_enc, x_dec, y_dec\n",
    "\n",
    "dataset = TextDataset(data_ids_ja, data_ids_en)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "x_enc, x_dec, y_dec = next(iter(train_loader))\n",
    "x_enc.shape, x_dec.shape, y_dec.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築\n",
    "\n",
    "EncoderとDecoderを作る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "入力文を入れて隠れ状態を出力するだけのRNN(LSTM)。LSTMと線形層で作る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで1つ工夫を加える。普通に作っても動きはするが、学習が上手くいかない可能性がある。\n",
    "\n",
    "encoderへの入力はpaddingされたデータである。ここで、paddingされた範囲が多い=padトークンが多く含まれているデータは、padの数が多くなるにつれて、隠れ状態がある一定の値に収束してしまう。RNNに同じトークンを何度も入力することで隠れ状態が収束していってしまうのだ。  \n",
    "そうなってしまうと、入力文に依る隠れ状態の違いが少なくなり、翻訳を学習できない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そこで、encoderが出力する隠れ状態として、padトークンを除いた最後のトークン=EOSを入力した時点のものを使用する。  \n",
    "\n",
    "以上を踏まえ、encoderを以下のように実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        # self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        eos_positions = x == eos_id\n",
    "            # eosに対応する位置のみがTrueとなったTensor: (batch_size, seq_len)\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        # hs, _ = self.lstm(x) # (batch_size, seq_len, hidden_size)\n",
    "        hs, _ = self.rnn(x) # (batch_size, seq_len, hidden_size)\n",
    "        h = hs[eos_positions] # (batch_size, hidden_size)\n",
    "        h = self.fc(h).unsqueeze(0) # (1, batch_size, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Encoderから出力された隠れ状態を受け取り、出力文を生成するRNN。Encoder同様、LSTMと線形層で作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        # self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        # hs, (h, c) = self.lstm(x, hc)\n",
    "        hs, h = self.rnn(x, hc)\n",
    "            # hs: (batch_size, seq_len, hidden_size)\n",
    "            # h: (1, batch_size, hidden_size)\n",
    "            # c: (1, batch_size, hidden_size)\n",
    "        y = self.fc(hs) # (batch_size, seq_len, n_vocab)\n",
    "        # return y, (h, c)\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "EncoderとDecoderを合わせて、入力から出力までの一連の処理を行うモデルを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        h = self.encoder(x_enc)\n",
    "        # hc = (h, torch.zeros_like(h))\n",
    "        # y, _ = self.decoder(x_dec, hc)\n",
    "        y, _ = self.decoder(x_dec, h)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "def loss_fn(y, t):\n",
    "    loss = cross_entropy(y.reshape(-1, n_vocab_ja), t.ravel())\n",
    "    return loss\n",
    "\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_enc, x_dec, y_dec in test_loader:\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            total_loss += loss.item()\n",
    "    loss = total_loss / len(test_loader)\n",
    "    return loss\n",
    "\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    prog.start(n_iter=len(train_loader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for x_enc, x_dec, y_dec in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prog.update(loss.item())\n",
    "\n",
    "        if prog.now_epoch % prog_unit == 0:\n",
    "            test_loss = eval_model(model)\n",
    "            prog.memo(f'test: {test_loss:.5f}', no_step=True)\n",
    "        prog.memo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size, embed_size = 512, 512\n",
    "encoder = Encoder(n_vocab_ja, embed_size, hidden_size)\n",
    "decoder = Decoder(n_vocab_en, embed_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1-20/200: ############################## 100% [00:01:54.80] loss train: 4.15695, test: 3.99432 \n",
      "  21-40/200: ############################## 100% [00:01:55.74] loss train: 2.67285, test: 3.99094 \n",
      "  41-60/200: ############################## 100% [00:01:55.98] loss train: 1.92427, test: 4.28475 \n",
      "  61-80/200: ############################## 100% [00:01:56.13] loss train: 1.39262, test: 4.66566 \n",
      " 81-100/200: ############################## 100% [00:01:56.07] loss train: 0.99579, test: 5.06977 \n",
      "101-120/200: ############################## 100% [00:01:55.67] loss train: 0.69274, test: 5.48003 \n",
      "121-140/200: ############################## 100% [00:01:56.16] loss train: 0.46373, test: 5.89901 \n",
      "141-160/200: ############################## 100% [00:02:00.77] loss train: 0.29749, test: 6.29669 \n",
      "161-180/200: ############################## 100% [00:02:00.28] loss train: 0.18124, test: 6.70021 \n",
      "181-200/200: ############################## 100% [00:02:05.66] loss train: 0.11154, test: 7.02597 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, n_epochs=200, prog_unit=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 翻訳\n",
    "\n",
    "作成したモデルに日本語文を入力し、英語に翻訳して出力する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_id = sp_en.unk_id() # UNKのID\n",
    "def token_sampling(y: List[float]) -> int:\n",
    "    \"\"\"モデルの出力から単語をサンプリングする\"\"\"\n",
    "    y[unk_id] = -torch.inf\n",
    "    probs = F.softmax(y, dim=-1)\n",
    "    token, = random.choices(range(n_vocab_en), weights=probs)\n",
    "    return token\n",
    "\n",
    "\n",
    "bos_id = sp_en.bos_id()\n",
    "eos_id = sp_en.eos_id()\n",
    "@torch.no_grad()\n",
    "def translate(\n",
    "    model: nn.Module,\n",
    "    in_text: str, # 入力文（日本語）\n",
    "    max_len: int = 50, # 出力のトークン数の上限\n",
    "    decisive: bool = True, # サンプリングを決定的にするか\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    in_ids = sp_ja.encode(in_text)\n",
    "    in_ids = torch.tensor(in_ids + [eos_id], device=device)\n",
    "\n",
    "    h = model.encoder(in_ids)\n",
    "    next_token = bos_id\n",
    "\n",
    "    token_ids = []\n",
    "    for _ in range(max_len):\n",
    "        x = torch.tensor([[next_token]], device=device)\n",
    "        hc = (h, torch.zeros_like(h))\n",
    "        # y, hc = model.decoder(x, hc)\n",
    "        y, h = model.decoder(x, h)\n",
    "        y = y[0]\n",
    "        if decisive:\n",
    "            next_token = y.argmax().item()\n",
    "        else:\n",
    "            next_token = token_sampling(y)\n",
    "        token_ids.append(next_token)\n",
    "        if next_token == eos_id:\n",
    "            break\n",
    "    sentence = sp_en.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは訓練データに含まれているものから。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 駅情報\n",
      "output: Information\n",
      "answer: Information\n",
      "\n",
      "input: 三条京阪駅（さんじょうけいはんえき）は、京都市東山区にある、京都市営地下鉄東西線の鉄道駅。\n",
      "output: Located in the Higashiyama Ward of Kyoto City, Sanjyo-Keihan Station is a stop on the Tozai Line, a Kyoto Municipal Subway Line.\n",
      "answer: Located in the Higashiyama Ward of Kyoto City, Sanjyo-Keihan Station is a stop on the Tozai Line, a Kyoto Municipal Subway Line.\n",
      "\n",
      "input: 駅番号はT11。\n",
      "output: The station number is T11.\n",
      "answer: The station number is T11.\n",
      "\n",
      "input: 京阪電気鉄道\n",
      "output: Keihan Electric Railway\n",
      "answer: The Keihan Electric Railway\n",
      "\n",
      "input: 京阪本線・京阪鴨東線（三条駅 (京都府)）\n",
      "output: Keihan Main Line and Keihan Oto Line in Sanjyo Station (in Kyoto Prefecture)\n",
      "answer: Keihan Main Line and Keihan Oto Line in Sanjyo Station (in Kyoto Prefecture)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for x, t in zip(data_ja[:n], data_en[:n]):\n",
    "    print('input:', x)\n",
    "    print('output:', translate(model, x))\n",
    "    print('answer:', t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "含まれていないものも試してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'この駅は京都市内の中心部にあります。',\n",
    "    '京都'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: この駅は京都市内の中心部にあります。\n",
      "output: Marutamachi Station (Kyoto Prefecture) of Kitakinki Tango Railway Corporation (KTR), Osaka in Kyoto)\n",
      "\n",
      "input: 京都\n",
      "output: E Station: bound Station/Kyoto Station Hachijoed Keihan Station.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print('input:', sentence)\n",
    "    print('output:', translate(model, sentence))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
