{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "\n",
    "*Sequence to Sequence*.  \n",
    "*Encoder-Decoder Model*とも。\n",
    "\n",
    "文章を入力とし、文章を出力するモデル。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext import transforms\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchvision.transforms import Compose\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 発想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで、RNNを用いて、入力した単語列に続く単語を予測するモデルを作成し、単語の予測を繰り返すことで文章を生成した。\n",
    "\n",
    "RNNは時系列の情報を保持するために隠れ状態$h_t$を用いる。隠れ状態の初期値$h_0$は0ベクトルとしているが、ここで、何らかの入力データから生成したベクトルを$h_0$として用いることを考える。このとき、上手く学習させれば、その入力に基づいた文章を生成できそう。\n",
    "\n",
    "例えば、入力を画像をとし、CNNを用いて抽出した特徴量を$h_0$として用いるようにすれば、入力画像に基づいた文章が生成できる。画像のキャプションなどが例に挙げられる。  \n",
    "学習方法は簡単で、入力画像に対して適切な文章が出力されるように学習させるだけ。隠れ状態を通じてRNNからCNNまで逆伝播を繋げる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、入力に文章を用いることはできないだろうか。RNNに文章を入力し、最後に出力された隠れ状態を文章ベクトルとする。これを別のRNNへの入力$h_0$とすれば、入力文に基づいた文章生成が可能になる。  \n",
    "文章ベクトルを生成するRNNを***Encoder***、文章ベクトルを受け取って出力文を生成するRNNを***Decoder***と呼ぶ。\n",
    "\n",
    "この発想は翻訳タスクに大きく役立つ。入力と出力に同じ意味を持った異なる言語の文章を設定すれば、入力文と同じ意味を持った文章生成が可能になる。\n",
    "\n",
    "本章では、入力に日本語文、出力に入力と同じ意味を持つ英語文を設定し、日本語→英語の翻訳を行うモデルを作成する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## データセット\n",
    "\n",
    "翻訳モデルを作るには、同じ意味を持つ文章が複数の言語でまとまっているデータが必要。このようなデータは対訳コーパスと呼んだりする。  \n",
    "本章では以下のデータセットを使用する。\n",
    "- [日英中基本文データ](https://nlp.ist.i.kyoto-u.ac.jp/index.php?%E6%97%A5%E8%8B%B1%E4%B8%AD%E5%9F%BA%E6%9C%AC%E6%96%87%E3%83%87%E3%83%BC%E3%82%BF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 5304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>japanese</th>\n",
       "      <th>english</th>\n",
       "      <th>chinese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#0001</td>\n",
       "      <td>Xではないかとつくづく疑問に思う</td>\n",
       "      <td>I often wonder if it might be X.</td>\n",
       "      <td>难道不会是X吗，我实在是感到怀疑。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#0002</td>\n",
       "      <td>Xがいいなといつも思います</td>\n",
       "      <td>I always think X would be nice.</td>\n",
       "      <td>我总觉得X不错。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#0003</td>\n",
       "      <td>それがあるようにいつも思います</td>\n",
       "      <td>It always seems like it is there.</td>\n",
       "      <td>我总觉得那好像是有的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#0004</td>\n",
       "      <td>それが多すぎないかと正直思う</td>\n",
       "      <td>I honestly feel like there is too much.</td>\n",
       "      <td>老实说我觉得那太多了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#0005</td>\n",
       "      <td>山田はみんなに好かれるタイプの人だと思う</td>\n",
       "      <td>I think that Yamada is the type everybody likes.</td>\n",
       "      <td>我想山田是受大家欢迎的那种人。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id              japanese   \n",
       "0  #0001      Xではないかとつくづく疑問に思う  \\\n",
       "1  #0002         Xがいいなといつも思います   \n",
       "2  #0003       それがあるようにいつも思います   \n",
       "3  #0004        それが多すぎないかと正直思う   \n",
       "4  #0005  山田はみんなに好かれるタイプの人だと思う   \n",
       "\n",
       "                                            english            chinese  \n",
       "0                  I often wonder if it might be X.  难道不会是X吗，我实在是感到怀疑。  \n",
       "1                   I always think X would be nice.           我总觉得X不错。  \n",
       "2                 It always seems like it is there.        我总觉得那好像是有的。  \n",
       "3           I honestly feel like there is too much.        老实说我觉得那太多了。  \n",
       "4  I think that Yamada is the type everybody likes.    我想山田是受大家欢迎的那种人。  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('data/JEC_basic_sentence_v1-3.xls', header=None)\n",
    "df.columns = ['id', 'japanese', 'english', 'chinese']\n",
    "print('num of data:', len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分かち書き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger('-Owakati')\n",
    "def tokenize(data: List[str], l='en') -> List[List[str]]:\n",
    "    if l == 'ja':\n",
    "        return [tagger.parse(sentence).split() for sentence in data]\n",
    "    elif l == 'en':\n",
    "        return [sent.replace('.', ' .').lower().split() for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['X', 'で', 'は', 'ない', 'か', 'と', 'つくづく', '疑問', 'に', '思う'],\n",
       " ['i', 'often', 'wonder', 'if', 'it', 'might', 'be', 'x', '.'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ja = tokenize(df['japanese'], l='ja')\n",
    "text_en = tokenize(df['english'], l='en')\n",
    "\n",
    "# examples\n",
    "text_ja[0], text_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en: You can keep the camera rolling from beginning to end or choose to record key sections only.\n",
      "ja: ミーティングの最初から最後までカメラを回すこともできるし、録画キーセクションのみを選択することもできます。\n",
      "\n",
      "en: I have customers from all over the country, and they are stunned when they see them–especially my Custom Templates! - Burt Ward, Founder, Gentle Giants Rescue and Adoptions – Florida, USA Video Newsletters allow me to send out a ton of information to my client list about new product ideas and upcoming events, all in one message.\n",
      "ja: 私達の顧客は国中に広がっています。 彼らはVideo Emailsを見て、驚きに目を見張ります。 特にカスタムテンプレートは最高です ! - バート・ワード、ジェントルジャイアンツ・レスキュー・アンド・アドプション創設者ー 米国フロリダ州 Video Newslettersを使えば、新製品や予定されているイベントなど情報をたっぷり、しかもひとつのメッセージでリ顧客リストに送ることができます。\n",
      "\n",
      "en: Whether you're at the office or on the go, Live Meetings is the perfect unified solution for collaborative meetings, large-scale webinars, and everything in between.\n",
      "ja: オフィスにいても外出先でも、 Live Meetings を使えば、小さな会議から大規模のスケールのウェビナーまで、すべて完璧に企画できます。\n",
      "\n",
      "en: The doctors are out of control, so you have to choose a folk remedy that you believe in and try it! . ...So that’s what it’s all about. The choice is yours.\n",
      "ja: 医師もお手上げ状態なのですから、自分の信じる民間療法を選択してやってみるしかありません。 . . .となるわけです。\n",
      "\n",
      "en: To respond to these changes in society, the pension system should also be reformed.\n",
      "ja: 社会が構造変化しているのに、昔の仕組みをそのまま維持しようとするところに無理がある。\n",
      "\n",
      "num of data: 10001\n"
     ]
    }
   ],
   "source": [
    "path = 'data/en-ja/en-ja.bicleaner05.txt'\n",
    "\n",
    "text_en = []\n",
    "text_ja = []\n",
    "lim = 10000\n",
    "with open(path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i > lim:\n",
    "            break\n",
    "        line = line.strip().split('\\t')\n",
    "        text_en.append(line[3])\n",
    "        text_ja.append(line[4])\n",
    "\n",
    "# examples\n",
    "for _ in range(5):\n",
    "    i = random.randint(0, len(text_en))\n",
    "    print('en:', text_en[i])\n",
    "    print('ja:', text_ja[i])\n",
    "    print()\n",
    "\n",
    "print('num of data:', len(text_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en: ['and', 'everyone', 'will', 'not', 'care', 'that', 'it', 'is', 'not', 'you', '.']\n",
      "ja: ['鼻', '・', '口', 'の', 'ところ', 'は', 'あらかじめ', '少し', '切っ', 'て', 'おく', 'と', 'いい', 'です', 'ね', '。']\n"
     ]
    }
   ],
   "source": [
    "text_en = tokenize(text_en, l='en')\n",
    "text_ja = tokenize(text_ja, l='ja')\n",
    "\n",
    "# example\n",
    "print('en:', text_en[0])\n",
    "print('ja:', text_ja[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習データ\n",
    "\n",
    "入力と出力のペアを作成する。  \n",
    "前処理はこれまでと同じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad, bos, eos, unk = '<pad>', '<bos>', '<eos>', '<unk>'\n",
    "specials = [pad, bos, eos, unk]\n",
    "vocab_ja = build_vocab_from_iterator(text_ja, specials=specials)\n",
    "vocab_en = build_vocab_from_iterator(text_en, specials=specials)\n",
    "\n",
    "# 後で使う\n",
    "def ids_to_sentence(token_ids, l='en'):\n",
    "    \"\"\"ID列 -> 文章\"\"\"\n",
    "    vocab = eval(f'vocab_{l}')\n",
    "    tokens = []\n",
    "    for i in token_ids[1:]:\n",
    "        if i == vocab.get_stoi()[eos]:\n",
    "            break\n",
    "        tokens.append(vocab.get_itos()[i])\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11419, 12149)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_ja = Compose([\n",
    "    transforms.AddToken(eos, begin=False),\n",
    "    transforms.VocabTransform(vocab_ja),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_en = Compose([\n",
    "    transforms.AddToken(bos, begin=True),\n",
    "    transforms.AddToken(eos, begin=False),\n",
    "    transforms.VocabTransform(vocab_en),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 語彙数\n",
    "n_vocab_ja = len(vocab_ja)\n",
    "n_vocab_en = len(vocab_en)\n",
    "n_vocab_ja, n_vocab_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力文と出力文のペアを作成する。  \n",
    "\n",
    "Decoder（出力文を生成するRNN）への入力も用意する必要がある。出力文の頭に\\<BOS>を付与したものとする。\n",
    "\n",
    "例）\n",
    "Encoderへの入力（入力文） | Decoderへの入力 | Eecoderの出力（出力文）\n",
    "--- | --- | ---\n",
    "夏 休み が 終わり ました 。 \\<EOS> | \\<BOS> Summer vacation is over . | Summer vacation is over . \\<EOS>\n",
    "ツイッター は 亡くなり ました 。 \\<EOS> | \\<BOS> Twitter is dead . | Twitter is dead . \\<EOS>\n",
    "今日 から X で 暮らし ましょう 。 \\<EOS> | \\<BOS> Let 's live in X from today . | Let 's live in X from today . \\<EOS>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaderを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_enc: torch.Size([32, 63])\n",
      "x_dec: torch.Size([32, 45])\n",
      "y_dec: torch.Size([32, 45])\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, in_text, out_text, in_transform, out_transform):\n",
    "        # 前処理\n",
    "        self.in_text = [in_transform(text) for text in in_text]\n",
    "        self.out_text = [out_transform(text) for text in out_text]\n",
    "        self.n_samples = len(in_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        in_text = self.in_text[index]\n",
    "        out_text = self.out_text[index]\n",
    "        return in_text, out_text[:-1], out_text[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "# パディング\n",
    "def to_padded_tensor(text_data: List[int], pad_value: int = 0) -> torch.Tensor:\n",
    "    data = pad_sequence(text_data, batch_first=True, padding_value=pad_value)\n",
    "    return data\n",
    "\n",
    "# バッチ内の系列長を揃える\n",
    "def collate_fn(batch):\n",
    "    x_enc, x_dec, y_dec = zip(*batch)\n",
    "    x_enc = to_padded_tensor(x_enc)\n",
    "    x_dec = to_padded_tensor(x_dec)\n",
    "    y_dec = to_padded_tensor(y_dec)\n",
    "    return x_enc, x_dec, y_dec\n",
    "\n",
    "dataset = TextDataset(text_ja, text_en, transform_ja, transform_en)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# examples\n",
    "x_enc, x_dec, y_dec = next(iter(dataloader))\n",
    "print('x_enc:', x_enc.shape)\n",
    "print('x_dec:', x_dec.shape)\n",
    "print('y_dec:', y_dec.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築\n",
    "\n",
    "モデルを作るよん。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "入力文を入れて隠れ状態を出力するだけのRNN。RNN層と線形層で作る。\n",
    "\n",
    "RNN層からの隠れ状態は、最後の単語を入力した時点のものを使用する。最後のトークンではないので注意。\\<EOS>が入力された時点の隠れ状態を使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_id = vocab_ja.get_stoi()[eos]\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        eos_positions = x == eos_id\n",
    "        x = self.embedding(x)\n",
    "        y, _ = self.rnn(x)\n",
    "        h = y[eos_positions] # (batch_size, hidden_size)\n",
    "        h = self.fc(h).unsqueeze(0) # (1, batch_size, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Encoderから出力された隠れ状態を受け取り、出力文を生成するRNN。Encoder同様、RNN層と線形層で作る。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        x = self.embedding(x)\n",
    "        y, h = self.rnn(x, h)\n",
    "        y = self.fc(y)\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "EncoderとDecoderを合わせて、入力から出力までの一連の処理を行うモデルを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, n_in_vocab, n_out_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(n_in_vocab, embed_size, hidden_size)\n",
    "        self.decoder = Decoder(n_out_vocab, embed_size, hidden_size)\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        h = self.encoder(x_enc)\n",
    "        y, _ = self.decoder(x_dec, h)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習\n",
    "\n",
    "学習するよん。特に変わったことはしないよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_en[pad])\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    model.train()\n",
    "    prog.start(n_iter=len(dataloader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        for x_enc, x_dec, y_dec in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device).ravel()\n",
    "\n",
    "            y_pred = model(x_enc, x_dec).reshape(-1, n_vocab_en)\n",
    "            loss = criterion(y_pred, y_dec)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prog.update(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(len(vocab_ja), len(vocab_en), 1024, 1024).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1-10/100: ######################################## 100% [00:02:33.43] loss: 2.08008 \n",
      "  11-20/100: ######################################## 100% [00:02:33.47] loss: 0.78840 \n",
      "  21-30/100: ######################################## 100% [00:02:31.78] loss: 0.28815 \n",
      "  31-40/100: ######################################## 100% [00:02:32.02] loss: 0.08198 \n",
      "  41-50/100: ######################################## 100% [00:02:33.32] loss: 0.03434 \n",
      "  51-60/100: ######################################## 100% [00:02:33.86] loss: 0.02659 \n",
      "  61-70/100: ######################################## 100% [00:02:33.75] loss: 0.02492 \n",
      "  71-80/100: ######################################## 100% [00:02:33.21] loss: 0.02349 \n",
      "  81-90/100: ######################################## 100% [00:02:33.98] loss: 0.01798 \n",
      " 91-100/100: ######################################## 100% [00:02:35.44] loss: 0.02284 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, 100, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 翻訳\n",
    "\n",
    "作成したモデルに日本語文を入力し、英語に翻訳して出力する。  \n",
    "未知語は非対応。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def translate(model, text, max_len=100, decisive=True):\n",
    "    model.eval()\n",
    "    tokens = tokenize([text], 'ja')[0]\n",
    "    tokens = transform_ja(tokens).unsqueeze(0).to(device)\n",
    "    h = model.encoder(tokens)\n",
    "    next_token = vocab_en[bos]\n",
    "\n",
    "    tokens = []\n",
    "    for _ in range(max_len):\n",
    "        next_token = torch.tensor(next_token).reshape(1, 1).to(device)\n",
    "        y, h = model.decoder(next_token, h)\n",
    "        y = F.softmax(y.ravel(), dim=0)\n",
    "\n",
    "        if decisive:\n",
    "            next_token = y.argmax().item() # 決定的な出力\n",
    "        else:\n",
    "            next_token = random.choices(range(len(y)), weights=y)[0] # 確率的な出力\n",
    "\n",
    "        if next_token == vocab_en[eos]:\n",
    "            break\n",
    "        tokens.append(next_token)\n",
    "    return ' '.join([vocab_en.get_itos()[t] for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは学習データに含まれているものから。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m n \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n\u001b[0;32m----> 3\u001b[0m     i \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(df))\n\u001b[1;32m      4\u001b[0m     sentence \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mjapanese\u001b[39m\u001b[39m'\u001b[39m][i]\n\u001b[1;32m      5\u001b[0m     answer \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m][i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(df))\n",
    "    sentence = df['japanese'][i]\n",
    "    answer = df['english'][i]\n",
    "    print('input:', sentence)\n",
    "    print('output:', translate(model, sentence))\n",
    "    print('answer:', answer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データにない適当な文章も試してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 私は猫です。\n",
      "output: it is the same for me .\n",
      "\n",
      "input: 昨日はいい天気でしたね。\n",
      "output: about how much do not just change into words .\n",
      "\n",
      "input: 彼は健康な身体を持っています。\n",
      "output: when i was 20 years old, i bought the car by myself .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    '私は猫です。',\n",
    "    '昨日はいい天気でしたね。',\n",
    "    '彼は健康な身体を持っています。'\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print('input:', sentence)\n",
    "    print('output:', translate(model, sentence))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "めちゃくちゃね。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
