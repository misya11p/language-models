{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "\n",
    "あるシーケンスから別のシーケンスへの変換を行うSeq2Seqというモデルを学び、機械翻訳へ応用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress(\n",
    "    width=20,\n",
    "    with_test=True,\n",
    "    label=\"ppl train\",\n",
    "    round=2,\n",
    "    agg_fn=lambda s, w: math.exp(s / w)\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 条件付き言語モデル\n",
    "\n",
    "言語モデルに文脈以外の条件を付与する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 言語モデルへの条件付け"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまでの言語モデルは文脈を条件とした確率モデルであった。\n",
    "\n",
    "$$\n",
    "p(w_t|w_{<t})\n",
    "$$\n",
    "\n",
    "ここで、文脈以外の条件を追加してみる。\n",
    "\n",
    "$$\n",
    "p(w_t|w_{<t}, c)\n",
    "$$\n",
    "\n",
    "これは、なんらかの条件$c$に基づいた文章を生成するモデルと見られる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えば条件を画像とする場合、画像に基づいた文章を生成するモデルとなり、画像のキャプション生成などに使える。また条件を音声とする場合、音声に基づいた文章を生成するモデルとなり、音声認識などに使える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、文章を条件とする場合を考えてみる。この場合、文章から文章を生成するモデルとなる。これはどんなことに使えるだろう。\n",
    "\n",
    "例えば文章要約が挙げられる。条件としてある文章を与え、そこから要点のみをまとめた新たな文章を生成する。また、機械翻訳も考えられそう。入力された文章から、同じ意味を持った別の言語の文章を生成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNへの条件付け\n",
    "\n",
    "RNNがこれらの条件を考慮するためにはどうすればよいだろうか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "といっても、条件から適当に特徴量を抽出し、それをRNNのどこかに繋げるだけでよい。適当なところで、足したり、結合して線形変換したり、やりようはいくらでもある。また、隠れ状態の初期値として条件を与える方法も考えられる。これまで0ベクトルとしていたところに、条件から抽出した特徴量を与える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、特徴抽出モデルをNNとすると、当然そのNNまで勾配が届くので、RNNと同時に学習することができる。実際に、CNNとRNNを繋げた画像のキャプション生成モデルが提案されている[1]。CNNで抽出した画像特徴量をRNNに隠れ状態の初期値として与えている。\n",
    "\n",
    "[1] [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Seq2Seq\n",
    "\n",
    "*Sequence to Sequence*。*Encoder-Decoder Model*とも。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまでRNNを言語モデルとして使ってきたが、RNNにはもう少しできることがある。それは特徴抽出である。ある時系列データを入力したときに得られる最後の隠れ状態には全ての時刻の情報が含まれており、これは特徴量と見ることが出来る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、時系列データを言語モデルへの条件として扱うことを考える。前節より、言語モデルをRNNとすると、隠れ状態の初期値として条件を与えることができる。そして時系列データの特徴抽出にはRNNが使えるため、最終的に2つのRNNを繋げたモデルができる。このモデルは時系列データ（Sequence）から時系列データへの変換を行うモデルと見られ、**Sequence to Sequence**または**Seq2Seq**と呼ばれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seqは時系列データからの特徴抽出を行うRNNと時系列データを生成するRNNに分かれている。前者を**Encoder**、後者を**Decoder**と呼ぶ。ここから、Seq2Seqは**Encoder-Decoderモデル**とも呼ばれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seqを用いることで時系列データから時系列データの生成が可能になる。言語モデルと関連した例を挙げると、機械翻訳や文章要約などがある。\n",
    "\n",
    "本章では機械翻訳でSeq2Seqを学ぶ。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 教師データの作成\n",
    "\n",
    "翻訳モデルの学習に必要な教師データを作成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 対訳コーパス\n",
    "\n",
    "翻訳モデルを作るには、同じ意味を持つ文章が複数の言語でまとまっているデータが必要。このようなデータは対訳コーパスと呼んだりする。\n",
    "\n",
    "本章では以下のデータセットから日本語と英語の対訳コーパスを使用する。\n",
    "\n",
    "- [iwslt2017  |  TensorFlow Datasets](https://www.tensorflow.org/datasets/community_catalog/huggingface/iwslt2017?hl=ja#iwslt2017-en-ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load(\n",
    "    \"huggingface:iwslt2017/iwslt2017-en-ja\",\n",
    "    data_dir=\"data\",\n",
    "    split=\"train\"\n",
    ")\n",
    "ds = list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 223108 \n",
      "\n",
      "腐敗が 病的に民主主義を 破壊してしまうということです つまり どのようなシステムであっても その構成員がごく少数の 構成員によって選ばれる場合には その構成員がごく少数の 構成員によって選ばれる場合には それは ごく少数の構成員が ごくごく少数の構成員が 改革を阻止できることを意味します\n",
      "It's a pathological, democracy-destroying corruption, because in any system where the members are dependent upon the tiniest fraction of us for their election, that means the tiniest number of us, the tiniest, tiniest number of us, can block reform.\n",
      "\n",
      "これは大変なことです\n",
      "This is a big deal.\n",
      "\n",
      "疑問というのは 西洋社会におけるすべての議論は 課税レベルに関することです\n",
      "We ask the question -- the whole debate in the Western world is about the level of taxation.\n",
      "\n",
      "つまり大きな意味では 技術というのは何も こんな機器だけをさすのではないのです 習慣、テクニック、心理的手法 なども含めて 技術と呼べるのです\n",
      "So in a broad sense, we don't need to think about technology as only little gadgets, like these things here, but even institutions and techniques, psychological methods and so forth.\n",
      "\n",
      "ひも理論が難しいなんて人もいますが 楽なもんです\n",
      "And some people think that string theory is tough.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_ja = []\n",
    "data_en = []\n",
    "for sample in ds:\n",
    "    ja = sample[\"translation\"][\"ja\"].decode()\n",
    "    en = sample[\"translation\"][\"en\"].decode()\n",
    "    data_ja.append(ja)\n",
    "    data_en.append(en)\n",
    "\n",
    "print(\"num of data:\", len(data_ja), \"\\n\")\n",
    "for _ in range(5):\n",
    "    i = random.randint(0, len(data_ja))\n",
    "    print(data_ja[i])\n",
    "    print(data_en[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "書き出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile_ja = \"data/iwslt2017_ja.txt\"\n",
    "with open(textfile_ja, \"w\") as f:\n",
    "    f.write(\"\\n\".join(data_ja))\n",
    "\n",
    "textfile_en = \"data/iwslt2017_en.txt\"\n",
    "with open(textfile_en, \"w\") as f:\n",
    "    f.write(\"\\n\".join(data_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile_ja = f\"data/iwslt2017_ja.txt\"\n",
    "textfile_en = f\"data/iwslt2017_en.txt\"\n",
    "\n",
    "with open(textfile_ja) as f:\n",
    "    data_ja = f.read().splitlines()\n",
    "with open(textfile_en) as f:\n",
    "    data_en = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語、英語別々にトークナイザを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_prefix_ja = f\"models/tokenizer_iwslt2017_ja\"\n",
    "tokenizer_prefix_en = f\"models/tokenizer_iwslt2017_en\"\n",
    "pad_id = 3\n",
    "vocab_size = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_ja,\n",
    "    model_prefix=tokenizer_prefix_ja,\n",
    "    vocab_size=vocab_size,\n",
    "    pad_id=pad_id\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_en,\n",
    "    model_prefix=tokenizer_prefix_en,\n",
    "    vocab_size=vocab_size,\n",
    "    pad_id=pad_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary (ja): 8000\n",
      "num of vocabrary (en): 8000\n"
     ]
    }
   ],
   "source": [
    "sp_ja = spm.SentencePieceProcessor(f\"{tokenizer_prefix_ja}.model\")\n",
    "sp_en = spm.SentencePieceProcessor(f\"{tokenizer_prefix_en}.model\")\n",
    "\n",
    "unk_id = sp_ja.unk_id()\n",
    "bos_id = sp_ja.bos_id()\n",
    "eos_id = sp_ja.eos_id()\n",
    "pad_id = sp_ja.pad_id()\n",
    "\n",
    "n_vocab_ja = len(sp_ja)\n",
    "n_vocab_en = len(sp_en)\n",
    "print(\"num of vocabrary (ja):\", n_vocab_ja)\n",
    "print(\"num of vocabrary (en):\", n_vocab_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids_ja = sp_ja.encode(data_ja)\n",
    "data_ids_en = sp_en.encode(data_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOS, EOSの追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids_ja, ids_en in zip(data_ids_ja, data_ids_en):\n",
    "    ids_en.insert(0, bos_id)\n",
    "    ids_ja.append(eos_id)\n",
    "    ids_en.append(eos_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 教師データの作成\n",
    "\n",
    "入力文と正解のペアを作成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常のRNNLMでは、トークン列とそれを1つずらしたトークン列がペアとなる。Seq2SeqではこのペアがDecoderへの入力と正解となり、これに加えてEncoderへの入力を用意する。\n",
    "\n",
    "例）\n",
    "\n",
    "入力（Encoder） | 入力（Decoder） | 正解\n",
    "--- | --- | ---\n",
    "夏 休み が 終わり ました 。 \\<EOS> | \\<BOS> Summer vacation is over . | Summer vacation is over . \\<EOS>\n",
    "ツイッター は 亡くなり ました 。 \\<EOS> | \\<BOS> Twitter is dead . | Twitter is dead . \\<EOS>\n",
    "今日 から X で 暮らし ましょう 。 \\<EOS> | \\<BOS> Let 's live in X from today . | Let 's live in X from today . \\<EOS>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataLoader`の作成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train data: 178487\n",
      "num of test data: 44621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 72]), torch.Size([32, 67]), torch.Size([32, 67]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids_ja, data_ids_en):\n",
    "        self.data_ja = [torch.tensor(ids) for ids in data_ids_ja]\n",
    "        self.data_en = [torch.tensor(ids) for ids in data_ids_en]\n",
    "        self.n_data = len(self.data_ja)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ja = self.data_ja[idx]\n",
    "        en = self.data_en[idx]\n",
    "        x_enc = ja # Encoderへの入力\n",
    "        x_dec = en[:-1] # Decoderへの入力\n",
    "        y_dec = en[1:] # Decoderの出力\n",
    "        return x_enc, x_dec, y_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "def collate_fn(batch): # padding\n",
    "    x_enc, x_dec, y_dec= zip(*batch)\n",
    "    x_enc = pad_sequence(x_enc, batch_first=True, padding_value=pad_id)\n",
    "    x_dec = pad_sequence(x_dec, batch_first=True, padding_value=pad_id)\n",
    "    y_dec = pad_sequence(y_dec, batch_first=True, padding_value=pad_id)\n",
    "    return x_enc, x_dec, y_dec\n",
    "\n",
    "dataset = TextDataset(data_ids_ja, data_ids_en)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "print(\"num of train data:\", len(train_dataset))\n",
    "print(\"num of test data:\", len(test_dataset))\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# example\n",
    "x_enc, x_dec, y_dec = next(iter(train_loader))\n",
    "x_enc.shape, x_dec.shape, y_dec.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築\n",
    "\n",
    "Encoder、Decoderを作成し、Seq2Seqモデルを作成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "入力文を入れて固定長のベクトルを出力するだけのRNN。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、何をDecoderに渡すかを考える。先で「最後の隠れ状態」と言ったが、実はそれ以外にもいくつか選択肢がある。\n",
    "\n",
    "Decoderに渡したい隠れ状態として満たしてほしい条件は以下である。\n",
    "\n",
    "- （padを除いた）全ての入力を参照して出力されている\n",
    "- 固定長\n",
    "\n",
    "これらを全て満たしていれば何でもよい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず考えられるのは最後の隠れ状態。これが一番シンプルで実装も簡単。\n",
    "\n",
    "ただこれだと学習時にpadトークンが隠れ状態に関与してしまう。多くの場合、推論時にpadトークンは存在しないため、学習時もpadトークンを考慮しない出力をさせた方が良さそう。また、padの数が多くなるにつれて隠れ状態がある一定の値に収束してしまう（経験談）。RNNに同じトークンを何度も入力することで隠れ状態が収束してしまうみたい。そうなってしまうと、入力文に依る隠れ状態の違いが少なくなり、入力文を考慮した出力が行えない。\n",
    "\n",
    "ではどうするかというと、「padを除いた最後の隠れ状態」とする。eosトークンが入力された時点の隠れ状態とも言える。これで上記の問題を解決できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "他にもいくつかの案が考えられる。例えば、「padを除いた全ての隠れ状態の平均」とか。これも条件を満たす。ただ今回は無難に、「padを除いた最後の隠れ状態」とする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ではEncoderを実装する。これまでのRNN同様、適当にLSTMと線形層で作る。ここで精度向上のため、以下の工夫を加える。\n",
    "\n",
    "- LSTM層を3層に増やす\n",
    "- 残差結合を取り入れる\n",
    "\n",
    "これらの工夫はSeq2Seq固有のものではなく、一般的なRNNに応用できる。例えば前章までのRNNLMにも適用でき、精度向上に繋がるはず。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm1 = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_skip = nn.Linear(embed_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        eos_pos = x == eos_id\n",
    "            # eosに対応する位置のみがTrueとなったTensor: (batch_size, seq_len)\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "\n",
    "        skip = self.fc_skip(x)\n",
    "        hs1, _ = self.lstm1(x) # (batch_size, seq_len, hidden_size)\n",
    "        hs1 = hs1 + skip\n",
    "        hs1 = self.dropout(hs1)\n",
    "\n",
    "        skip = hs1\n",
    "        hs2, _ = self.lstm2(hs1)\n",
    "        hs2 = hs2 + skip\n",
    "        hs2 = self.dropout(hs2)\n",
    "\n",
    "        skip = hs2\n",
    "        hs3, _ = self.lstm3(hs2)\n",
    "        hs3 = hs3 + skip\n",
    "        hs3 = self.dropout(hs3)\n",
    "\n",
    "        h1 = hs1[eos_pos] # (batch_size, hidden_size)\n",
    "        h2 = hs2[eos_pos]\n",
    "        h3 = hs3[eos_pos]\n",
    "        return h1, h2, h3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全ての層からの隠れ状態をタプルで出力する。もちろん、最後の層だけ使うみたいな事もできる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Encoderから出力された隠れ状態を受け取り、出力文を生成するRNN。Encoder同様、LSTMと線形層にいくつかの工夫を加えて作る。LSTM層の数はEncoderと同じにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm1 = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "        self.fc_skip = nn.Linear(embed_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        hc1, hc2, hc3 = hc\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "\n",
    "        skip = self.fc_skip(x) # (batch_size, seq_len, hidden_size)\n",
    "        hs, hc1 = self.lstm1(x, hc1) # (batch_size, seq_len, hidden_size)\n",
    "        hs = hs + skip\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        skip = hs\n",
    "        hs, hc2 = self.lstm2(hs, hc2)\n",
    "        hs = hs + skip\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        skip = hs\n",
    "        hs, _ = self.lstm3(hs)\n",
    "        hs = hs + skip\n",
    "        hs = self.dropout(hs)\n",
    "\n",
    "        y = self.fc(hs) # (batch_size, seq_len, n_vocab)\n",
    "        return y, (hc1, hc2, hc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hc`には各LSTM層に対応する隠れ状態がタプルで含まれていることを想定している。具体的に何が入るかは次へ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "EncoderとDecoderを合わせて、入力から出力までの一連の処理を行うモデルを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        h = self.encoder(x_enc)\n",
    "        hc = self.get_hc(h)\n",
    "        y, _ = self.decoder(x_dec, hc)\n",
    "        return y\n",
    "\n",
    "    def get_hc(self, h):\n",
    "        h = torch.stack(h, dim=0).unsqueeze(1)\n",
    "        c = torch.zeros_like(h)\n",
    "        hc = zip(h, c)\n",
    "        return hc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoderから受け取った`h`（`(h1, h2, h3)`）から`hc`（`(hc1, hc2, hc3)`）を作成し、Decoderに渡す。なお`hcn`は`(hn, cn)`で`cn`は0ベクトル。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of parameters: 25,691,456\n"
     ]
    }
   ],
   "source": [
    "hidden_size, embed_size = 512, 512\n",
    "encoder = Encoder(n_vocab_ja, embed_size, hidden_size)\n",
    "decoder = Decoder(n_vocab_en, embed_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"num of parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 実践\n",
    "\n",
    "実際にSeq2Seqを学習させて翻訳を行ってみる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "def loss_fn(y, t):\n",
    "    loss = cross_entropy(y.reshape(-1, n_vocab_ja), t.ravel())\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for x_enc, x_dec, y_dec in test_loader:\n",
    "        x_enc = x_enc.to(device)\n",
    "        x_dec = x_dec.to(device)\n",
    "        y_dec = y_dec.to(device)\n",
    "\n",
    "        y = model(x_enc, x_dec)\n",
    "        loss = loss_fn(y, y_dec)\n",
    "        losses.append(loss.item())\n",
    "    loss = sum(losses) / len(losses)\n",
    "    ppl = math.exp(loss)\n",
    "    return ppl\n",
    "\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    prog.start(n_iter=len(train_loader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for x_enc, x_dec, y_dec in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prog.update(loss.item())\n",
    "\n",
    "        if prog.now_epoch % prog_unit == 0:\n",
    "            test_ppl = eval_model(model)\n",
    "            prog.memo(f\"test: {test_ppl:.2f}\", no_step=True)\n",
    "        prog.memo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/20:                        0% [00:00:00.98] ppl train: 7654.46 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/20: #################### 100% [00:04:53.78] ppl train: 149.26, test: 90.66 \n",
      " 2/20: #################### 100% [00:04:46.93] ppl train: 85.17, test: 70.28 \n",
      " 3/20: #################### 100% [00:04:47.46] ppl train: 69.05, test: 61.11 \n",
      " 4/20: #################### 100% [00:04:47.45] ppl train: 60.01, test: 55.41 \n",
      " 5/20: #################### 100% [00:04:47.37] ppl train: 53.89, test: 51.44 \n",
      " 6/20: #################### 100% [00:04:47.91] ppl train: 49.29, test: 48.57 \n",
      " 7/20: #################### 100% [00:04:47.46] ppl train: 45.79, test: 46.47 \n",
      " 8/20: #################### 100% [00:04:47.44] ppl train: 42.86, test: 45.14 \n",
      " 9/20: #################### 100% [00:04:47.50] ppl train: 40.44, test: 43.57 \n",
      "10/20: #################### 100% [00:04:49.70] ppl train: 38.41, test: 42.47 \n",
      "11/20: #################### 100% [00:04:48.26] ppl train: 36.57, test: 41.65 \n",
      "12/20: #################### 100% [00:04:47.66] ppl train: 34.96, test: 40.98 \n",
      "13/20: #################### 100% [00:04:45.36] ppl train: 33.53, test: 40.57 \n",
      "14/20: #################### 100% [00:04:43.22] ppl train: 32.23, test: 39.88 \n",
      "15/20: #################### 100% [00:04:41.30] ppl train: 31.04, test: 39.85 \n",
      "16/20: #################### 100% [00:04:46.03] ppl train: 29.96, test: 39.62 \n",
      "17/20: #################### 100% [00:04:47.09] ppl train: 28.97, test: 39.51 \n",
      "18/20: #################### 100% [00:04:47.59] ppl train: 28.07, test: 39.28 \n",
      "19/20: #################### 100% [00:04:49.25] ppl train: 27.25, test: 39.09 \n",
      "20/20: #################### 100% [00:04:50.12] ppl train: 26.45, test: 39.29 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, n_epochs=20, prog_unit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/lm_seq2seq.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 翻訳\n",
    "\n",
    "作成したモデルに日本語文を入力し、英語に翻訳して出力する。決定的な出力にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_sampling(y, decisive=True):\n",
    "    y = y.squeeze(0, 1)\n",
    "    if decisive:\n",
    "        token = y.argmax().item()\n",
    "    else:\n",
    "        y[unk_id] = -torch.inf\n",
    "        probs = F.softmax(y, dim=-1)\n",
    "        token, = random.choices(range(n_vocab_en), weights=probs)\n",
    "    return token\n",
    "\n",
    "\n",
    "bos_id = sp_en.bos_id()\n",
    "eos_id = sp_en.eos_id()\n",
    "@torch.no_grad()\n",
    "def translate(\n",
    "    model: nn.Module,\n",
    "    in_text: str, # 入力文（日本語）\n",
    "    max_len: int = 100, # 出力のトークン数の上限\n",
    "    decisive: bool = True, # サンプリングを決定的にするか\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    in_ids = sp_ja.encode(in_text)\n",
    "    in_ids = torch.tensor(in_ids + [eos_id], device=device).unsqueeze(0)\n",
    "\n",
    "    h_enc = model.encoder(in_ids)\n",
    "    hc = model.get_hc(h_enc)\n",
    "    next_token = bos_id\n",
    "\n",
    "    token_ids = []\n",
    "    while len(token_ids) <= max_len and next_token != eos_id:\n",
    "        x = torch.tensor([next_token], device=device).reshape(1, 1)\n",
    "        y, hc = model.decoder(x, hc)\n",
    "        next_token = token_sampling(y, decisive)\n",
    "        token_ids.append(next_token)\n",
    "\n",
    "    sentence = sp_en.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは訓練データから。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 水面すれすれで 1つを除いてすべて無人島です そこには約35人の管理者が住んでいます\n",
      "output: It's a lot of rainwater, but they're also heavily hotterday than the Gulf Coast Redwood rainforest.\n",
      "answer: They're very low to the water, and they're all uninhabited, except one island has about 35 caretakers on it.\n",
      "\n",
      "input: このような文化が― 実際には残りすべての世界は実際― 西側諸国と比べて とても弱い立場にありましたが こういった国は西側諸国を 理解するように強いられてきました なぜなら 社会の中に西洋の存在があったからです\n",
      "output: And the culture of violence is very similar to the extent that we have been able to cope with the Arab Springs, and then finally, in spite of the poorest congressionalities, the oldest Arab Springspherence, but also the same thing as the Arab Spring.\n",
      "answer: Whereas those cultures -- virtually the rest of the world, in fact, which have been in a far weaker position, vis-a-vis the West -- have been thereby forced to understand the West, because of the West's presence in those societies.\n",
      "\n",
      "input: あなたはあなたを構成する部品を 合体させたものなのです\n",
      "output: You can't afford to use your genomes.\n",
      "answer: You are the sum of your parts.\n",
      "\n",
      "input: 細胞のことをミニ宇宙ステーションにたとえて 電気的観点から 考えることができます\n",
      "output: And we can compare the circular magnetism to transmitteriates.\n",
      "answer: We can think of a cell from an electrical perspective as if it's a mini space station.\n",
      "\n",
      "input: シックだったけれど、なんていうか、とっても退屈そうだった\n",
      "output: It was like a lot of fungibbed, but unfortunately, they were terrified.\n",
      "answer: You know, she was very chic, but she was very filled with ennui, you know.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(train_dataset))\n",
    "    x, _, t = train_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print(\"input:\", x)\n",
    "    print(\"output:\", translate(model, x))\n",
    "    print(\"answer:\", t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "あまりよくないね。\n",
    "\n",
    "訓練データに含まれていないものも試してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 素晴らしく 満ち足りた気持ちになります\n",
      "output: It was a lot of patience.\n",
      "answer: I get this extraordinary feeling of well-being.\n",
      "\n",
      "input: 嫁ぎ先で彼女はゾッとしました トイレが無いのです\n",
      "output: And she's getting rid of the pesticides.\n",
      "answer: And she was horrified to get there and find that they didn't have a toilet.\n",
      "\n",
      "input: 数ヶ月前ニューヨークのタイムズスクウェアで パキスタン系イスラム教徒の男が車の爆破を企てました\n",
      "output: In 2005, I was invited to write a book called \"The Seventy Jobs\" was published in 1998.\n",
      "answer: Like a couple months ago in Times Square in New York, there was this Pakistani Muslim guy who tried to blow up a car bomb.\n",
      "\n",
      "input: 私はそれをふるさとの安全と呼んでいます\n",
      "output: And I'm trying to protect them.\n",
      "answer: And I call it \"hometown security.\"\n",
      "\n",
      "input: じっと見ていると画面から客席の方に浮かびあがってきます\n",
      "output: You can see the imagery from the beginning, and they're going to be able to read.\n",
      "answer: And if you sit here long enough, it'll float off the page into the audience.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(test_dataset))\n",
    "    x, _, t = test_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print(\"input:\", x)\n",
    "    print(\"output:\", translate(model, x))\n",
    "    print(\"answer:\", t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ありがとう\n",
      "output: Thank you.\n",
      "\n",
      "input: 猫はかわいいのです\n",
      "output: You know, they're nuts.\n",
      "\n",
      "input: 上手く文章が書けるようになりました\n",
      "output: They were playing video games.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "sentences = [\n",
    "    \"ありがとう\",\n",
    "    \"猫はかわいいのです\",\n",
    "    \"上手く文章が書けるようになりました\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"input:\", sentence)\n",
    "    print(\"output:\", translate(model, sentence))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "びみょう。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
