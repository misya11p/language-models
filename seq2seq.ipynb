{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "\n",
    "*Sequence to Sequence*.  \n",
    "*Encoder-Decoder Model*とも。\n",
    "\n",
    "2つのモデルを用いて、入力されたシーケンスに基づいた別のシーケンスを出力するモデル。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで、RNN（やLSTM）を用いて、入力した単語列に続く単語を予測するモデルを作成し、単語の予測を繰り返すことで文章を生成した。\n",
    "\n",
    "RNNは時系列の情報を保持するために隠れ状態$h_t$を用いる。隠れ状態の初期値$h_0$は0ベクトルとしているが、ここで、何らかの入力データから生成したベクトルを$h_0$として用いることを考える。このとき、上手く学習させれば、その入力に基づいた文章を生成できるのではないか。\n",
    "\n",
    "例えば、入力を画像をとし、CNNを用いて抽出した特徴量を$h_0$として用いるようにすれば、入力画像に基づいた文章が生成できる。画像のキャプション生成などに応用できそう。隠れ状態を通じてRNNからCNNまで逆伝播が繋がるので、画像と文章のペアさえ用意すれば学習できそう。というかできる[1]。\n",
    "\n",
    "[1] [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、入力に文章を用いることはできないだろうか。RNNに文章を入力し、最後に出力された隠れ状態を文章ベクトルとする。これを別のRNNへの入力$h_0$とすれば、入力文に基づいた文章生成が可能になる。\n",
    "\n",
    "この発想は翻訳タスクに大きく役立つ。入力と出力に同じ意味を持った異なる言語の文章を設定すれば、入力文と同じ意味を持った文章生成が可能になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章ではこの翻訳モデルを作成する。入力に日本語文、出力に入力と同じ意味を持つ英語文を設定し、日本語→英語の翻訳を行うモデルを作成する。こういった、シーケンスをシーケンスに変換するモデルを *Seq2Seq (Sequence to Sequence)* と呼ぶ。\n",
    "\n",
    "Seq2Seqは以下の二つのRNNから構成される。\n",
    "\n",
    "- ***Encoder*** : 文章ベクトルを生成するRNN\n",
    "- ***Decoder*** : 文章ベクトルを受け取って出力文を生成するRNN\n",
    "\n",
    "このことから***Encoder-Decoderモデル***とも呼ばれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress(\n",
    "    width=20,\n",
    "    with_test=True,\n",
    "    label=\"ppl train\",\n",
    "    round=2,\n",
    "    agg_fn=lambda s, w: math.exp(s / w)\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 対訳コーパス\n",
    "\n",
    "翻訳モデルを作るには、同じ意味を持つ文章が複数の言語でまとまっているデータが必要。このようなデータは対訳コーパスと呼んだりする。\n",
    "\n",
    "本章では以下のデータセットから日本語と英語の対訳コーパスを使用する。\n",
    "\n",
    "- [iwslt2017  |  TensorFlow Datasets](https://www.tensorflow.org/datasets/community_catalog/huggingface/iwslt2017?hl=ja#iwslt2017-en-ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load(\n",
    "    \"huggingface:iwslt2017/iwslt2017-en-ja\",\n",
    "    data_dir=\"data\",\n",
    "    split=\"train\"\n",
    ")\n",
    "ds = list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 223108 \n",
      "\n",
      "腐敗が 病的に民主主義を 破壊してしまうということです つまり どのようなシステムであっても その構成員がごく少数の 構成員によって選ばれる場合には その構成員がごく少数の 構成員によって選ばれる場合には それは ごく少数の構成員が ごくごく少数の構成員が 改革を阻止できることを意味します\n",
      "It's a pathological, democracy-destroying corruption, because in any system where the members are dependent upon the tiniest fraction of us for their election, that means the tiniest number of us, the tiniest, tiniest number of us, can block reform.\n",
      "\n",
      "これは大変なことです\n",
      "This is a big deal.\n",
      "\n",
      "疑問というのは 西洋社会におけるすべての議論は 課税レベルに関することです\n",
      "We ask the question -- the whole debate in the Western world is about the level of taxation.\n",
      "\n",
      "つまり大きな意味では 技術というのは何も こんな機器だけをさすのではないのです 習慣、テクニック、心理的手法 なども含めて 技術と呼べるのです\n",
      "So in a broad sense, we don't need to think about technology as only little gadgets, like these things here, but even institutions and techniques, psychological methods and so forth.\n",
      "\n",
      "ひも理論が難しいなんて人もいますが 楽なもんです\n",
      "And some people think that string theory is tough.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_ja = []\n",
    "data_en = []\n",
    "for sample in ds:\n",
    "    ja = sample[\"translation\"][\"ja\"].decode()\n",
    "    en = sample[\"translation\"][\"en\"].decode()\n",
    "    data_ja.append(ja)\n",
    "    data_en.append(en)\n",
    "\n",
    "print(\"num of data:\", len(data_ja), \"\\n\")\n",
    "for _ in range(5):\n",
    "    i = random.randint(0, len(data_ja))\n",
    "    print(data_ja[i])\n",
    "    print(data_en[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "書き出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile_ja = \"data/iwslt2017_ja.txt\"\n",
    "with open(textfile_ja, \"w\") as f:\n",
    "    f.write(\"\\n\".join(data_ja))\n",
    "\n",
    "textfile_en = \"data/iwslt2017_en.txt\"\n",
    "with open(textfile_en, \"w\") as f:\n",
    "    f.write(\"\\n\".join(data_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile_ja = f\"data/iwslt2017_ja.txt\"\n",
    "textfile_en = f\"data/iwslt2017_en.txt\"\n",
    "\n",
    "with open(textfile_ja) as f:\n",
    "    data_ja = f.read().splitlines()\n",
    "with open(textfile_en) as f:\n",
    "    data_en = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語、英語別々にトークナイザを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_prefix_ja = f\"models/tokenizer_iwslt2017_ja\"\n",
    "tokenizer_prefix_en = f\"models/tokenizer_iwslt2017_en\"\n",
    "pad_id = 3\n",
    "vocab_size = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_ja,\n",
    "    model_prefix=tokenizer_prefix_ja,\n",
    "    vocab_size=vocab_size,\n",
    "    pad_id=pad_id\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_en,\n",
    "    model_prefix=tokenizer_prefix_en,\n",
    "    vocab_size=vocab_size,\n",
    "    pad_id=pad_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary (ja): 8000\n",
      "num of vocabrary (en): 8000\n"
     ]
    }
   ],
   "source": [
    "sp_ja = spm.SentencePieceProcessor(f\"{tokenizer_prefix_ja}.model\")\n",
    "sp_en = spm.SentencePieceProcessor(f\"{tokenizer_prefix_en}.model\")\n",
    "\n",
    "unk_id = sp_ja.unk_id()\n",
    "bos_id = sp_ja.bos_id()\n",
    "eos_id = sp_ja.eos_id()\n",
    "pad_id = sp_ja.pad_id()\n",
    "\n",
    "n_vocab_ja = len(sp_ja)\n",
    "n_vocab_en = len(sp_en)\n",
    "print(\"num of vocabrary (ja):\", n_vocab_ja)\n",
    "print(\"num of vocabrary (en):\", n_vocab_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids_ja = sp_ja.encode(data_ja)\n",
    "data_ids_en = sp_en.encode(data_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOS, EOSの追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids_ja, ids_en in zip(data_ids_ja, data_ids_en):\n",
    "    ids_en.insert(0, bos_id)\n",
    "    ids_ja.append(eos_id)\n",
    "    ids_en.append(eos_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データの作成\n",
    "\n",
    "入力文と正解のペアを作成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoderへの入力（入力文）とDecoderの出力（正解）のペアを作成する。また、Decoderへの入力を考える必要がある。今回は教師強制を採用し、出力文の頭に\\<BOS>を付与したものをDecoderへの入力とする。\n",
    "\n",
    "例）\n",
    "Encoderへの入力（入力文） | Decoderへの入力 | Decoderの出力（出力文）\n",
    "--- | --- | ---\n",
    "夏 休み が 終わり ました 。 \\<EOS> | \\<BOS> Summer vacation is over . | Summer vacation is over . \\<EOS>\n",
    "ツイッター は 亡くなり ました 。 \\<EOS> | \\<BOS> Twitter is dead . | Twitter is dead . \\<EOS>\n",
    "今日 から X で 暮らし ましょう 。 \\<EOS> | \\<BOS> Let 's live in X from today . | Let 's live in X from today . \\<EOS>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataLoader`の作成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train data: 178487\n",
      "num of test data: 44621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 109]), torch.Size([32, 93]), torch.Size([32, 93]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids_ja, data_ids_en):\n",
    "        self.data_ja = [torch.tensor(ids) for ids in data_ids_ja]\n",
    "        self.data_en = [torch.tensor(ids) for ids in data_ids_en]\n",
    "        self.n_data = len(self.data_ja)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ja = self.data_ja[idx]\n",
    "        en = self.data_en[idx]\n",
    "        x_enc = ja # Encoderへの入力\n",
    "        x_dec = en[:-1] # Decoderへの入力\n",
    "        y_dec = en[1:] # Decoderの出力\n",
    "        return x_enc, x_dec, y_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "def collate_fn(batch): # padding\n",
    "    x_enc, x_dec, y_dec= zip(*batch)\n",
    "    x_enc = pad_sequence(x_enc, batch_first=True, padding_value=pad_id)\n",
    "    x_dec = pad_sequence(x_dec, batch_first=True, padding_value=pad_id)\n",
    "    y_dec = pad_sequence(y_dec, batch_first=True, padding_value=pad_id)\n",
    "    return x_enc, x_dec, y_dec\n",
    "\n",
    "dataset = TextDataset(data_ids_ja, data_ids_en)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "print(\"num of train data:\", len(train_dataset))\n",
    "print(\"num of test data:\", len(test_dataset))\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# example\n",
    "x_enc, x_dec, y_dec = next(iter(train_loader))\n",
    "x_enc.shape, x_dec.shape, y_dec.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Seq2Seqを用いた翻訳モデル\n",
    "\n",
    "Encoder、Decoderを作成し、Seq2Seqモデルを作成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "入力文を入れて隠れ状態を出力するだけのRNN。LSTMと線形層で作る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、何を隠れ状態とするかを考える。Decoderに渡したい隠れ状態として満たしてほしい条件は以下である。\n",
    "\n",
    "- （padを除いた）全ての入力を参照して出力されている\n",
    "- 固定長\n",
    "\n",
    "これらを全て満たしていれば何でもよい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "よくあるのは最後の隠れ状態。これが一番シンプルで実装も簡単。\n",
    "\n",
    "ただこれだと学習時にpadトークンが隠れ状態に関与してしまう。多くの場合、推論時にpadトークンは存在しないため、学習時もpadトークンを考慮しない出力をさせた方が良さそう。また、padの数が多くなるにつれて隠れ状態がある一定の値に収束してしまう（経験談）。RNNに同じトークンを何度も入力することで隠れ状態が収束してしまうみたい。そうなってしまうと、入力文に依る隠れ状態の違いが少なくなり、入力文を考慮した出力が行えない。\n",
    "\n",
    "ではどうするかというと、「padを除いた最後の隠れ状態」とする。eosトークンが入力された時点の隠れ状態とも言える。これで上記の問題を解決できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "他にもいくつかの案が考えられる。例えば、「padを除いた全ての隠れ状態の平均」とか。これも条件を満たす。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、RNNを双方向にするという手法もある。Encoderは入力文の特徴を抽出できれば良いので、文の初めから順に処理しなければならない訳ではない。例えば逆から処理してもよい。\n",
    "\n",
    "双方向RNNでは、順方向と逆方向の双方向で演算を行い、各時刻で二つの隠れ状態を出力する。両端の時刻の隠れ状態を取得することで二種類の隠れ状態が得られ、それらを足すないしは結合することで最終的な一つの隠れ状態が得られる。\n",
    "\n",
    "双方向RNNは`torch.nn.RNN`の引数`bidirectional=True`で実装できる。ただ、各隠れ状態の出力がpadトークンを除いて行われて欲しいという望みがあり、それを実現するための実装が難しそうなので、今回は不採用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結局どうしようかという感じだが、無難に「padを除いた最後の隠れ状態」を採用する。Encoderを以下のように実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        eos_pos = x == eos_id\n",
    "            # eosに対応する位置のみがTrueとなったTensor: (batch_size, seq_len)\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        hs, _ = self.lstm(x) # (batch_size, seq_len, hidden_size)\n",
    "        h = hs[eos_pos] # (batch_size, hidden_size)\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc(h) # (batch_size, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Encoderから出力された隠れ状態を受け取り、出力文を生成するRNN。Encoder同様、LSTMと線形層で作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        hs, hc = self.lstm(x, hc) # (batch_size, seq_len, hidden_size)\n",
    "        hs = self.dropout(hs)\n",
    "        y = self.fc(hs) # (batch_size, seq_len, n_vocab)\n",
    "        return y, hc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "EncoderとDecoderを合わせて、入力から出力までの一連の処理を行うモデルを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        h = self.encoder(x_enc)\n",
    "        hc = self.get_hc(h)\n",
    "        y, _ = self.decoder(x_dec, hc)\n",
    "        return y\n",
    "\n",
    "    def get_hc(self, h):\n",
    "        h = h.unsqueeze(0)\n",
    "        c = torch.zeros_like(h)\n",
    "        hc = (h, c)\n",
    "        return hc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of parameters: 16,761,152\n"
     ]
    }
   ],
   "source": [
    "hidden_size, embed_size = 512, 512\n",
    "encoder = Encoder(n_vocab_ja, embed_size, hidden_size)\n",
    "decoder = Decoder(n_vocab_en, embed_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"num of parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 実践"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "def loss_fn(y, t):\n",
    "    loss = cross_entropy(y.reshape(-1, n_vocab_ja), t.ravel())\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for x_enc, x_dec, y_dec in test_loader:\n",
    "        x_enc = x_enc.to(device)\n",
    "        x_dec = x_dec.to(device)\n",
    "        y_dec = y_dec.to(device)\n",
    "\n",
    "        y = model(x_enc, x_dec)\n",
    "        loss = loss_fn(y, y_dec)\n",
    "        losses.append(loss.item())\n",
    "    loss = sum(losses) / len(losses)\n",
    "    ppl = math.exp(loss)\n",
    "    return ppl\n",
    "\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    prog.start(n_iter=len(train_loader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for x_enc, x_dec, y_dec in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prog.update(loss.item())\n",
    "\n",
    "        if prog.now_epoch % prog_unit == 0:\n",
    "            test_ppl = eval_model(model)\n",
    "            prog.memo(f\"test: {test_ppl:.2f}\", no_step=True)\n",
    "        prog.memo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/20: #################### 100% [00:02:19.34] ppl train: 173.17, test: 111.86 \n",
      " 2/20: #################### 100% [00:02:17.17] ppl train: 97.66, test: 85.91  \n",
      " 3/20: #################### 100% [00:02:16.70] ppl train: 78.36, test: 73.13 \n",
      " 4/20: #################### 100% [00:02:16.97] ppl train: 67.43, test: 65.63 \n",
      " 5/20: #################### 100% [00:02:17.20] ppl train: 60.25, test: 60.59 \n",
      " 6/20: #################### 100% [00:02:16.97] ppl train: 54.97, test: 57.03 \n",
      " 7/20: #################### 100% [00:02:17.02] ppl train: 50.86, test: 54.11 \n",
      " 8/20: #################### 100% [00:02:17.59] ppl train: 47.50, test: 51.97 \n",
      " 9/20: #################### 100% [00:02:17.06] ppl train: 44.67, test: 50.31 \n",
      "10/20: #################### 100% [00:02:17.68] ppl train: 42.29, test: 48.80 \n",
      "11/20: #################### 100% [00:02:17.02] ppl train: 40.26, test: 47.76 \n",
      "12/20: #################### 100% [00:02:15.11] ppl train: 38.44, test: 46.93 \n",
      "13/20: #################### 100% [00:02:18.09] ppl train: 36.83, test: 46.28 \n",
      "14/20: #################### 100% [00:02:20.58] ppl train: 35.39, test: 45.80 \n",
      "15/20: #################### 100% [00:02:26.09] ppl train: 34.09, test: 45.45 \n",
      "16/20: #################### 100% [00:02:19.98] ppl train: 32.91, test: 45.07 \n",
      "17/20: #################### 100% [00:02:22.16] ppl train: 31.81, test: 44.93 \n",
      "18/20: #################### 100% [00:02:21.75] ppl train: 30.84, test: 44.85 \n",
      "19/20: #################### 100% [00:02:21.41] ppl train: 29.89, test: 44.81 \n",
      "20/20: #################### 100% [00:02:18.97] ppl train: 29.05, test: 45.02 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, n_epochs=20, prog_unit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/lm_seq2seq.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 翻訳\n",
    "\n",
    "作成したモデルに日本語文を入力し、英語に翻訳して出力する。決定的な出力にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_sampling(y, decisive=True):\n",
    "    y = y.squeeze(0, 1)\n",
    "    if decisive:\n",
    "        token = y.argmax().item()\n",
    "    else:\n",
    "        y[unk_id] = -torch.inf\n",
    "        probs = F.softmax(y, dim=-1)\n",
    "        token, = random.choices(range(n_vocab_en), weights=probs)\n",
    "    return token\n",
    "\n",
    "\n",
    "bos_id = sp_en.bos_id()\n",
    "eos_id = sp_en.eos_id()\n",
    "@torch.no_grad()\n",
    "def translate(\n",
    "    model: nn.Module,\n",
    "    in_text: str, # 入力文（日本語）\n",
    "    max_len: int = 100, # 出力のトークン数の上限\n",
    "    decisive: bool = True, # サンプリングを決定的にするか\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    in_ids = sp_ja.encode(in_text)\n",
    "    in_ids = torch.tensor(in_ids + [eos_id], device=device).unsqueeze(0)\n",
    "\n",
    "    h_enc = model.encoder(in_ids)\n",
    "    hc = model.get_hc(h_enc)\n",
    "    next_token = bos_id\n",
    "\n",
    "    token_ids = []\n",
    "    while len(token_ids) <= max_len and next_token != eos_id:\n",
    "        x = torch.tensor([next_token], device=device).reshape(1, 1)\n",
    "        y, hc = model.decoder(x, hc)\n",
    "        next_token = token_sampling(y, decisive)\n",
    "        token_ids.append(next_token)\n",
    "\n",
    "    sentence = sp_en.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは訓練データから。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: お気づきでしょうか どの国でも 同じ3線が急勾配です\n",
      "output: And you can see that the most important thing is that the average person in the world is not going to be the same as the United States.\n",
      "answer: Notice also that on all of these graphs, the slope is steeper on in-group, authority, purity.\n",
      "\n",
      "input: こうして男性職員の割合は 14%から7%に落ち込みました\n",
      "output: And the average American soldiers were killed.\n",
      "answer: We went from 14 percent to seven percent.\n",
      "\n",
      "input: でもホントにインターネットは世界を変えた\n",
      "output: But I think we're going to be able to do this for the future.\n",
      "answer: No, but it's true: the Internet has changed the world.\n",
      "\n",
      "input: 覚えていてください 環境の問題とは 溶けている氷河や凍土の事だけではなく 私たちの子どもの問題でもあるのです\n",
      "output: And the reason why this is, is that the world is not just a little bit of the  ⁇ -ray of the Earth's body, and it's a very powerful way to get it.\n",
      "answer: And my urging is that when we think about environmental issues that we remember that it's not just about melting glaciers and ice caps, but it's also about our children as well.\n",
      "\n",
      "input: 骨が見えるほどです\n",
      "output: It's a little bit of a protein that's built on the surface.\n",
      "answer: You can see his bones through the skin.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(train_dataset))\n",
    "    x, _, t = train_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print(\"input:\", x)\n",
    "    print(\"output:\", translate(model, x))\n",
    "    print(\"answer:\", t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練データに含まれていないものも試してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: でも私は こんなことを 考えるようになりました 「今までとは違う方法で ストーリーを語るために 現代の進歩した技術を使えないか ― 私たちが この100年使ってきた 旧来の映像制作のツールでは 語り切れないような 別の形のストーリーを語れないか?」\n",
      "output: But I wanted to be a part of the world, and I'd like to ask you, \"How do you think about the world's world, and the world's most important, and the world's most important, and I'm going to do this, and I'm going to do this, and I'm going to do this with the world.\"\n",
      "answer: But I started thinking about, is there a way that I can use modern and developing technologies to tell stories in different ways and tell different kinds of stories that maybe I couldn't tell using the traditional tools of filmmaking that we've been using for 100 years?\n",
      "\n",
      "input: うーん\n",
      "output: Yeah.\n",
      "answer: Mmm.\n",
      "\n",
      "input: あの時の父さんは 砂浜から僕に叫んでたよね」\n",
      "output: And I'm going to go to the door, and I'm going to go to the door.\"\n",
      "answer: I remember my image of you was that you were up on the shore yelling at me.\"\n",
      "\n",
      "input: これで年齢の話はもういいわね\n",
      "output: So I'm going to talk about the future of the future.\n",
      "answer: So this takes care of my age.\n",
      "\n",
      "input: 公共部門の債務が少なければ 政府も どちらか選ぶことなく 教育・保健に投資をしながら 債務利息の支払いができます\n",
      "output: If we can do that, we can get to the local community, and we're going to have to be able to do that, and we're going to have to be able to do that.\n",
      "answer: When public sector debt is low, governments don't have to choose between investing in education and health and paying interest on that debt you owe.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(test_dataset))\n",
    "    x, _, t = test_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print(\"input:\", x)\n",
    "    print(\"output:\", translate(model, x))\n",
    "    print(\"answer:\", t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ありがとう\n",
      "output: Thank you.\n",
      "\n",
      "input: 猫はかわいいのです\n",
      "output: The sympathy is a little bit of a good thing.\n",
      "\n",
      "input: 上手く文章が書けるようになりました\n",
      "output: And they were all the way to read.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "sentences = [\n",
    "    \"ありがとう\",\n",
    "    \"猫はかわいいのです\",\n",
    "    \"上手く文章が書けるようになりました\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"input:\", sentence)\n",
    "    print(\"output:\", translate(model, sentence))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
