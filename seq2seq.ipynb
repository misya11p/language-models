{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "\n",
    "*Sequence to Sequence*.  \n",
    "*Encoder-Decoder Model*とも。\n",
    "\n",
    "2つのモデルを用いて、入力されたシーケンスに基づいた別のシーケンスを出力するモデル。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress(with_test=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 発想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで、RNNを用いて、入力した単語列に続く単語を予測するモデルを作成し、単語の予測を繰り返すことで文章を生成した。\n",
    "\n",
    "RNNは時系列の情報を保持するために隠れ状態$h_t$を用いる。隠れ状態の初期値$h_0$は0ベクトルとしているが、ここで、何らかの入力データから生成したベクトルを$h_0$として用いることを考える。このとき、上手く学習させれば、その入力に基づいた文章を生成できそう。\n",
    "\n",
    "例えば、入力を画像をとし、CNNを用いて抽出した特徴量を$h_0$として用いるようにすれば、入力画像に基づいた文章が生成できる。画像のキャプションなどが例に挙げられる。  \n",
    "学習方法は簡単で、入力画像に対して適切な文章が出力されるように学習させるだけ。隠れ状態を通じてRNNからCNNまで逆伝播を繋げる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、入力に文章を用いることはできないだろうか。RNNに文章を入力し、最後に出力された隠れ状態を文章ベクトルとする。これを別のRNNへの入力$h_0$とすれば、入力文に基づいた文章生成が可能になる。  \n",
    "\n",
    "この発想は翻訳タスクに大きく役立つ。入力と出力に同じ意味を持った異なる言語の文章を設定すれば、入力文と同じ意味を持った文章生成が可能になる。  \n",
    "こういった、シーケンスをシーケンスに変換するモデルを *seq2seq (Sequence to Sequence)* と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章では、入力に日本語文、出力に入力と同じ意味を持つ英語文を設定し、日本語→英語の翻訳を行うモデルを作成する。\n",
    "\n",
    "このモデルは以下の二つのRNNから構成される。\n",
    "- ***Encoder*** : 文章ベクトルを生成するRNN\n",
    "- ***Decoder*** : 文章ベクトルを受け取って出力文を生成するRNN\n",
    "\n",
    "このことから***Encoder-Decoderモデル***とも呼ばれる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 対訳コーパス\n",
    "\n",
    "翻訳モデルを作るには、同じ意味を持つ文章が複数の言語でまとまっているデータが必要。このようなデータは対訳コーパスと呼んだりする。\n",
    "\n",
    "本章では以下のデータセットを使用する。\n",
    "- [iwslt2017  |  TensorFlow Datasets](https://www.tensorflow.org/datasets/community_catalog/huggingface/iwslt2017?hl=ja#iwslt2017-en-ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load(\n",
    "    'huggingface:iwslt2017/iwslt2017-en-ja',\n",
    "    data_dir='data',\n",
    "    split='train'\n",
    ")\n",
    "ds = list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 223108 \n",
      "\n",
      "送金の媒体を利用して このような債券を移民に販売できます 毎月の送金のタイミングに合わせて 毎月の送金のタイミングに合わせて 債券を売ることができます\n",
      "Remittance channels can be used to sell these bonds to migrants because when they come on a monthly basis to send remittances, that's when you can actually sell it to them.\n",
      "\n",
      "現在のロボットでは 真似できない\n",
      "like no robot we have yet.\n",
      "\n",
      "いつの日か 顕微鏡をいくつもいくつも並べて 全てのニューロンとシナプスを捉えた 巨大なイメージデータベースを作り\n",
      "Someday, a fleet of microscopes will capture every neuron and every synapse in a vast database of images.\n",
      "\n",
      "向こうでは、180パーセントの税金をガソリン車にかけ 排気ガスゼロの車に対しては、税金をかけません\n",
      "They put 180 percent tax on gasoline cars and zero tax on zero-emission cars.\n",
      "\n",
      "自然な発達過程には不適切であり とりわけ少年に悪影響を与えます\n",
      "It's not developmentally appropriate, and it's particularly bad for boys.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_ja = []\n",
    "data_en = []\n",
    "for sample in ds:\n",
    "    ja = sample['translation']['ja'].decode()\n",
    "    en = sample['translation']['en'].decode()\n",
    "    data_ja.append(ja)\n",
    "    data_en.append(en)\n",
    "\n",
    "print('num of data:', len(data_ja), '\\n')\n",
    "for _ in range(5):\n",
    "    i = random.randint(0, len(data_ja))\n",
    "    print(data_ja[i])\n",
    "    print(data_en[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "書き出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile_ja = 'data/iwslt2017_ja.txt'\n",
    "with open(textfile_ja, 'w') as f:\n",
    "    f.write('\\n'.join(data_ja))\n",
    "\n",
    "textfile_en = 'data/iwslt2017_en.txt'\n",
    "with open(textfile_en, 'w') as f:\n",
    "    f.write('\\n'.join(data_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ数を減らしたものも作っておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 10000\n",
    "data_ja = data_ja[:n_data]\n",
    "data_en = data_en[:n_data]\n",
    "\n",
    "textfile_ja = f'data/iwslt2017_ja_{n_data}.txt'\n",
    "with open(textfile_ja, 'w') as f:\n",
    "    f.write('\\n'.join(data_ja))\n",
    "\n",
    "textfile_en = f'data/iwslt2017_en_{n_data}.txt'\n",
    "with open(textfile_en, 'w') as f:\n",
    "    f.write('\\n'.join(data_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークナイザの学習。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/iwslt2017_ja_10000.txt\n",
      "  input_format: \n",
      "  model_prefix: models/tokenizer_iwslt2017_ja_10000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 3\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: data/iwslt2017_ja_10000.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 10000 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=412475\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=2153\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 10000 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=137779\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 87856 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 10000\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 30607\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 30607 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=42933 obj=39.6967 num_tokens=124760 num_tokens/piece=2.90592\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=38770 obj=37.1193 num_tokens=125352 num_tokens/piece=3.23322\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=28979 obj=38.5973 num_tokens=134756 num_tokens/piece=4.65013\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=28841 obj=38.0819 num_tokens=134865 num_tokens/piece=4.67616\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=21607 obj=40.2975 num_tokens=147395 num_tokens/piece=6.82163\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=21575 obj=39.7714 num_tokens=147487 num_tokens/piece=6.83601\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=16176 obj=42.0136 num_tokens=161429 num_tokens/piece=9.97954\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16173 obj=41.526 num_tokens=161506 num_tokens/piece=9.98615\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=12129 obj=43.7905 num_tokens=174261 num_tokens/piece=14.3673\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=12127 obj=43.4057 num_tokens=174277 num_tokens/piece=14.371\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=9095 obj=45.5653 num_tokens=187595 num_tokens/piece=20.6262\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=9095 obj=45.1722 num_tokens=187614 num_tokens/piece=20.6283\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=45.4645 num_tokens=189196 num_tokens/piece=21.4995\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=45.422 num_tokens=189195 num_tokens/piece=21.4994\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: models/tokenizer_iwslt2017_ja_10000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: models/tokenizer_iwslt2017_ja_10000.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/iwslt2017_en_10000.txt\n",
      "  input_format: \n",
      "  model_prefix: models/tokenizer_iwslt2017_en_10000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 3\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: data/iwslt2017_en_10000.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 10000 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=969452\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9606% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999606\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 10000 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=492999\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 40504 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 10000\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 23479\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 23479 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=13852 obj=10.7231 num_tokens=47546 num_tokens/piece=3.43243\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=12100 obj=8.6129 num_tokens=47844 num_tokens/piece=3.95405\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=9075 obj=8.62923 num_tokens=51263 num_tokens/piece=5.64882\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=9065 obj=8.5931 num_tokens=51261 num_tokens/piece=5.65483\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=8.60184 num_tokens=51657 num_tokens/piece=5.87011\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8799 obj=8.59885 num_tokens=51656 num_tokens/piece=5.87067\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: models/tokenizer_iwslt2017_en_10000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: models/tokenizer_iwslt2017_en_10000.vocab\n"
     ]
    }
   ],
   "source": [
    "pad_id = 3\n",
    "vocab_size = 8000\n",
    "\n",
    "tokenizer_prefix_ja = f'models/tokenizer_iwslt2017_ja_{n_data}'\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_ja,\n",
    "    model_prefix=tokenizer_prefix_ja,\n",
    "    vocab_size=vocab_size,\n",
    "    pad_id=pad_id\n",
    ")\n",
    "\n",
    "tokenizer_prefix_en = f'models/tokenizer_iwslt2017_en_{n_data}'\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_en,\n",
    "    model_prefix=tokenizer_prefix_en,\n",
    "    vocab_size=vocab_size,\n",
    "    pad_id=pad_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理\n",
    "\n",
    "データ数を減らした方を使う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークナイザ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary (ja): 8000\n",
      "num of vocabrary (en): 8000\n"
     ]
    }
   ],
   "source": [
    "sp_ja = spm.SentencePieceProcessor(f'{tokenizer_prefix_ja}.model')\n",
    "sp_en = spm.SentencePieceProcessor(f'{tokenizer_prefix_en}.model')\n",
    "\n",
    "unk_id = sp_ja.unk_id()\n",
    "bos_id = sp_ja.bos_id()\n",
    "eos_id = sp_ja.eos_id()\n",
    "\n",
    "n_vocab_ja = len(sp_ja)\n",
    "n_vocab_en = len(sp_en)\n",
    "print('num of vocabrary (ja):', n_vocab_ja)\n",
    "print('num of vocabrary (en):', n_vocab_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids_ja = sp_ja.encode(data_ja)\n",
    "data_ids_en = sp_en.encode(data_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOS, EOSの追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids_ja, ids_en in zip(data_ids_ja, data_ids_en):\n",
    "    ids_en.insert(0, bos_id)\n",
    "    ids_ja.append(eos_id)\n",
    "    ids_en.append(eos_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データの作成\n",
    "\n",
    "入力文と正解のペアを作成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoderへの入力（入力文）とDecoderの出力（正解）のペアを作成する。  \n",
    "また、Decoderへの入力を考える必要がある。今回は教師強制を採用し、出力文の頭に\\<BOS>を付与したものをDecoderへの入力とする。\n",
    "\n",
    "例）\n",
    "Encoderへの入力（入力文） | Decoderへの入力 | Eecoderの出力（出力文）\n",
    "--- | --- | ---\n",
    "夏 休み が 終わり ました 。 \\<EOS> | \\<BOS> Summer vacation is over . | Summer vacation is over . \\<EOS>\n",
    "ツイッター は 亡くなり ました 。 \\<EOS> | \\<BOS> Twitter is dead . | Twitter is dead . \\<EOS>\n",
    "今日 から X で 暮らし ましょう 。 \\<EOS> | \\<BOS> Let 's live in X from today . | Let 's live in X from today . \\<EOS>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaderの作成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 68]), torch.Size([32, 68]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids_ja, data_ids_en):\n",
    "        self.data_ja = [torch.tensor(ids) for ids in data_ids_ja]\n",
    "        self.data_en = [torch.tensor(ids) for ids in data_ids_en]\n",
    "        self.n_data = len(self.data_ja)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ja = self.data_ja[idx]\n",
    "        en = self.data_en[idx]\n",
    "        x_enc = ja # encoderへの入力\n",
    "        x_dec = en[:-1] # decoderへの入力\n",
    "        y_dec = en[1:] # decoderの出力\n",
    "        return x_enc, x_dec, y_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "def collate_fn(batch): # padding\n",
    "    x_enc, x_dec, y_dec= zip(*batch)\n",
    "    x_enc = pad_sequence(x_enc, batch_first=True, padding_value=pad_id)\n",
    "    x_dec = pad_sequence(x_dec, batch_first=True, padding_value=pad_id)\n",
    "    y_dec = pad_sequence(y_dec, batch_first=True, padding_value=pad_id)\n",
    "    return x_enc, x_dec, y_dec\n",
    "\n",
    "dataset = TextDataset(data_ids_ja, data_ids_en)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "x_enc, x_dec, y_dec = next(iter(train_loader))\n",
    "x_enc.shape, x_dec.shape, y_dec.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築\n",
    "\n",
    "EncoderとDecoderを作る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "入力文を入れて隠れ状態を出力するだけのRNN(LSTM)。LSTMと線形層で作る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで1つ工夫を加える。普通に作っても動きはするが、学習が上手くいかない可能性がある。\n",
    "\n",
    "encoderへの入力はpaddingされたデータである。ここで、paddingされた範囲が多い=padトークンが多く含まれているデータは、padの数が多くなるにつれて、隠れ状態がある一定の値に収束してしまう。RNNに同じトークンを何度も入力することで隠れ状態が収束していってしまうのだ。  \n",
    "そうなってしまうと、入力文に依る隠れ状態の違いが少なくなり、翻訳を学習できない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そこで、encoderが出力する隠れ状態として、padトークンを除いた最後のトークン=EOSを入力した時点のものを使用する。  \n",
    "\n",
    "以上を踏まえ、encoderを以下のように実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        eos_positions = x == eos_id\n",
    "            # eosに対応する位置のみがTrueとなったTensor: (batch_size, seq_len)\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        hs, _ = self.lstm(x) # (batch_size, seq_len, hidden_size)\n",
    "        hs = self.dropout(hs)\n",
    "        h = hs[eos_positions].unsqueeze(0) # (1, batch_size, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Encoderから出力された隠れ状態を受け取り、出力文を生成するRNN。Encoder同様、LSTMと線形層で作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        hs, (h, c) = self.lstm(x, hc)\n",
    "            # hs: (batch_size, seq_len, hidden_size)\n",
    "            # h: (1, batch_size, hidden_size)\n",
    "            # c: (1, batch_size, hidden_size)\n",
    "        hs = self.dropout(hs)\n",
    "        y = self.fc(hs) # (batch_size, seq_len, n_vocab)\n",
    "        return y, (h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "EncoderとDecoderを合わせて、入力から出力までの一連の処理を行うモデルを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        h = self.encoder(x_enc)\n",
    "        hc = (h, torch.zeros_like(h))\n",
    "        y, _ = self.decoder(x_dec, hc)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "def loss_fn(y, t):\n",
    "    loss = cross_entropy(y.reshape(-1, n_vocab_ja), t.ravel())\n",
    "    return loss\n",
    "\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x_enc, x_dec, y_dec in test_loader:\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            losses.append(loss.item())\n",
    "    loss = sum(losses) / len(losses)\n",
    "    return loss\n",
    "\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    prog.start(n_iter=len(train_loader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for x_enc, x_dec, y_dec in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prog.update(loss.item())\n",
    "\n",
    "        if prog.now_epoch % prog_unit == 0:\n",
    "            test_loss = eval_model(model)\n",
    "            prog.memo(f'test: {test_loss:.5f}', no_step=True)\n",
    "        prog.memo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size, embed_size = 512, 512\n",
    "encoder = Encoder(n_vocab_ja, embed_size, hidden_size)\n",
    "decoder = Decoder(n_vocab_en, embed_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1-10/100: ############################## 100% [00:01:06.61] loss train: 5.51102, test: 5.15894 \n",
      "  11-20/100: ############################## 100% [00:01:20.69] loss train: 4.72939, test: 4.96114 \n",
      "  21-30/100: ############################## 100% [00:01:13.05] loss train: 4.28014, test: 4.96832 \n",
      "  31-40/100: ############################## 100% [00:01:23.97] loss train: 3.89920, test: 5.08351 \n",
      "  41-50/100: ############################## 100% [00:01:24.39] loss train: 3.55994, test: 5.24198 \n",
      "  51-60/100: ############################## 100% [00:01:17.29] loss train: 3.25648, test: 5.43300 \n",
      "  61-70/100: #                                4% [00:00:03.33] loss train: 3.08988               "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/komiya/private/study/language-models/seq2seq.ipynb セル 40\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/komiya/private/study/language-models/seq2seq.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train(model, optimizer, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, prog_unit\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/home/komiya/private/study/language-models/seq2seq.ipynb セル 40\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/komiya/private/study/language-models/seq2seq.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m y \u001b[39m=\u001b[39m model(x_enc, x_dec)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/komiya/private/study/language-models/seq2seq.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y, y_dec)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/komiya/private/study/language-models/seq2seq.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/komiya/private/study/language-models/seq2seq.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/komiya/private/study/language-models/seq2seq.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m prog\u001b[39m.\u001b[39mupdate(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/nlp/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/nlp/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, optimizer, n_epochs=100, prog_unit=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 翻訳\n",
    "\n",
    "作成したモデルに日本語文を入力し、英語に翻訳して出力する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_sampling(y, decisive=True):\n",
    "    y.squeeze_(0)\n",
    "    if decisive:\n",
    "        token = y.argmax().item()\n",
    "    else:\n",
    "        y[unk_id] = -torch.inf\n",
    "        probs = F.softmax(y, dim=-1)\n",
    "        token, = random.choices(range(n_vocab_en), weights=probs)\n",
    "    return token\n",
    "\n",
    "\n",
    "bos_id = sp_en.bos_id()\n",
    "eos_id = sp_en.eos_id()\n",
    "@torch.no_grad()\n",
    "def translate(\n",
    "    model: nn.Module,\n",
    "    in_text: str, # 入力文（日本語）\n",
    "    max_len: int = 100, # 出力のトークン数の上限\n",
    "    decisive: bool = True, # サンプリングを決定的にするか\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    in_ids = sp_ja.encode(in_text)\n",
    "    in_ids = torch.tensor(in_ids + [eos_id], device=device)\n",
    "\n",
    "    h = model.encoder(in_ids)\n",
    "    hc = (h, torch.zeros_like(h))\n",
    "    next_token = bos_id\n",
    "\n",
    "    token_ids = []\n",
    "    while len(token_ids) <= max_len and next_token != eos_id:\n",
    "        x = torch.tensor([[next_token]], device=device)\n",
    "        y, hc = model.decoder(x, hc)\n",
    "        next_token = token_sampling(y, decisive)\n",
    "        token_ids.append(next_token)\n",
    "\n",
    "    sentence = sp_en.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは訓練データから。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 彼らがバーミンガムの 少年少女十字軍について テレビで見たり聞いたりするのは 私達が1863年のリンカーンを 映画で観るのと似ています 歴史なのです\n",
      "output: And so for them, when they hear about the Children's Crusade in Birmingham, in many ways, if they see it on TV, it's like our looking at the 1863 \"Lincoln\" movie: It's history.\n",
      "answer: And so for them, when they hear about the Children's Crusade in Birmingham, in many ways, if they see it on TV, it's like our looking at the 1863 \"Lincoln\" movie: It's history.\n",
      "\n",
      "input: 一番大切なのは menschであること」 どうもありがとうございました\n",
      "output: The most important thing is to do it: not all the time right now.\n",
      "answer: The most important thing is to be a mensch.\" Thank you.\n",
      "\n",
      "input: 食べ物みたいに直接的対象ではありません 子供や恋人というのも\n",
      "output: It would hardly do to eat your baby or your lover.\n",
      "answer: It would hardly do to eat your baby or your lover.\n",
      "\n",
      "input: ありがとうございました\n",
      "output: Thank you.\n",
      "answer: Thank you.\n",
      "\n",
      "input: そう考えると ジャーナリストになった自分も含めた皆が 問題の一部であると 私は気づきました\n",
      "output: It was when I became a journalist that I really realized how I was part of this problem, and how was important this.\n",
      "answer: It was when I became a journalist that I really realized how I was part of this problem, and how we all are part of this problem.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(train_dataset))\n",
    "    x, _, t = train_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print('input:', x)\n",
    "    print('output:', translate(model, x))\n",
    "    print('answer:', t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一致。\n",
    "\n",
    "訓練データに含まれていないものも試してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: このような指数的成長の 一貫性は\n",
      "output: It's a real delicate machine running for example.\n",
      "answer: It's pretty remarkable how smooth an exponential process that is.\n",
      "\n",
      "input: 本日は 2つの プロジェクトを紹介します まずは最初のプロジェクトです\n",
      "output: This Syrian young man survived one of these boats an up ⁇ \n",
      "answer: I'm going to share with you two projects that are investigations along these lines, and we'll start with this one.\n",
      "\n",
      "input: 何がすごいかと言うと アメリカで公民権が獲得されたのは 60年代だったのです\n",
      "output: So, this is the website of an indignity the machineary political world, and you know what's about data.\n",
      "answer: Now, what is remarkable is that civil rights in America were achieved in the 1960s.\n",
      "\n",
      "input: 私たちは地球への負担をより軽くする 術を学んでいくだろうと 確信しています 新しいデジタルツールによって起こることが とても大規模で非常に有益で それ以前のすべてのものが お笑い草になるほどのものだと\n",
      "output: And if you look at honeybees, and I have GPS -- after I'm sure some of you have in the same way to show you pretty much more'tly doing, but supportive for one time, I think most of the best definition good is better good. The good life is complexly in life.\n",
      "answer: I'm very confident we're going to learn to live more lightly on the planet, and I am extremely confident that what we're going to do with our new digital tools is going to be so profound and so beneficial that it's going to make a mockery out of everything that came before.\n",
      "\n",
      "input: 顔にはあらゆる感覚が集中しています 特別な感覚― 視覚 聴覚 嗅覚 味覚 そして言葉を発する口があります\n",
      "output: Imagine putting together, as a consumer is now, total strangers and not only do this little thing.\n",
      "answer: It's where our senses are located, our special senses -- our vision, our speech, our hearing, our smell, our taste.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(test_dataset))\n",
    "    x, _, t = test_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print('input:', x)\n",
    "    print('output:', translate(model, x))\n",
    "    print('answer:', t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ありがとう。\n",
      "output: Thank you.\n",
      "\n",
      "input: 猫はかわいいね。\n",
      "output: But it's not a very simple solution. The story with Ry.\n",
      "\n",
      "input: 上手く文章が書けるようになりました。\n",
      "output: They were built to transmit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "sentences = [\n",
    "    'ありがとう。',\n",
    "    '猫はかわいいね。',\n",
    "    '上手く文章が書けるようになりました。'\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print('input:', sentence)\n",
    "    print('output:', translate(model, sentence))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "びみょう。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
