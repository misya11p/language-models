{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext import transforms\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchvision.transforms import Compose\n",
    "import MeCab\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 5304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>japanese</th>\n",
       "      <th>english</th>\n",
       "      <th>chinese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#0001</td>\n",
       "      <td>Xではないかとつくづく疑問に思う</td>\n",
       "      <td>I often wonder if it might be X.</td>\n",
       "      <td>难道不会是X吗，我实在是感到怀疑。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#0002</td>\n",
       "      <td>Xがいいなといつも思います</td>\n",
       "      <td>I always think X would be nice.</td>\n",
       "      <td>我总觉得X不错。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#0003</td>\n",
       "      <td>それがあるようにいつも思います</td>\n",
       "      <td>It always seems like it is there.</td>\n",
       "      <td>我总觉得那好像是有的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#0004</td>\n",
       "      <td>それが多すぎないかと正直思う</td>\n",
       "      <td>I honestly feel like there is too much.</td>\n",
       "      <td>老实说我觉得那太多了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#0005</td>\n",
       "      <td>山田はみんなに好かれるタイプの人だと思う</td>\n",
       "      <td>I think that Yamada is the type everybody likes.</td>\n",
       "      <td>我想山田是受大家欢迎的那种人。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id              japanese   \n",
       "0  #0001      Xではないかとつくづく疑問に思う  \\\n",
       "1  #0002         Xがいいなといつも思います   \n",
       "2  #0003       それがあるようにいつも思います   \n",
       "3  #0004        それが多すぎないかと正直思う   \n",
       "4  #0005  山田はみんなに好かれるタイプの人だと思う   \n",
       "\n",
       "                                            english            chinese  \n",
       "0                  I often wonder if it might be X.  难道不会是X吗，我实在是感到怀疑。  \n",
       "1                   I always think X would be nice.           我总觉得X不错。  \n",
       "2                 It always seems like it is there.        我总觉得那好像是有的。  \n",
       "3           I honestly feel like there is too much.        老实说我觉得那太多了。  \n",
       "4  I think that Yamada is the type everybody likes.    我想山田是受大家欢迎的那种人。  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('data/JEC_basic_sentence_v1-3.xls', header=None)\n",
    "df.columns = ['id', 'japanese', 'english', 'chinese']\n",
    "print('num of data:', len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger('-Owakati')\n",
    "def tokenize(data: List[str], l='en') -> List[List[str]]:\n",
    "    if l == 'ja':\n",
    "        return [tagger.parse(sentence).split() for sentence in data]\n",
    "    elif l == 'en':\n",
    "        return [sentence.replace('.', ' .').lower().split() for sentence in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ja = tokenize(df['japanese'], l='ja')\n",
    "text_en = tokenize(df['english'], l='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad, bos, eos, unk = '<pad>', '<bos>', '<eos>', '<unk>'\n",
    "max_len = 30\n",
    "specials = [pad, bos, eos, unk]\n",
    "vocab_ja = build_vocab_from_iterator(text_ja, specials=specials)\n",
    "vocab_en = build_vocab_from_iterator(text_en, specials=specials)\n",
    "\n",
    "transform_ja = Compose([\n",
    "    transforms.Truncate(max_len-2),\n",
    "    transforms.AddToken(bos, begin=True),\n",
    "    transforms.AddToken(eos, begin=False),\n",
    "    transforms.VocabTransform(vocab_ja),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.PadTransform(max_len, vocab_ja[pad])\n",
    "])\n",
    "\n",
    "transform_en = Compose([\n",
    "    transforms.Truncate(max_len-2),\n",
    "    transforms.AddToken(bos, begin=True),\n",
    "    transforms.AddToken(eos, begin=False),\n",
    "    transforms.VocabTransform(vocab_en),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.PadTransform(max_len, vocab_en[pad])\n",
    "])\n",
    "\n",
    "n_vocab_ja = len(vocab_ja)\n",
    "n_vocab_en = len(vocab_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_enc: torch.Size([32, 30])\n",
      "x_dec: torch.Size([32, 29])\n",
      "label: torch.Size([32, 29])\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, in_text, out_text, in_transform, out_transform):\n",
    "        self.in_text = in_text\n",
    "        self.out_text = out_text\n",
    "        self.in_transform = in_transform\n",
    "        self.out_transform = out_transform\n",
    "        self.n_samples = len(in_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        in_text = self.in_text[index]\n",
    "        out_text = self.out_text[index]\n",
    "        in_text = self.in_transform(in_text)\n",
    "        out_text = self.out_transform(out_text)\n",
    "        return in_text, out_text[:-1], out_text[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "dataset = TextDataset(text_ja, text_en, transform_ja, transform_en)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "x_enc, x_dec, label = next(iter(dataloader))\n",
    "print('x_enc:', x_enc.shape)\n",
    "print('x_dec:', x_dec.shape)\n",
    "print('label:', label.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築\n",
    "\n",
    "最初LSTMで作ったが一旦RNNにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        # self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h = self.rnn(x)\n",
    "        h = self.fc(h)\n",
    "        # _, h = self.lstm(x)\n",
    "        # h = self.fc(h[0][0])\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        # self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        x = self.embedding(x)\n",
    "        y, h = self.rnn(x, h)\n",
    "        # y, h = self.lstm(x, h)\n",
    "        y = self.fc(y)\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, n_in_vocab, n_out_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(n_in_vocab, embed_size, hidden_size)\n",
    "        self.decoder = Decoder(n_out_vocab, embed_size, hidden_size)\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        h = self.encoder(x_enc)\n",
    "        y, _ = self.decoder(x_dec, h)\n",
    "        # h0 = h.unsqueeze(0)\n",
    "        # c0 = torch.zeros_like(h0)\n",
    "        # y, _ = self.decoder(x_dec, (h0, c0))\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_en[pad])\n",
    "def train(model, optimizer, n_epochs):\n",
    "    eval_tokens = []\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        for x_enc, x_dec, label in tqdm(dataloader, desc=f'{epoch}/{n_epochs}', disable=True):\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            label = label.to(device)\n",
    "            y_pred = model(x_enc, x_dec)\n",
    "            loss = criterion(y_pred.reshape(-1, y_pred.shape[-1]), label.ravel())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        eval_tokens.append((y_pred, label))\n",
    "        print(f'{epoch}/{n_epochs} loss: {epoch_loss/len(dataloader)}', flush=True)\n",
    "    return eval_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(len(vocab_ja), len(vocab_en), 1024, 1024).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval_tokensについては後程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20 loss: 5.436596850314772\n",
      "2/20 loss: 4.307769970721509\n",
      "3/20 loss: 3.52607438219599\n",
      "4/20 loss: 2.8977347698556373\n",
      "5/20 loss: 2.405354056013636\n",
      "6/20 loss: 2.037142706922738\n",
      "7/20 loss: 1.759437583297132\n",
      "8/20 loss: 1.5826276310955185\n",
      "9/20 loss: 1.4604639554598244\n",
      "10/20 loss: 1.395873271557222\n",
      "11/20 loss: 1.351504603064204\n",
      "12/20 loss: 1.3370569161621921\n",
      "13/20 loss: 1.3281421690102082\n",
      "14/20 loss: 1.322664046143911\n",
      "15/20 loss: 1.3227734738085644\n",
      "16/20 loss: 1.3196085526282528\n",
      "17/20 loss: 1.3183850365948964\n",
      "18/20 loss: 1.3180801940251545\n",
      "19/20 loss: 1.3127760104386204\n",
      "20/20 loss: 1.3138846130256194\n"
     ]
    }
   ],
   "source": [
    "eval_tokens = train(model, optimizer, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 翻訳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def translate(model, text, max_len=100):\n",
    "    model.eval()\n",
    "    tokens = tokenize([text], 'ja')[0]\n",
    "    tokens = transform_ja(tokens).unsqueeze(0).to(device)\n",
    "    h = model.encoder(tokens)\n",
    "    # h0 = h.unsqueeze(0)\n",
    "    # c0 = torch.zeros_like(h0)\n",
    "    # h = (h0, c0)\n",
    "    next_token = vocab_en[bos]\n",
    "\n",
    "    tokens = []\n",
    "    for _ in range(max_len):\n",
    "        next_token = torch.tensor(next_token).reshape(1, 1).to(device)\n",
    "        y, h = model.decoder(next_token, h)\n",
    "        y = F.softmax(y.ravel(), dim=0)\n",
    "        # next_token = random.choices(range(len(y)), weights=y)[0] # 確率的な出力\n",
    "        next_token = y.argmax().item() # 決定的な出力\n",
    "        if next_token == vocab_en[eos]:\n",
    "            break\n",
    "        tokens.append(next_token)\n",
    "    return ' '.join([vocab_en.get_itos()[t] for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he will access a site with unexpurgated videos of cute girls .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, 'Xではないかとつくづく疑問に思う')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he will access a site with unexpurgated videos of cute girls .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, '山田はみんなに好かれるタイプの人だと思う')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lossはいい感じに減っているが、翻訳は上手くいっていない。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## メモ\n",
    "\n",
    "なぜ翻訳が上手くいかないか。\n",
    "\n",
    "### 考察\n",
    "\n",
    "RNN系は学習時と推論時で挙動が異なる。具体的には（decoderへの）入力が異なり、学習時は正解データを入力するが、推論時は推論結果（前の時間でモデルが出力した単語）を入力する。これが原因で、lossは低いが推論は上手くいかないのではないか。\n",
    "\n",
    "とりあえず、調べてみる。学習時にどの様な文章が生成されているのかを調べる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成: he will remembered his time in worked as a salesperson . <eos> decision-making . . . . . . . weeks weeks weeks reissued \"boob \"boob \"boob purchase, purchase,\n",
      "正解: he suddenly recalled the time he worked as a salesperson . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he hall of energetic live performances . <eos> 〜 . . . . . . . weeks weeks weeks counter, \"boob \"boob \"boob purchase, purchase, purchase, purchase, wondering wondering\n",
      "正解: the two give energetic live performances . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he will well of several styles . <eos> decision-making . . . . . . . weeks weeks weeks counter, \"boob \"boob \"boob purchase, purchase, purchase, purchase, wondering wondering\n",
      "正解: he performs music of several styles . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he is known when atp is released from the enzyme . <eos> in . . . . . . . weeks weeks weeks counter, reissued \"boob \"boob purchase, purchase,\n",
      "正解: energy is used when atp is released from the enzyme . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he hall equipment is more body-friendly . <eos> needed . . . . . . . weeks weeks weeks counter, \"boob \"boob \"boob purchase, purchase, purchase, purchase, wondering wondering\n",
      "正解: the new equipment is more body-friendly . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he people brought what you want to know . <eos> in . . . . . . . weeks weeks weeks reissued \"boob \"boob \"boob purchase, purchase, purchase, purchase,\n",
      "正解: tell people clearly what you want to know . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成: he will access up the shop . <eos> rights . . . . . . . weeks weeks counter, reissued \"boob \"boob \"boob purchase, purchase, purchase, purchase, wondering wondering\n",
      "正解: he will step into the shop . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he provide with environmental regulations and other rules . <eos> . . . . . . . . weeks weeks weeks counter, \"boob \"boob purchase, purchase, purchase, purchase, wondering\n",
      "正解: we comply with laws, regulations and other rules . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he can definitely a just by writing . <eos> and . . . . . . . weeks weeks weeks reissued \"boob \"boob \"boob purchase, purchase, purchase, purchase, wondering\n",
      "正解: you will get rewards just by writing . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he will yellow give our own piece together . <eos> . . . . . . . . weeks weeks weeks counter, \"boob \"boob \"boob purchase, purchase, purchase, purchase,\n",
      "正解: he and i made our own piece together . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "n_data = 10\n",
    "y, t = eval_tokens[n_epochs - 1]\n",
    "y = y.argmax(dim=-1)[:n_data]\n",
    "t = t[:n_data]\n",
    "\n",
    "for yi, ti in zip(y, t):\n",
    "    print('生成:', ' '.join([vocab_en.get_itos()[i] for i in yi]))\n",
    "    print('正解:', ' '.join([vocab_en.get_itos()[i] for i in ti]))\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20epoch時点での最後のバッチの生成結果。  \n",
    "これを見ると、毎回'he'から始まっていることが分かる。ただ、そこからいくつか単語が進むと正解と同じ文章が生成されるようになる。これはモデルが正解の単語を入力として受け取っているからである。推論時は間違った'he'という単語をそのまま受け取るので'he'に続く尤もらしい単語列を生成する。\n",
    "\n",
    "現状、どんな隠れ状態を受け取っても'he'が出力される。これは学習をやり直しても変わらない。なぜ必ず'he'が出力されるかというと、データセット内の初めの単語に'he'が多いから。多分。’\\<bos>'の次の単語は'he'である確率が最も高く、上記の結果は`argmax()`によって確定的に単語を出力しているため、全て'he'となる。\n",
    "\n",
    "対策として、隠れ状態によって初めの単語が変化するようにしなければならない。どうすればいいかな"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配消失"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoderが適切な隠れ状態を出力できていない場合、decoderがencoderへの入力を考慮しないモデルになってしまう気がする。ただの言語モデルと変わらなくなる。だから'\\<bos>'の次に'he'がくる。データセット内ではその確率が最も高いから。\n",
    "\n",
    "encoderがしっかり学習できているかを確かめるため、重みを見てみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(len(vocab_ja), len(vocab_en), 1024, 1024).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2442, -0.5861, -0.4899,  ..., -0.8273,  0.1949,  0.5169],\n",
       "        [ 0.1649, -0.4432, -0.6046,  ..., -0.3246,  0.7523, -0.5198],\n",
       "        [ 0.0740,  1.0601,  0.4548,  ..., -0.6372, -1.9226,  1.2581],\n",
       "        ...,\n",
       "        [ 0.0293,  0.7944,  1.9904,  ...,  0.7912,  0.4629, -0.9529],\n",
       "        [-0.6784, -0.2369, -0.9609,  ..., -0.6254, -0.5470, -0.2561],\n",
       "        [ 0.7684,  0.1238, -0.8585,  ...,  2.5816, -1.8934, -0.9014]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 loss: 5.402651743716504\n",
      "2/5 loss: 4.281170398355966\n",
      "3/5 loss: 3.484377849532897\n",
      "4/5 loss: 2.851475468601089\n",
      "5/5 loss: 2.370304086122168\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2354, -0.5864, -0.4949,  ..., -0.8267,  0.1834,  0.5068],\n",
       "        [ 0.1649, -0.4432, -0.6046,  ..., -0.3246,  0.7523, -0.5199],\n",
       "        [ 0.1061,  1.0941,  0.4907,  ..., -0.6067, -1.9590,  1.2263],\n",
       "        ...,\n",
       "        [ 0.0293,  0.7944,  1.9904,  ...,  0.7912,  0.4629, -0.9529],\n",
       "        [-0.6784, -0.2369, -0.9609,  ..., -0.6254, -0.5470, -0.2561],\n",
       "        [ 0.7684,  0.1238, -0.8585,  ...,  2.5816, -1.8934, -0.9014]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.embedding.weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5epoch学習させたが、ほとんど変わっていない。全く変わっていないわけではないので、勾配は行き届いている。ただ勾配が小さすぎるだけ。勾配消失というやつ。\n",
    "\n",
    "勾配を見てみると、こんな感じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7154e-09, -5.8014e-09,  1.7624e-09,  ..., -4.5326e-09,\n",
       "          1.3269e-09,  2.0343e-09],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-1.2370e-32, -5.0238e-32, -1.0095e-31,  ..., -1.6434e-31,\n",
       "          1.2991e-31,  6.2421e-32],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.embedding.weight.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ほぼ0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
