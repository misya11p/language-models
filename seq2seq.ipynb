{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "\n",
    "*Sequence to Sequence*.  \n",
    "*Encoder-Decoder Model*とも。\n",
    "\n",
    "文章を入力とし、文章を出力するモデル。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "from glob import glob\n",
    "import os\n",
    "from typing import List\n",
    "import random\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 発想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで、RNNを用いて、入力した単語列に続く単語を予測するモデルを作成し、単語の予測を繰り返すことで文章を生成した。\n",
    "\n",
    "RNNは時系列の情報を保持するために隠れ状態$h_t$を用いる。隠れ状態の初期値$h_0$は0ベクトルとしているが、ここで、何らかの入力データから生成したベクトルを$h_0$として用いることを考える。このとき、上手く学習させれば、その入力に基づいた文章を生成できそう。\n",
    "\n",
    "例えば、入力を画像をとし、CNNを用いて抽出した特徴量を$h_0$として用いるようにすれば、入力画像に基づいた文章が生成できる。画像のキャプションなどが例に挙げられる。  \n",
    "学習方法は簡単で、入力画像に対して適切な文章が出力されるように学習させるだけ。隠れ状態を通じてRNNからCNNまで逆伝播を繋げる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、入力に文章を用いることはできないだろうか。RNNに文章を入力し、最後に出力された隠れ状態を文章ベクトルとする。これを別のRNNへの入力$h_0$とすれば、入力文に基づいた文章生成が可能になる。  \n",
    "\n",
    "この発想は翻訳タスクに大きく役立つ。入力と出力に同じ意味を持った異なる言語の文章を設定すれば、入力文と同じ意味を持った文章生成が可能になる。  \n",
    "こういったシーケンスをシーケンスに変換するモデルを *seq2seq (Sequence to Sequence)* モデルと呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章では、入力に日本語文、出力に入力と同じ意味を持つ英語文を設定し、日本語→英語の翻訳を行うモデルを作成する。\n",
    "\n",
    "このモデルは以下の二つのRNNから構成される。\n",
    "- ***Encoder***: 文章ベクトルを生成するRNN\n",
    "- ***Decoder***: 文章ベクトルを受け取って出力文を生成するRNN\n",
    "\n",
    "このことから***Encoder-Decoderモデル***とも呼ばれる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## データセット\n",
    "\n",
    "翻訳モデルを作るには、同じ意味を持つ文章が複数の言語でまとまっているデータが必要。このようなデータは対訳コーパスと呼んだりする。\n",
    "\n",
    "本章では以下のデータセットを使用する。\n",
    "- [Wikipedia日英京都関連文書対訳コーパス](https://alaginrc.nict.go.jp/WikiCorpus/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en: All the universe is substantial and has all the nine factors at its onset.\n",
      "ja: 諸法の生ずる時は、体及び余の法は凡て九事有り。\n",
      "\n",
      "en: In his later years he got acquainted with Shingoro TAKENO, a rich merchant in Sakai, and while Sanetaka taught TAKENO the study of poetry and waka poems, TAKENO gave him a lot of financial support.\n",
      "ja: 晩年には堺の富商である武野新五郎と知己となり、実隆が武野に歌学や和歌を指導する一方、武野からは少なからず経済的援助を受けていた。\n",
      "\n",
      "en: Himorogi in Koshinto considered shiniki as a place for a god to stay, or a border to the other world and this world, and was feared and respected.\n",
      "ja: 古神道において神籬は、神の宿る場所としての神域、または常世（とこよ）と現世（うつしよ）の端境と考えれ、恐れ敬った。\n",
      "\n",
      "en: Some people say that this is the first appearance of present seishu (refined sake), but it is controversial because of the following.\n",
      "ja: これを以て現在の清酒（せいしゅ）の初見とみなす説があるが、それは以下のように議論の分かれるところである。\n",
      "\n",
      "en: After appointment to Shogoinoge (Senior Fifth Rank, Lower Grade) the next year, he became Kurodo (Chamberlain) as well as Bicchu gon no suke (acting assistant governor of Bicchu Province) in 1168 and resigned Togu gakushi.\n",
      "ja: 翌年正五位下に叙され、仁安3年（1168年）には蔵人兼備中権介となり東宮学士を辞した。\n",
      "\n",
      "num of data: 443596\n"
     ]
    }
   ],
   "source": [
    "data_ja, data_en = [], []\n",
    "root_dir = 'data/jaen-kyoto/'\n",
    "for xml_file in glob(os.path.join(root_dir, '*/*.xml')):\n",
    "    try:\n",
    "        tree = ET.parse(xml_file)\n",
    "    except ET.ParseError:\n",
    "        continue\n",
    "    root = tree.getroot()\n",
    "    for sentence in root.iter('sen'):\n",
    "        ja = sentence.find('j').text\n",
    "        en = sentence.findall('e')[-1].text\n",
    "        if ja and en:\n",
    "            data_ja.append(ja)\n",
    "            data_en.append(en)\n",
    "\n",
    "# examples\n",
    "for _ in range(5):\n",
    "    i = random.randint(0, len(data_en))\n",
    "    print('en:', data_en[i])\n",
    "    print('ja:', data_ja[i])\n",
    "    print()\n",
    "\n",
    "n_data = len(data_en)\n",
    "print('num of data:', n_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多すぎるので減らす。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 10000\n",
    "data_ja = data_ja[:n_data]\n",
    "data_en = data_en[:n_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "書き出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile_ja = 'data/kyoto_ja.txt'\n",
    "with open(textfile_ja, 'w') as f:\n",
    "    f.write('\\n'.join(data_ja))\n",
    "\n",
    "textfile_en = 'data/kyoto_en.txt'\n",
    "with open(textfile_en, 'w') as f:\n",
    "    f.write('\\n'.join(data_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークナイザの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = 3\n",
    "\n",
    "vocab_size_ja = 8000\n",
    "tokenizer_prefix_ja = 'models/tokenizer_kyoto_ja'\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_ja, # データセット\n",
    "    model_prefix=tokenizer_prefix_ja,\n",
    "    vocab_size=vocab_size_ja,\n",
    "    pad_id=pad_id\n",
    ")\n",
    "\n",
    "vocab_size_en = 8000\n",
    "tokenizer_prefix_en = 'models/tokenizer_kyoto_en'\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_en, # データセット\n",
    "    model_prefix=tokenizer_prefix_en,\n",
    "    vocab_size=vocab_size_en,\n",
    "    pad_id=pad_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary (ja): 8000\n",
      "num of vocabrary (en): 8000\n"
     ]
    }
   ],
   "source": [
    "sp_ja = spm.SentencePieceProcessor(f'{tokenizer_prefix_ja}.model')\n",
    "sp_en = spm.SentencePieceProcessor(f'{tokenizer_prefix_en}.model')\n",
    "n_vocab_ja = len(sp_ja)\n",
    "n_vocab_en = len(sp_en)\n",
    "print('num of vocabrary (ja):', n_vocab_ja)\n",
    "print('num of vocabrary (en):', n_vocab_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids_ja = sp_ja.encode(data_ja)\n",
    "data_ids_en = sp_en.encode(data_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOS, EOSの追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_id = sp_ja.bos_id()\n",
    "eos_id = sp_ja.eos_id()\n",
    "for ids_ja, ids_en in zip(data_ids_ja, data_ids_en):\n",
    "    ids_en.insert(0, bos_id)\n",
    "    ids_ja.append(eos_id)\n",
    "    ids_en.append(eos_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習データ\n",
    "\n",
    "入力文と正解のペアを作成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoderへの入力（入力文）とDecoderの出力（正解）のペアを作成する。  \n",
    "また、Decoderへの入力を考える必要がある。今回は教師強制を採用し、出力文の頭に\\<BOS>を付与したものをDecoderへの入力とする。\n",
    "\n",
    "例）\n",
    "Encoderへの入力（入力文） | Decoderへの入力 | Eecoderの出力（出力文）\n",
    "--- | --- | ---\n",
    "夏 休み が 終わり ました 。 \\<EOS> | \\<BOS> Summer vacation is over . | Summer vacation is over . \\<EOS>\n",
    "ツイッター は 亡くなり ました 。 \\<EOS> | \\<BOS> Twitter is dead . | Twitter is dead . \\<EOS>\n",
    "今日 から X で 暮らし ましょう 。 \\<EOS> | \\<BOS> Let 's live in X from today . | Let 's live in X from today . \\<EOS>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaderを作成する。  \n",
    "**まずバッチサイズ1で進める。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   4,   26, 5721,  699,   27,    8,  200, 7995,   11,    6,   16,  738,\n",
       "             8, 3041, 2198,  635,   12,   26, 5687,   14,  775, 7653,   27,    6,\n",
       "           135,  379,   12,   26, 5687,   14,  775,   14,  783, 2720, 1128,  552,\n",
       "           719, 1929, 2833,  824,  597,   12,  989,    7,    5,    2]]),\n",
       " tensor([[   1,   21,  449,  146,    7,   43, 5001,  500,   31,   15,   43, 3651,\n",
       "           356, 2707, 4092,  220,  144,    4, 5172, 1955,   69,   20,    4,  225,\n",
       "           715,    5,    8,   43, 3651,  356, 1310,  500,   31,   23,    4, 4513,\n",
       "             7,    4,  281,   12, 3346,   45,  952,   38,   46,  194, 3711,    9,\n",
       "           511, 2369,   95,  270,   54]]),\n",
       " tensor([[  21,  449,  146,    7,   43, 5001,  500,   31,   15,   43, 3651,  356,\n",
       "          2707, 4092,  220,  144,    4, 5172, 1955,   69,   20,    4,  225,  715,\n",
       "             5,    8,   43, 3651,  356, 1310,  500,   31,   23,    4, 4513,    7,\n",
       "             4,  281,   12, 3346,   45,  952,   38,   46,  194, 3711,    9,  511,\n",
       "          2369,   95,  270,   54,    2]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids_ja, data_ids_en):\n",
    "        self.data_ja = [torch.tensor(ids) for ids in data_ids_ja]\n",
    "        self.data_en = [torch.tensor(ids) for ids in data_ids_en]\n",
    "        self.n_data = len(self.data_ja)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ja = self.data_ja[idx]\n",
    "        en = self.data_en[idx]\n",
    "        x_enc = ja # encoderへの入力\n",
    "        x_dec = en[:-1] # decoderへの入力\n",
    "        y_dec = en[1:] # decoderの出力\n",
    "        return x_enc, x_dec, y_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "batch_size = 1\n",
    "dataset = TextDataset(data_ids_ja, data_ids_en)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "x_enc, x_dec, y_dec = next(iter(dataloader))\n",
    "x_enc, x_dec, y_dec # example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築\n",
    "\n",
    "EncoderとDecoderを作る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "入力文を入れて隠れ状態を出力するだけのRNN。RNN層と線形層で作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        _, h = self.rnn(x) # h: (1, batch_size, hidden_size)\n",
    "        h = self.fc(h) # (1, batch_size, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Encoderから出力された隠れ状態を受け取り、出力文を生成するRNN。Encoder同様、RNN層と線形層で作る。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        x = self.embedding(x) # (seq_len, embed_size)\n",
    "        y, h = self.rnn(x, h) # y: (seq_len, hidden_size), h: (1, hidden_size)\n",
    "        y = self.fc(y) # (seq_len, n_vocab)\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "EncoderとDecoderを合わせて、入力から出力までの一連の処理を行うモデルを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        h = self.encoder(x_enc)\n",
    "        y, _ = self.decoder(x_dec, h)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, n_epochs, prog_unit=1):\n",
    "    model.train()\n",
    "    prog.start(n_iter=len(dataloader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        for x_enc, x_dec, y_dec in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "\n",
    "            y_pred = model(x_enc, x_dec)\n",
    "            loss = criterion(y_pred.reshape(-1, n_vocab_ja), y_dec.ravel())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prog.update(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size, embed_size = 1024, 1024\n",
    "encoder = Encoder(n_vocab_ja, embed_size, hidden_size)\n",
    "decoder = Decoder(n_vocab_en, embed_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦動くことが確認できれば良いので、エポック数1で学習させる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1: ##                                         6% [00:00:08.58] loss: 6.49081 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, optimizer, criterion, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, n_epochs, prog_unit)\u001b[0m\n\u001b[1;32m     13\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 15\u001b[0m prog\u001b[39m.\u001b[39mupdate(loss\u001b[39m.\u001b[39;49mitem())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ミニバッチ学習\n",
    "\n",
    "ミニバッチ学習に対応させる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずはRMMLMのときのようにバッチ内のサイズが揃うようにpaddingをする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch): # padding\n",
    "    x_enc, x_dec, y_dec= zip(*batch)\n",
    "    x_enc = pad_sequence(x_enc, batch_first=True, padding_value=pad_id)\n",
    "    x_dec = pad_sequence(x_dec, batch_first=True, padding_value=pad_id)\n",
    "    y_dec = pad_sequence(y_dec, batch_first=True, padding_value=pad_id)\n",
    "    return x_enc, x_dec, y_dec\n",
    "\n",
    "batch_size = 32\n",
    "dataset = TextDataset(data_ids_ja, data_ids_en)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数はpadトークンを無視するように変更する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、1つ工夫を加える。このままでも動きはするが、学習が上手くいかない可能性がある。\n",
    "\n",
    "encoderへの入力はpaddingされたデータである。ここで、paddingされた範囲が多い=padトークンが多く含まれているデータは、padの数が多くなるにつれて、隠れ状態がある一定の値に収束してしまう。RNNに同じトークンを何度も入力することで隠れ状態が収束していってしまうのだ。  \n",
    "そうなってしまうと、入力文に依る隠れ状態の違いが少なくなり、翻訳を学習できない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そこで、encoderが出力する隠れ状態として、padトークンを除いた最後のトークン=EOSを入力した時点のものを使用する。  \n",
    "encoderを以下のように変更する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_id = sp_ja.eos_id()\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        eos_positions = x == eos_id\n",
    "            # eosに対応する位置のみがTrueとなったTensor: (batch_size, seq_len)\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        y, _ = self.rnn(x) # y: (batch_size, seq_len, hidden_size)\n",
    "        h = y[eos_positions] # (batch_size, hidden_size)\n",
    "        h = self.fc(h).unsqueeze(0) # (1, batch_size, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ではこれで学習させてみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size, embed_size = 1024, 1024\n",
    "encoder = Encoder(n_vocab_ja, embed_size, hidden_size)\n",
    "decoder = Decoder(n_vocab_en, embed_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1-10/100: ######################################## 100% [00:02:01.03] loss: 3.55206 \n",
      "  11-20/100: ######################################## 100% [00:02:03.89] loss: 2.31923 \n",
      "  21-30/100: ######################################## 100% [00:02:03.33] loss: 1.53096 \n",
      "  31-40/100: ######################################## 100% [00:02:01.06] loss: 0.96194 \n",
      "  41-50/100: ######################################## 100% [00:02:03.29] loss: 0.56233 \n",
      "  51-60/100: ######################################## 100% [00:02:03.42] loss: 0.30002 \n",
      "  61-70/100: ######################################## 100% [00:02:03.65] loss: 0.14746 \n",
      "  71-80/100: ######################################## 100% [00:02:03.85] loss: 0.07586 \n",
      "  81-90/100: ######################################## 100% [00:02:03.21] loss: 0.05850 \n",
      " 91-100/100: ######################################## 100% [00:02:02.93] loss: 0.04216 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, n_epochs=100, prog_unit=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 翻訳\n",
    "\n",
    "作成したモデルに日本語文を入力し、英語に翻訳して出力する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_id = sp_en.unk_id() # UNKのID\n",
    "def token_sampling(y: List[float]) -> int:\n",
    "    \"\"\"モデルの出力から単語をサンプリングする\"\"\"\n",
    "    y[unk_id] = -torch.inf\n",
    "    probs = F.softmax(y, dim=-1)\n",
    "    token, = random.choices(range(n_vocab_en), weights=probs)\n",
    "    return token\n",
    "\n",
    "\n",
    "bos_id = sp_en.bos_id()\n",
    "eos_id = sp_en.eos_id()\n",
    "@torch.no_grad()\n",
    "def translate(\n",
    "    model: nn.Module,\n",
    "    in_text: str, # 入力文（日本語）\n",
    "    max_len: int = 100, # 出力のトークン数の上限\n",
    "    decisive: bool = True, # サンプリングを決定的にするか\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    in_ids = sp_ja.encode(in_text)\n",
    "    in_ids = torch.tensor(in_ids + [eos_id], device=device)\n",
    "\n",
    "    h = model.encoder(in_ids)\n",
    "    next_token = bos_id\n",
    "\n",
    "    token_ids = []\n",
    "    for _ in range(max_len):\n",
    "        x = torch.tensor([[next_token]], device=device)\n",
    "        y, h = model.decoder(x, h)\n",
    "        y = y[0]\n",
    "        if decisive:\n",
    "            next_token = y.argmax().item()\n",
    "        else:\n",
    "            next_token = token_sampling(y)\n",
    "        token_ids.append(next_token)\n",
    "        if next_token == eos_id:\n",
    "            break\n",
    "    sentence = sp_en.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 駅情報\n",
      "output: Information\n",
      "answer: Information\n",
      "\n",
      "input: 三条京阪駅（さんじょうけいはんえき）は、京都市東山区にある、京都市営地下鉄東西線の鉄道駅。\n",
      "output: Located in the Higashiyama Ward of Kyoto City, Sanjyo-Keihan Station is a stop on the Tozai Line, a Kyoto Municipal Subway Line.\n",
      "answer: Located in the Higashiyama Ward of Kyoto City, Sanjyo-Keihan Station is a stop on the Tozai Line, a Kyoto Municipal Subway Line.\n",
      "\n",
      "input: 駅番号はT11。\n",
      "output: The station number is T11.\n",
      "answer: The station number is T11.\n",
      "\n",
      "input: 京阪電気鉄道\n",
      "output: Keihan Electric Railway\n",
      "answer: The Keihan Electric Railway\n",
      "\n",
      "input: 京阪本線・京阪鴨東線（三条駅 (京都府)）\n",
      "output: Keihan Main Line and Keihan Oto Line in Sanjyo Station (in Kyoto Prefecture)\n",
      "answer: Keihan Main Line and Keihan Oto Line in Sanjyo Station (in Kyoto Prefecture)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for x, t in zip(data_ja[:n], data_en[:n]):\n",
    "    print('input:', x)\n",
    "    print('output:', translate(model, x))\n",
    "    print('answer:', t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'この駅は京都市内の中心部にあります。',\n",
    "    '京都'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: この駅は京都市内の中心部にあります。\n",
      "output: Trains at Demachiyanagi Station, the train with no one-sided one-way--which considered to a seven-car train.\n",
      "\n",
      "input: 京都\n",
      "output: Nonomiya-jinja Shrine (via Gion)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print('input:', sentence)\n",
    "    print('output:', translate(model, sentence))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
