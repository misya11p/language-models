{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "\n",
    "*Sequence to Sequence*.  \n",
    "*Encoder-Decoder Model*とも。\n",
    "\n",
    "2つのモデルを用いて、入力されたシーケンスに基づいた別のシーケンスを出力するモデル。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress(width=20, with_test=True, label=\"ppl train\", round=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Seq2Seqの発想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで、RNNを用いて、入力した単語列に続く単語を予測するモデルを作成し、単語の予測を繰り返すことで文章を生成した。\n",
    "\n",
    "RNNは時系列の情報を保持するために隠れ状態$h_t$を用いる。隠れ状態の初期値$h_0$は0ベクトルとしているが、ここで、何らかの入力データから生成したベクトルを$h_0$として用いることを考える。このとき、上手く学習させれば、その入力に基づいた文章を生成できるのではないか。\n",
    "\n",
    "例えば、入力を画像をとし、CNNを用いて抽出した特徴量を$h_0$として用いるようにすれば、入力画像に基づいた文章が生成できる。画像のキャプション生成などに応用できそう。隠れ状態を通じてRNNからCNNまで逆伝播が繋がるので、画像と文章のペアさえ用意すれば学習できそう。というかできる[1]。\n",
    "\n",
    "[1] [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、入力に文章を用いることはできないだろうか。RNNに文章を入力し、最後に出力された隠れ状態を文章ベクトルとする。これを別のRNNへの入力$h_0$とすれば、入力文に基づいた文章生成が可能になる。\n",
    "\n",
    "この発想は翻訳タスクに大きく役立つ。入力と出力に同じ意味を持った異なる言語の文章を設定すれば、入力文と同じ意味を持った文章生成が可能になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章ではこの翻訳モデルを作成する。入力に日本語文、出力に入力と同じ意味を持つ英語文を設定し、日本語→英語の翻訳を行うモデルを作成する。こういった、シーケンスをシーケンスに変換するモデルを *seq2seq (Sequence to Sequence)* と呼ぶ。\n",
    "\n",
    "seq2seqは以下の二つのRNNから構成される。\n",
    "- ***Encoder*** : 文章ベクトルを生成するRNN\n",
    "- ***Decoder*** : 文章ベクトルを受け取って出力文を生成するRNN\n",
    "\n",
    "このことから***Encoder-Decoderモデル***とも呼ばれる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 対訳コーパス\n",
    "\n",
    "翻訳モデルを作るには、同じ意味を持つ文章が複数の言語でまとまっているデータが必要。このようなデータは対訳コーパスと呼んだりする。\n",
    "\n",
    "本章では以下のデータセットを使用する。\n",
    "- [iwslt2017  |  TensorFlow Datasets](https://www.tensorflow.org/datasets/community_catalog/huggingface/iwslt2017?hl=ja#iwslt2017-en-ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load(\n",
    "    'huggingface:iwslt2017/iwslt2017-en-ja',\n",
    "    data_dir='data',\n",
    "    split='train'\n",
    ")\n",
    "ds = list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 223108 \n",
      "\n",
      "僕の旅は 14年前に始まりました\n",
      "My journey started 14 years ago.\n",
      "\n",
      "際立って成功したキャンペーンです その力に注目してみましょう\n",
      "Remarkably successful campaign, but notice the power of it.\n",
      "\n",
      "「ありがとう ご職業は？」\n",
      "I said, \"Great. What do you do?\"\n",
      "\n",
      "アメリカの誇りで包まれた 我々を定義する場所としての地位を 私達は長い間 この国の他所の風景に 譲ってきました グランドキャニオン ヨセミテ イエローストーン\n",
      "cloaking them with this American pride, places that we now consider to define us: Grand Canyon, Yosemite, Yellowstone.\n",
      "\n",
      "何をすべきかよく分からず とりあえず 周りの人々に 話しかけたんです 彼は人々にユニークなことを聞きました 「自由になった感想は？」\n",
      "He doesn't quite know what to do, so he starts going around the crowd and starts talking to people and he says to people in this rather unique way, \"How does it feel to be free?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_ja = []\n",
    "data_en = []\n",
    "for sample in ds:\n",
    "    ja = sample['translation']['ja'].decode()\n",
    "    en = sample['translation']['en'].decode()\n",
    "    data_ja.append(ja)\n",
    "    data_en.append(en)\n",
    "\n",
    "print('num of data:', len(data_ja), '\\n')\n",
    "for _ in range(5):\n",
    "    i = random.randint(0, len(data_ja))\n",
    "    print(data_ja[i])\n",
    "    print(data_en[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "書き出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile_ja = 'data/iwslt2017_ja.txt'\n",
    "with open(textfile_ja, 'w') as f:\n",
    "    f.write('\\n'.join(data_ja))\n",
    "\n",
    "textfile_en = 'data/iwslt2017_en.txt'\n",
    "with open(textfile_en, 'w') as f:\n",
    "    f.write('\\n'.join(data_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ数を減らしたものも作っておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 50000\n",
    "data_ja = data_ja[:n_data]\n",
    "data_en = data_en[:n_data]\n",
    "\n",
    "textfile_ja = f'data/iwslt2017_ja_{n_data}.txt'\n",
    "with open(textfile_ja, 'w') as f:\n",
    "    f.write('\\n'.join(data_ja))\n",
    "\n",
    "textfile_en = f'data/iwslt2017_en_{n_data}.txt'\n",
    "with open(textfile_en, 'w') as f:\n",
    "    f.write('\\n'.join(data_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークナイザの学習。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/iwslt2017_ja_50000.txt\n",
      "  input_format: \n",
      "  model_prefix: models/tokenizer_iwslt2017_ja_50000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 3\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: data/iwslt2017_ja_50000.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 50000 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=2036250\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=2256\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 50000 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=692277\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 397873 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 50000\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 146555\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 146555 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=161842 obj=37.3897 num_tokens=517036 num_tokens/piece=3.1947\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=144191 obj=34.6556 num_tokens=519824 num_tokens/piece=3.60511\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=107690 obj=35.3651 num_tokens=551958 num_tokens/piece=5.12543\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=107087 obj=34.9852 num_tokens=552555 num_tokens/piece=5.15987\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=80148 obj=36.3396 num_tokens=592829 num_tokens/piece=7.39668\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=80013 obj=35.9562 num_tokens=593318 num_tokens/piece=7.41527\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=59978 obj=37.4882 num_tokens=638716 num_tokens/piece=10.6492\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=59945 obj=37.119 num_tokens=638971 num_tokens/piece=10.6593\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=44953 obj=38.6081 num_tokens=680559 num_tokens/piece=15.1393\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=44944 obj=38.3141 num_tokens=680849 num_tokens/piece=15.1488\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=33708 obj=39.8196 num_tokens=723850 num_tokens/piece=21.4741\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=33708 obj=39.5252 num_tokens=724087 num_tokens/piece=21.4812\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=25281 obj=41.1122 num_tokens=766334 num_tokens/piece=30.3126\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=25281 obj=40.8469 num_tokens=766453 num_tokens/piece=30.3174\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=18960 obj=42.4763 num_tokens=809748 num_tokens/piece=42.7082\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=18960 obj=42.2127 num_tokens=809784 num_tokens/piece=42.7101\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=14220 obj=43.9906 num_tokens=857838 num_tokens/piece=60.3262\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=14220 obj=43.7066 num_tokens=858263 num_tokens/piece=60.356\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=10665 obj=45.4899 num_tokens=907576 num_tokens/piece=85.0985\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=10665 obj=45.184 num_tokens=907639 num_tokens/piece=85.1045\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=46.47 num_tokens=941416 num_tokens/piece=106.979\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=46.2613 num_tokens=941419 num_tokens/piece=106.979\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: models/tokenizer_iwslt2017_ja_50000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: models/tokenizer_iwslt2017_ja_50000.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/iwslt2017_en_50000.txt\n",
      "  input_format: \n",
      "  model_prefix: models/tokenizer_iwslt2017_en_50000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 3\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: data/iwslt2017_en_50000.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 50000 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=4809618\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9584% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999584\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 50000 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2472448\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 92936 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 50000\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 60151\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 60151 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=31295 obj=10.5838 num_tokens=121722 num_tokens/piece=3.8895\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=26878 obj=8.30403 num_tokens=122240 num_tokens/piece=4.54796\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20153 obj=8.29384 num_tokens=130443 num_tokens/piece=6.47263\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20144 obj=8.27497 num_tokens=130439 num_tokens/piece=6.47533\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15108 obj=8.38218 num_tokens=142770 num_tokens/piece=9.44996\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15108 obj=8.35608 num_tokens=142767 num_tokens/piece=9.44976\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11331 obj=8.50854 num_tokens=156863 num_tokens/piece=13.8437\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11331 obj=8.47856 num_tokens=156845 num_tokens/piece=13.8421\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=8.63325 num_tokens=169640 num_tokens/piece=19.2773\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=8.60565 num_tokens=169610 num_tokens/piece=19.2739\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: models/tokenizer_iwslt2017_en_50000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: models/tokenizer_iwslt2017_en_50000.vocab\n"
     ]
    }
   ],
   "source": [
    "pad_id = 3\n",
    "vocab_size = 8000\n",
    "\n",
    "tokenizer_prefix_ja = f'models/tokenizer_iwslt2017_ja_{n_data}'\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_ja,\n",
    "    model_prefix=tokenizer_prefix_ja,\n",
    "    vocab_size=vocab_size,\n",
    "    pad_id=pad_id\n",
    ")\n",
    "\n",
    "tokenizer_prefix_en = f'models/tokenizer_iwslt2017_en_{n_data}'\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_en,\n",
    "    model_prefix=tokenizer_prefix_en,\n",
    "    vocab_size=vocab_size,\n",
    "    pad_id=pad_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理\n",
    "\n",
    "データ数を減らした方を使う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークナイザ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary (ja): 8000\n",
      "num of vocabrary (en): 8000\n"
     ]
    }
   ],
   "source": [
    "sp_ja = spm.SentencePieceProcessor(f'{tokenizer_prefix_ja}.model')\n",
    "sp_en = spm.SentencePieceProcessor(f'{tokenizer_prefix_en}.model')\n",
    "\n",
    "unk_id = sp_ja.unk_id()\n",
    "bos_id = sp_ja.bos_id()\n",
    "eos_id = sp_ja.eos_id()\n",
    "\n",
    "n_vocab_ja = len(sp_ja)\n",
    "n_vocab_en = len(sp_en)\n",
    "print('num of vocabrary (ja):', n_vocab_ja)\n",
    "print('num of vocabrary (en):', n_vocab_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids_ja = sp_ja.encode(data_ja)\n",
    "data_ids_en = sp_en.encode(data_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOS, EOSの追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids_ja, ids_en in zip(data_ids_ja, data_ids_en):\n",
    "    ids_en.insert(0, bos_id)\n",
    "    ids_ja.append(eos_id)\n",
    "    ids_en.append(eos_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データの作成\n",
    "\n",
    "入力文と正解のペアを作成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoderへの入力（入力文）とDecoderの出力（正解）のペアを作成する。  \n",
    "また、Decoderへの入力を考える必要がある。今回は教師強制を採用し、出力文の頭に\\<BOS>を付与したものをDecoderへの入力とする。\n",
    "\n",
    "例）\n",
    "Encoderへの入力（入力文） | Decoderへの入力 | Eecoderの出力（出力文）\n",
    "--- | --- | ---\n",
    "夏 休み が 終わり ました 。 \\<EOS> | \\<BOS> Summer vacation is over . | Summer vacation is over . \\<EOS>\n",
    "ツイッター は 亡くなり ました 。 \\<EOS> | \\<BOS> Twitter is dead . | Twitter is dead . \\<EOS>\n",
    "今日 から X で 暮らし ましょう 。 \\<EOS> | \\<BOS> Let 's live in X from today . | Let 's live in X from today . \\<EOS>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaderの作成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 69]), torch.Size([32, 87]), torch.Size([32, 87]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids_ja, data_ids_en):\n",
    "        self.data_ja = [torch.tensor(ids) for ids in data_ids_ja]\n",
    "        self.data_en = [torch.tensor(ids) for ids in data_ids_en]\n",
    "        self.n_data = len(self.data_ja)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ja = self.data_ja[idx]\n",
    "        en = self.data_en[idx]\n",
    "        x_enc = ja # encoderへの入力\n",
    "        x_dec = en[:-1] # decoderへの入力\n",
    "        y_dec = en[1:] # decoderの出力\n",
    "        return x_enc, x_dec, y_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "def collate_fn(batch): # padding\n",
    "    x_enc, x_dec, y_dec= zip(*batch)\n",
    "    x_enc = pad_sequence(x_enc, batch_first=True, padding_value=pad_id)\n",
    "    x_dec = pad_sequence(x_dec, batch_first=True, padding_value=pad_id)\n",
    "    y_dec = pad_sequence(y_dec, batch_first=True, padding_value=pad_id)\n",
    "    return x_enc, x_dec, y_dec\n",
    "\n",
    "dataset = TextDataset(data_ids_ja, data_ids_en)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "x_enc, x_dec, y_dec = next(iter(train_loader))\n",
    "x_enc.shape, x_dec.shape, y_dec.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Seq2Seqを用いた翻訳モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "入力文を入れて隠れ状態を出力するだけのRNN。LSTMと線形層で作る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで1つ工夫を加える。普通に作っても動きはするが、学習が上手くいかない可能性がある。\n",
    "\n",
    "学習時にencoderへ入力されるデータはpaddingされたデータとなる。ここで、paddingされた範囲が多い=padトークンが多く含まれているデータは、padの数が多くなるにつれて、隠れ状態がある一定の値に収束してしまう。RNNに同じトークンを何度も入力することで隠れ状態が収束してしまうのだ。そうなってしまうと、入力文に依る隠れ状態の違いが少なくなり、翻訳を学習できない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そこで、encoderが出力する隠れ状態として、padトークンを除いた最後のトークン=EOSを入力した時点のものを使用する。  \n",
    "\n",
    "以上を踏まえ、encoderを以下のように実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            batch_first=True,\n",
    "            num_layers=3,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        eos_positions = x == eos_id\n",
    "            # eosに対応する位置のみがTrueとなったTensor: (batch_size, seq_len)\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        x = self.dropout(x)\n",
    "        _, hc = self.lstm(x) # (batch_size, seq_len, hidden_size)\n",
    "        # h = hs[eos_positions].unsqueeze(0) # (1, batch_size, hidden_size)\n",
    "        return hc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Encoderから出力された隠れ状態を受け取り、出力文を生成するRNN。Encoder同様、LSTMと線形層で作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            batch_first=True,\n",
    "            num_layers=3,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        x = self.dropout(x)\n",
    "        hs, hc = self.lstm(x, hc)\n",
    "            # hs: (batch_size, seq_len, hidden_size)\n",
    "            # h: (3, batch_size, hidden_size)\n",
    "            # c: (3, batch_size, hidden_size)\n",
    "        # hs = self.dropout(hs)\n",
    "        y = self.fc(hs) # (batch_size, seq_len, n_vocab)\n",
    "        return y, hc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "EncoderとDecoderを合わせて、入力から出力までの一連の処理を行うモデルを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        hc = self.encoder(x_enc)\n",
    "        # hc = (h, torch.zeros_like(h))\n",
    "        y, _ = self.decoder(x_dec, hc)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of parameters: 24,903,488\n"
     ]
    }
   ],
   "source": [
    "hidden_size, embed_size = 512, 512\n",
    "encoder = Encoder(n_vocab_ja, embed_size, hidden_size)\n",
    "decoder = Decoder(n_vocab_en, embed_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"num of parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 実践"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "def loss_fn(y, t):\n",
    "    loss = cross_entropy(y.reshape(-1, n_vocab_ja), t.ravel())\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    ppls = []\n",
    "    for x_enc, x_dec, y_dec in test_loader:\n",
    "        x_enc = x_enc.to(device)\n",
    "        x_dec = x_dec.to(device)\n",
    "        y_dec = y_dec.to(device)\n",
    "\n",
    "        y = model(x_enc, x_dec)\n",
    "        loss = loss_fn(y, y_dec)\n",
    "        ppl = torch.exp(loss).item()\n",
    "        ppls.append(ppl)\n",
    "    ppl = sum(ppls) / len(ppls)\n",
    "    return ppl\n",
    "\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    prog.start(n_iter=len(train_loader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for x_enc, x_dec, y_dec in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ppl = torch.exp(loss).item()\n",
    "            prog.update(ppl)\n",
    "\n",
    "        if prog.now_epoch % prog_unit == 0:\n",
    "            test_ppl = eval_model(model)\n",
    "            prog.memo(f\"test: {test_ppl:.2f}\", no_step=True)\n",
    "        prog.memo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/20: #################### 100% [00:07:57.28] ppl train: 500.66, test: 285.36 \n",
      " 2/20: #################### 100% [00:08:04.10] ppl train: 231.94, test: 188.24 \n",
      " 3/20: #################### 100% [00:07:59.97] ppl train: 175.01, test: 156.47 \n",
      " 4/20: #################### 100% [00:08:01.69] ppl train: 149.81, test: 139.30 \n",
      " 5/20: #################### 100% [00:08:04.23] ppl train: 134.13, test: 127.81 \n",
      " 6/20: #################### 100% [00:08:00.38] ppl train: 122.81, test: 119.26 \n",
      " 7/20: #################### 100% [00:07:59.79] ppl train: 113.74, test: 112.51 \n",
      " 8/20: #################### 100% [00:08:00.70] ppl train: 106.42, test: 107.78 \n",
      " 9/20: #################### 100% [00:08:00.84] ppl train: 100.10, test: 102.64 \n",
      "10/20: #################### 100% [00:07:59.82] ppl train: 94.69, test: 98.87  \n",
      "11/20: #################### 100% [00:08:01.15] ppl train: 89.88, test: 96.11 \n",
      "12/20: #################### 100% [00:08:02.61] ppl train: 85.57, test: 93.35 \n",
      "13/20: #################### 100% [00:08:00.70] ppl train: 81.79, test: 91.05 \n",
      "14/20: #################### 100% [00:08:00.60] ppl train: 78.30, test: 88.62 \n",
      "15/20: #################### 100% [00:08:01.01] ppl train: 75.37, test: 87.44 \n",
      "16/20: #################### 100% [00:08:03.09] ppl train: 72.41, test: 85.77 \n",
      "17/20: #################### 100% [00:08:13.56] ppl train: 69.87, test: 85.00 \n",
      "18/20: #################### 100% [00:02:58.24] ppl train: 67.37, test: 83.51 \n",
      "19/20: #################### 100% [00:02:42.20] ppl train: 65.08, test: 82.42 \n",
      "20/20: #################### 100% [00:02:33.39] ppl train: 63.05, test: 81.52 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, n_epochs=20, prog_unit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/lm_seq2seq.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 翻訳\n",
    "\n",
    "作成したモデルに日本語文を入力し、英語に翻訳して出力する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_sampling(y, decisive=True):\n",
    "    y.squeeze_(0)\n",
    "    if decisive:\n",
    "        token = y.argmax().item()\n",
    "    else:\n",
    "        y[unk_id] = -torch.inf\n",
    "        probs = F.softmax(y, dim=-1)\n",
    "        token, = random.choices(range(n_vocab_en), weights=probs)\n",
    "    return token\n",
    "\n",
    "\n",
    "bos_id = sp_en.bos_id()\n",
    "eos_id = sp_en.eos_id()\n",
    "@torch.no_grad()\n",
    "def translate(\n",
    "    model: nn.Module,\n",
    "    in_text: str, # 入力文（日本語）\n",
    "    max_len: int = 100, # 出力のトークン数の上限\n",
    "    decisive: bool = True, # サンプリングを決定的にするか\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    in_ids = sp_ja.encode(in_text)\n",
    "    in_ids = torch.tensor(in_ids + [eos_id], device=device).unsqueeze(0)\n",
    "\n",
    "    hc = model.encoder(in_ids)\n",
    "    next_token = bos_id\n",
    "\n",
    "    token_ids = []\n",
    "    while len(token_ids) <= max_len and next_token != eos_id:\n",
    "        x = torch.tensor([next_token], device=device).reshape(1, 1)\n",
    "        y, hc = model.decoder(x, hc)\n",
    "        next_token = token_sampling(y, decisive)\n",
    "        token_ids.append(next_token)\n",
    "\n",
    "    sentence = sp_en.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは訓練データから。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 例えば 人々は見知らぬ人による誘拐を恐れますが データによると顔見知りによる誘拐の方が断然多いのです\n",
      "output: And if you're going to do a little bit of a few years, you're going to see that the world's spussion, and it's not a very good thing to do, but it's a very important thing to do.\n",
      "answer: One example would be, people fear kidnapping by strangers when the data supports kidnapping by relatives is much more common.\n",
      "\n",
      "input: ある私のお気に入りの研究では 夫が家事に積極的なほど 妻は夫に魅力を感じることが判明しました\n",
      "output: And I was a little bit of a few years ago, and I was a very good idea, and I was a little bit of a few years ago, and I was a little bit of a few years.\n",
      "answer: One of my favorite studies found that the more willing a husband is to do house work, the more attractive his wife will find him.\n",
      "\n",
      "input: スローガンを一つ掲げます 「子供の仕事はおもちゃを壊すこと」です\n",
      "output: And I'm going to show you a little bit of a couple of years ago, and I'm going to say, \"I'm going to show you a little bit of a little bit of a day.\n",
      "answer: And we have a slogan that the best thing a child can do with a toy is to break it.\n",
      "\n",
      "input: 子供を作る準備は出来ているか\n",
      "output: I'm going to show you a little bit of a little bit.\n",
      "answer: Are they going to start having children right away?\n",
      "\n",
      "input: しかしラベルには「致死性の毒」と書いてあります グレースはその粉が毒だと思って\n",
      "output: But the first thing I'm going to do is I'm going to say, \"I'm going to go to the  ⁇ ueen of the  ⁇ \"C.\n",
      "answer: But the powder is labeled \"Deadly Poison,\" so Grace thinks that the powder is a deadly poison.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(train_dataset))\n",
    "    x, _, t = train_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print(\"input:\", x)\n",
    "    print(\"output:\", translate(model, x))\n",
    "    print(\"answer:\", t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練データに含まれていないものも試してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: このタペストリーの展示場に設置しました\n",
      "output: This is a very interesting thing, and this is a very good thing.\n",
      "answer: And we were put in this tapestry room.\n",
      "\n",
      "input: 全世界的なレベルでいうと、私たちは自分自身を超えるものを持たなくてはなりません。\n",
      "output: And we're going to be able to do that, and we're going to be able to do that.\n",
      "answer: On the global level, we have to have more than our own thing.\n",
      "\n",
      "input: 制約の中で 創造性を 発揮する術を学ぶことが 私達が自分を変え 全体として世界を変える ― 最も確かな道なのです\n",
      "output: And if you look at the same time, you can see that the world's population is not a very good thing, and it's a very important thing, and it's a very important thing.\n",
      "answer: Learning to be creative within the confines of our limitations is the best hope we have to transform ourselves and, collectively, transform our world.\n",
      "\n",
      "input: しかし、必要な結果が得られませんでした。\n",
      "output: But we're going to do this.\n",
      "answer: But they couldn't get the results they wanted.\n",
      "\n",
      "input: 引っ張って紙タオルを 切り取るタイプもあります\n",
      "output: And I'm going to show you a lot of people who are in the middle of the world.\n",
      "answer: There's the one that cuts it, that you have to tear off.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(test_dataset))\n",
    "    x, _, t = test_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print('input:', x)\n",
    "    print('output:', translate(model, x))\n",
    "    print('answer:', t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: ありがとう。\n",
      "output: herently.\n",
      "\n",
      "input: 猫はかわいいね。\n",
      "output: s a lot of people.\n",
      "\n",
      "input: 上手く文章が書けるようになりました。\n",
      "output: s the world's own.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "sentences = [\n",
    "    'ありがとう。',\n",
    "    '猫はかわいいね。',\n",
    "    '上手く文章が書けるようになりました。'\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print('input:', sentence)\n",
    "    print('output:', translate(model, sentence))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "びみょう。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
