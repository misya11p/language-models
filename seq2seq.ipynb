{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext import transforms\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchvision.transforms import Compose\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 5304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>japanese</th>\n",
       "      <th>english</th>\n",
       "      <th>chinese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#0001</td>\n",
       "      <td>Xではないかとつくづく疑問に思う</td>\n",
       "      <td>I often wonder if it might be X.</td>\n",
       "      <td>难道不会是X吗，我实在是感到怀疑。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#0002</td>\n",
       "      <td>Xがいいなといつも思います</td>\n",
       "      <td>I always think X would be nice.</td>\n",
       "      <td>我总觉得X不错。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#0003</td>\n",
       "      <td>それがあるようにいつも思います</td>\n",
       "      <td>It always seems like it is there.</td>\n",
       "      <td>我总觉得那好像是有的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#0004</td>\n",
       "      <td>それが多すぎないかと正直思う</td>\n",
       "      <td>I honestly feel like there is too much.</td>\n",
       "      <td>老实说我觉得那太多了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#0005</td>\n",
       "      <td>山田はみんなに好かれるタイプの人だと思う</td>\n",
       "      <td>I think that Yamada is the type everybody likes.</td>\n",
       "      <td>我想山田是受大家欢迎的那种人。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id              japanese   \n",
       "0  #0001      Xではないかとつくづく疑問に思う  \\\n",
       "1  #0002         Xがいいなといつも思います   \n",
       "2  #0003       それがあるようにいつも思います   \n",
       "3  #0004        それが多すぎないかと正直思う   \n",
       "4  #0005  山田はみんなに好かれるタイプの人だと思う   \n",
       "\n",
       "                                            english            chinese  \n",
       "0                  I often wonder if it might be X.  难道不会是X吗，我实在是感到怀疑。  \n",
       "1                   I always think X would be nice.           我总觉得X不错。  \n",
       "2                 It always seems like it is there.        我总觉得那好像是有的。  \n",
       "3           I honestly feel like there is too much.        老实说我觉得那太多了。  \n",
       "4  I think that Yamada is the type everybody likes.    我想山田是受大家欢迎的那种人。  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('data/JEC_basic_sentence_v1-3.xls', header=None)\n",
    "df.columns = ['id', 'japanese', 'english', 'chinese']\n",
    "print('num of data:', len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_ja = spacy.load('ja_core_news_sm')\n",
    "def tokenize(data: List[str], l='en') -> List[List[str]]:\n",
    "    nlp = eval('nlp_' + l)\n",
    "    return [[token.text for token in nlp(sentence)] for sentence in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ja = tokenize(df['japanese'], l='ja')\n",
    "text_en = tokenize(df['english'], l='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad, bos, eos, unk = '<pad>', '<bos>', '<eos>', '<unk>'\n",
    "max_len = 30\n",
    "specials = [pad, bos, eos, unk]\n",
    "vocab_ja = build_vocab_from_iterator(text_ja, specials=specials)\n",
    "vocab_en = build_vocab_from_iterator(text_en, specials=specials)\n",
    "\n",
    "transform_ja = Compose([\n",
    "    transforms.Truncate(max_len-2),\n",
    "    transforms.AddToken(bos, begin=True),\n",
    "    transforms.AddToken(eos, begin=False),\n",
    "    transforms.VocabTransform(vocab_ja),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.PadTransform(max_len, vocab_ja[pad])\n",
    "])\n",
    "\n",
    "transform_en = Compose([\n",
    "    transforms.Truncate(max_len-2),\n",
    "    transforms.AddToken(bos, begin=True),\n",
    "    transforms.AddToken(eos, begin=False),\n",
    "    transforms.VocabTransform(vocab_en),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.PadTransform(max_len, vocab_en[pad])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, in_text, out_text, in_transform, out_transform):\n",
    "        self.in_text = in_text\n",
    "        self.out_text = out_text\n",
    "        self.in_transform = in_transform\n",
    "        self.out_transform = out_transform\n",
    "        self.n_samples = len(in_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        in_text = self.in_text[index]\n",
    "        out_text = self.out_text[index]\n",
    "        in_text = self.in_transform(in_text)\n",
    "        out_text = self.out_transform(out_text)\n",
    "        return in_text, out_text[:-1], out_text[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "dataset = TextDataset(text_ja, text_en, transform_ja, transform_en)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        # self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h = self.rnn(x)\n",
    "        h = self.fc(h)\n",
    "        # _, h = self.lstm(x)\n",
    "        # h = self.fc(h[0][0])\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        # self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        x = self.embedding(x)\n",
    "        y, h = self.rnn(x, h)\n",
    "        # y, h = self.lstm(x, h)\n",
    "        y = self.fc(y)\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, n_in_vocab, n_out_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(n_in_vocab, embed_size, hidden_size)\n",
    "        self.decoder = Decoder(n_out_vocab, embed_size, hidden_size)\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        h = self.encoder(x_enc)\n",
    "        y, _ = self.decoder(x_dec, h)\n",
    "        # h0 = h.unsqueeze(0)\n",
    "        # c0 = torch.zeros_like(h0)\n",
    "        # y, _ = self.decoder(x_dec, (h0, c0))\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_en[pad])\n",
    "def train(model, optimizer, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        for x_enc, x_dec, label in tqdm(dataloader, desc=f'{epoch}/{n_epochs}', disable=True):\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            label = label.to(device)\n",
    "            y_pred = model(x_enc, x_dec)\n",
    "            loss = criterion(y_pred.reshape(-1, y_pred.shape[-1]), label.ravel())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'{epoch}/{n_epochs} loss: {epoch_loss/len(dataloader)}', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(len(vocab_ja), len(vocab_en), 1024, 1024).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 loss: 5.327651828168387\n",
      "2/10 loss: 4.158711934664163\n",
      "3/10 loss: 3.3700954942818147\n",
      "4/10 loss: 2.751070531017809\n",
      "5/10 loss: 2.2608760142900857\n",
      "6/10 loss: 1.9038497367537166\n",
      "7/10 loss: 1.6444121857723557\n",
      "8/10 loss: 1.4840695621019386\n",
      "9/10 loss: 1.3940537054854703\n",
      "10/10 loss: 1.3437765707452614\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 翻訳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def translate(model, text, max_len=100):\n",
    "    model.eval()\n",
    "    tokens = tokenize([text], 'ja')[0]\n",
    "    tokens = transform_ja(tokens).unsqueeze(0).to(device)\n",
    "    h = model.encoder(tokens)\n",
    "    # h0 = h.unsqueeze(0)\n",
    "    # c0 = torch.zeros_like(h0)\n",
    "    # h = (h0, c0)\n",
    "    next_token = vocab_en[bos]\n",
    "\n",
    "    tokens = []\n",
    "    for _ in range(max_len):\n",
    "        next_token = torch.tensor(next_token).reshape(1, 1).to(device)\n",
    "        y, h = model.decoder(next_token, h)\n",
    "        next_token = y.argmax().item()\n",
    "        if next_token == vocab_en[eos]:\n",
    "            break\n",
    "        tokens.append(next_token)\n",
    "    return ' '.join([vocab_en.get_itos()[t] for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He got a one - hour massage .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, 'Xではないかとつくづく疑問に思う')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
