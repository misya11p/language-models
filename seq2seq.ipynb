{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "\n",
    "*Sequence to Sequence*.  \n",
    "*Encoder-Decoder Model*とも。\n",
    "\n",
    "2つのモデルを用いて、入力されたシーケンスに基づいた別のシーケンスを出力するモデル。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xml.etree import ElementTree as ET\n",
    "# from glob import glob\n",
    "# import os\n",
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "from typing import List\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dlprog import train_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog = train_progress(with_test=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 発想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで、RNNを用いて、入力した単語列に続く単語を予測するモデルを作成し、単語の予測を繰り返すことで文章を生成した。\n",
    "\n",
    "RNNは時系列の情報を保持するために隠れ状態$h_t$を用いる。隠れ状態の初期値$h_0$は0ベクトルとしているが、ここで、何らかの入力データから生成したベクトルを$h_0$として用いることを考える。このとき、上手く学習させれば、その入力に基づいた文章を生成できそう。\n",
    "\n",
    "例えば、入力を画像をとし、CNNを用いて抽出した特徴量を$h_0$として用いるようにすれば、入力画像に基づいた文章が生成できる。画像のキャプションなどが例に挙げられる。  \n",
    "学習方法は簡単で、入力画像に対して適切な文章が出力されるように学習させるだけ。隠れ状態を通じてRNNからCNNまで逆伝播を繋げる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、入力に文章を用いることはできないだろうか。RNNに文章を入力し、最後に出力された隠れ状態を文章ベクトルとする。これを別のRNNへの入力$h_0$とすれば、入力文に基づいた文章生成が可能になる。  \n",
    "\n",
    "この発想は翻訳タスクに大きく役立つ。入力と出力に同じ意味を持った異なる言語の文章を設定すれば、入力文と同じ意味を持った文章生成が可能になる。  \n",
    "こういった、シーケンスをシーケンスに変換するモデルを *seq2seq (Sequence to Sequence)* と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章では、入力に日本語文、出力に入力と同じ意味を持つ英語文を設定し、日本語→英語の翻訳を行うモデルを作成する。\n",
    "\n",
    "このモデルは以下の二つのRNNから構成される。\n",
    "- ***Encoder*** : 文章ベクトルを生成するRNN\n",
    "- ***Decoder*** : 文章ベクトルを受け取って出力文を生成するRNN\n",
    "\n",
    "このことから***Encoder-Decoderモデル***とも呼ばれる。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 対訳コーパス\n",
    "\n",
    "翻訳モデルを作るには、同じ意味を持つ文章が複数の言語でまとまっているデータが必要。このようなデータは対訳コーパスと呼んだりする。\n",
    "\n",
    "本章では以下のデータセットを使用する。\n",
    "- [iwslt2017  |  TensorFlow Datasets](https://www.tensorflow.org/datasets/community_catalog/huggingface/iwslt2017?hl=ja#iwslt2017-en-ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load(\n",
    "    'huggingface:iwslt2017/iwslt2017-en-ja',\n",
    "    data_dir='data',\n",
    "    split='train'\n",
    ")\n",
    "ds = list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 223108 \n",
      "\n",
      "人々が衣食住を持たずに 繁栄の話しをするのはナンセンスです\n",
      "It would be nonsense to talk about people flourishing if they didn't have food, clothing and shelter.\n",
      "\n",
      "屋根を突き抜けて立つ ３本の木があるのです\n",
      "There are three trees popping through.\n",
      "\n",
      "最初に現れるものに手を染める\n",
      "So they take the first one that comes along, often.\n",
      "\n",
      "もちろんオーヴィルとウィルバーのライト兄弟、 ライトフライヤー号です\n",
      "So this is of course Orville and Wilbur Wright, and the Wright Flyer.\n",
      "\n",
      "次の犬は 亡霊となって語りかけてきます 犬の魂が再び主人に会いに この世に戻ってきたのです\n",
      "And our next dog speaks in something called the revenant, which means a spirit that comes back to visit you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_ja = []\n",
    "data_en = []\n",
    "for sample in ds:\n",
    "    ja = sample['translation']['ja'].decode()\n",
    "    en = sample['translation']['en'].decode()\n",
    "    data_ja.append(ja)\n",
    "    data_en.append(en)\n",
    "\n",
    "print('num of data:', len(data_ja), '\\n')\n",
    "for _ in range(5):\n",
    "    i = random.randint(0, len(data_ja))\n",
    "    print(data_ja[i])\n",
    "    print(data_en[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "書き出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile_ja = 'data/iwslt2017_ja.txt'\n",
    "with open(textfile_ja, 'w') as f:\n",
    "    f.write('\\n'.join(data_ja))\n",
    "\n",
    "textfile_en = 'data/iwslt2017_en.txt'\n",
    "with open(textfile_en, 'w') as f:\n",
    "    f.write('\\n'.join(data_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ数を減らしたものも作っておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 10000\n",
    "data_ja = data_ja[:n_data]\n",
    "data_en = data_en[:n_data]\n",
    "\n",
    "textfile_ja = f'data/kyoto_ja_{n_data}.txt'\n",
    "with open(textfile_ja, 'w') as f:\n",
    "    f.write('\\n'.join(data_ja))\n",
    "\n",
    "textfile_en = f'data/kyoto_en_{n_data}.txt'\n",
    "with open(textfile_en, 'w') as f:\n",
    "    f.write('\\n'.join(data_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークナイザの学習。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/kyoto_ja_10000.txt\n",
      "  input_format: \n",
      "  model_prefix: models/tokenizer_iwslt2017_ja_10000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 3\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: data/kyoto_ja_10000.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 10000 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=412475\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=2153\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 10000 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=137779\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 87856 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 10000\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 30607\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 30607 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=42933 obj=39.6967 num_tokens=124760 num_tokens/piece=2.90592\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=38770 obj=37.1193 num_tokens=125352 num_tokens/piece=3.23322\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=28979 obj=38.5973 num_tokens=134756 num_tokens/piece=4.65013\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=28841 obj=38.0819 num_tokens=134865 num_tokens/piece=4.67616\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=21607 obj=40.2975 num_tokens=147395 num_tokens/piece=6.82163\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=21575 obj=39.7714 num_tokens=147487 num_tokens/piece=6.83601\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=16176 obj=42.0136 num_tokens=161429 num_tokens/piece=9.97954\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16173 obj=41.526 num_tokens=161506 num_tokens/piece=9.98615\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=12129 obj=43.7905 num_tokens=174261 num_tokens/piece=14.3673\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=12127 obj=43.4057 num_tokens=174277 num_tokens/piece=14.371\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=9095 obj=45.5653 num_tokens=187595 num_tokens/piece=20.6262\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=9095 obj=45.1722 num_tokens=187614 num_tokens/piece=20.6283\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=45.4645 num_tokens=189196 num_tokens/piece=21.4995\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=45.422 num_tokens=189195 num_tokens/piece=21.4994\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: models/tokenizer_iwslt2017_ja_10000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: models/tokenizer_iwslt2017_ja_10000.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/kyoto_en_10000.txt\n",
      "  input_format: \n",
      "  model_prefix: models/tokenizer_iwslt2017_en_10000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 3\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: data/kyoto_en_10000.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 10000 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=969452\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9606% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999606\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 10000 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=492999\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 40504 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 10000\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 23479\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 23479 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=13852 obj=10.7231 num_tokens=47546 num_tokens/piece=3.43243\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=12100 obj=8.6129 num_tokens=47844 num_tokens/piece=3.95405\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=9075 obj=8.62923 num_tokens=51263 num_tokens/piece=5.64882\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=9065 obj=8.5931 num_tokens=51261 num_tokens/piece=5.65483\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=8.60184 num_tokens=51657 num_tokens/piece=5.87011\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8799 obj=8.59885 num_tokens=51656 num_tokens/piece=5.87067\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: models/tokenizer_iwslt2017_en_10000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: models/tokenizer_iwslt2017_en_10000.vocab\n"
     ]
    }
   ],
   "source": [
    "pad_id = 3\n",
    "\n",
    "vocab_size_ja = 8000\n",
    "tokenizer_prefix_ja = f'models/tokenizer_iwslt2017_ja_{n_data}'\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_ja, # データセット\n",
    "    model_prefix=tokenizer_prefix_ja,\n",
    "    vocab_size=vocab_size_ja,\n",
    "    pad_id=pad_id\n",
    ")\n",
    "\n",
    "vocab_size_en = 8000\n",
    "tokenizer_prefix_en = f'models/tokenizer_iwslt2017_en_{n_data}'\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=textfile_en, # データセット\n",
    "    model_prefix=tokenizer_prefix_en,\n",
    "    vocab_size=vocab_size_en,\n",
    "    pad_id=pad_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理\n",
    "\n",
    "データ数を減らした方を使う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークナイザ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of vocabrary (ja): 8000\n",
      "num of vocabrary (en): 8000\n"
     ]
    }
   ],
   "source": [
    "sp_ja = spm.SentencePieceProcessor(f'{tokenizer_prefix_ja}.model')\n",
    "sp_en = spm.SentencePieceProcessor(f'{tokenizer_prefix_en}.model')\n",
    "\n",
    "unk_id = sp_ja.unk_id()\n",
    "bos_id = sp_ja.bos_id()\n",
    "eos_id = sp_ja.eos_id()\n",
    "\n",
    "n_vocab_ja = len(sp_ja)\n",
    "n_vocab_en = len(sp_en)\n",
    "print('num of vocabrary (ja):', n_vocab_ja)\n",
    "print('num of vocabrary (en):', n_vocab_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids_ja = sp_ja.encode(data_ja)\n",
    "data_ids_en = sp_en.encode(data_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOS, EOSの追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids_ja, ids_en in zip(data_ids_ja, data_ids_en):\n",
    "    ids_en.insert(0, bos_id)\n",
    "    ids_ja.append(eos_id)\n",
    "    ids_en.append(eos_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習データの作成\n",
    "\n",
    "入力文と正解のペアを作成する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoderへの入力（入力文）とDecoderの出力（正解）のペアを作成する。  \n",
    "また、Decoderへの入力を考える必要がある。今回は教師強制を採用し、出力文の頭に\\<BOS>を付与したものをDecoderへの入力とする。\n",
    "\n",
    "例）\n",
    "Encoderへの入力（入力文） | Decoderへの入力 | Eecoderの出力（出力文）\n",
    "--- | --- | ---\n",
    "夏 休み が 終わり ました 。 \\<EOS> | \\<BOS> Summer vacation is over . | Summer vacation is over . \\<EOS>\n",
    "ツイッター は 亡くなり ました 。 \\<EOS> | \\<BOS> Twitter is dead . | Twitter is dead . \\<EOS>\n",
    "今日 から X で 暮らし ましょう 。 \\<EOS> | \\<BOS> Let 's live in X from today . | Let 's live in X from today . \\<EOS>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaderの作成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 72]), torch.Size([32, 55]), torch.Size([32, 55]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_ids_ja, data_ids_en):\n",
    "        self.data_ja = [torch.tensor(ids) for ids in data_ids_ja]\n",
    "        self.data_en = [torch.tensor(ids) for ids in data_ids_en]\n",
    "        self.n_data = len(self.data_ja)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ja = self.data_ja[idx]\n",
    "        en = self.data_en[idx]\n",
    "        x_enc = ja # encoderへの入力\n",
    "        x_dec = en[:-1] # decoderへの入力\n",
    "        y_dec = en[1:] # decoderの出力\n",
    "        return x_enc, x_dec, y_dec\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data\n",
    "\n",
    "def collate_fn(batch): # padding\n",
    "    x_enc, x_dec, y_dec= zip(*batch)\n",
    "    x_enc = pad_sequence(x_enc, batch_first=True, padding_value=pad_id)\n",
    "    x_dec = pad_sequence(x_dec, batch_first=True, padding_value=pad_id)\n",
    "    y_dec = pad_sequence(y_dec, batch_first=True, padding_value=pad_id)\n",
    "    return x_enc, x_dec, y_dec\n",
    "\n",
    "dataset = TextDataset(data_ids_ja, data_ids_en)\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "x_enc, x_dec, y_dec = next(iter(train_loader))\n",
    "x_enc.shape, x_dec.shape, y_dec.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築\n",
    "\n",
    "EncoderとDecoderを作る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "入力文を入れて隠れ状態を出力するだけのRNN(LSTM)。LSTMと線形層で作る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで1つ工夫を加える。普通に作っても動きはするが、学習が上手くいかない可能性がある。\n",
    "\n",
    "encoderへの入力はpaddingされたデータである。ここで、paddingされた範囲が多い=padトークンが多く含まれているデータは、padの数が多くなるにつれて、隠れ状態がある一定の値に収束してしまう。RNNに同じトークンを何度も入力することで隠れ状態が収束していってしまうのだ。  \n",
    "そうなってしまうと、入力文に依る隠れ状態の違いが少なくなり、翻訳を学習できない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そこで、encoderが出力する隠れ状態として、padトークンを除いた最後のトークン=EOSを入力した時点のものを使用する。  \n",
    "\n",
    "以上を踏まえ、encoderを以下のように実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        eos_positions = x == eos_id\n",
    "            # eosに対応する位置のみがTrueとなったTensor: (batch_size, seq_len)\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        hs, _ = self.lstm(x) # (batch_size, seq_len, hidden_size)\n",
    "        h = hs[eos_positions] # (batch_size, hidden_size)\n",
    "        h = self.fc(h).unsqueeze(0) # (1, batch_size, hidden_size)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Encoderから出力された隠れ状態を受け取り、出力文を生成するRNN。Encoder同様、LSTMと線形層で作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n",
    "        hs, (h, c) = self.lstm(x, hc)\n",
    "            # hs: (batch_size, seq_len, hidden_size)\n",
    "            # h: (1, batch_size, hidden_size)\n",
    "            # c: (1, batch_size, hidden_size)\n",
    "        y = self.fc(hs) # (batch_size, seq_len, n_vocab)\n",
    "        return y, (h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "EncoderとDecoderを合わせて、入力から出力までの一連の処理を行うモデルを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        h = self.encoder(x_enc)\n",
    "        hc = (h, torch.zeros_like(h))\n",
    "        y, _ = self.decoder(x_dec, hc)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "def loss_fn(y, t):\n",
    "    loss = cross_entropy(y.reshape(-1, n_vocab_ja), t.ravel())\n",
    "    return loss\n",
    "\n",
    "def eval_model(model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_enc, x_dec, y_dec in test_loader:\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            total_loss += loss.item()\n",
    "    loss = total_loss / len(test_loader)\n",
    "    return loss\n",
    "\n",
    "def train(model, optimizer, n_epochs, prog_unit=1):\n",
    "    prog.start(n_iter=len(train_loader), n_epochs=n_epochs, unit=prog_unit)\n",
    "    for _ in range(n_epochs):\n",
    "        model.train()\n",
    "        for x_enc, x_dec, y_dec in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y_dec = y_dec.to(device)\n",
    "\n",
    "            y = model(x_enc, x_dec)\n",
    "            loss = loss_fn(y, y_dec)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prog.update(loss.item())\n",
    "\n",
    "        if prog.now_epoch % prog_unit == 0:\n",
    "            test_loss = eval_model(model)\n",
    "            prog.memo(f'test: {test_loss:.5f}', no_step=True)\n",
    "        prog.memo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size, embed_size = 512, 512\n",
    "encoder = Encoder(n_vocab_ja, embed_size, hidden_size)\n",
    "decoder = Decoder(n_vocab_en, embed_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1-20/200: ############################## 100% [00:02:51.26] loss train: 4.88085, test: 4.91392 \n",
      "  21-40/200: ############################## 100% [00:02:09.91] loss train: 3.56565, test: 5.17883 \n",
      "  41-60/200: ############################## 100% [00:02:09.57] loss train: 2.55636, test: 5.72575 \n",
      "  61-80/200: ############################## 100% [00:02:06.33] loss train: 1.74147, test: 6.36254 \n",
      " 81-100/200: ############################## 100% [00:02:05.70] loss train: 1.12456, test: 7.03689 \n",
      "101-120/200: ############################## 100% [00:02:05.42] loss train: 0.68134, test: 7.78627 \n",
      "121-140/200: ############################## 100% [00:02:05.53] loss train: 0.38464, test: 8.41202 \n",
      "141-160/200: ############################## 100% [00:02:05.25] loss train: 0.20159, test: 8.95770 \n",
      "161-180/200: ############################## 100% [00:02:05.92] loss train: 0.10116, test: 9.36043 \n",
      "181-200/200: ############################## 100% [00:02:05.48] loss train: 0.05881, test: 9.83690 \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, n_epochs=200, prog_unit=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 翻訳\n",
    "\n",
    "作成したモデルに日本語文を入力し、英語に翻訳して出力する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_sampling(y, decisive=True):\n",
    "    y.squeeze_(0)\n",
    "    if decisive:\n",
    "            token = y.argmax().item()\n",
    "    else:\n",
    "        y[unk_id] = -torch.inf\n",
    "        probs = F.softmax(y, dim=-1)\n",
    "        token, = random.choices(range(n_vocab_en), weights=probs)\n",
    "    return token\n",
    "\n",
    "\n",
    "bos_id = sp_en.bos_id()\n",
    "eos_id = sp_en.eos_id()\n",
    "@torch.no_grad()\n",
    "def translate(\n",
    "    model: nn.Module,\n",
    "    in_text: str, # 入力文（日本語）\n",
    "    max_len: int = 100, # 出力のトークン数の上限\n",
    "    decisive: bool = True, # サンプリングを決定的にするか\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    in_ids = sp_ja.encode(in_text)\n",
    "    in_ids = torch.tensor(in_ids + [eos_id], device=device)\n",
    "\n",
    "    h = model.encoder(in_ids)\n",
    "    hc = (h, torch.zeros_like(h))\n",
    "    next_token = bos_id\n",
    "\n",
    "    token_ids = []\n",
    "    while len(token_ids) <= max_len and next_token != eos_id:\n",
    "        x = torch.tensor([[next_token]], device=device)\n",
    "        y, hc = model.decoder(x, hc)\n",
    "        next_token = token_sampling(y, decisive)\n",
    "        token_ids.append(next_token)\n",
    "\n",
    "    sentence = sp_en.decode(token_ids)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは訓練データから。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: これが重要なのです\n",
      "output: And that makes a difference.\n",
      "answer: And that makes a difference.\n",
      "\n",
      "input: 長期的に考えると 生命がいなければ自分たちで作ることになります\n",
      "output: But in the long run, if there's no life there, we create it ourselves.\n",
      "answer: But in the long run, if there's no life there, we create it ourselves.\n",
      "\n",
      "input: 実は要となるピッチはすでに登り終えています\n",
      "output: And all the hard pitches are actually below him.\n",
      "answer: And all the hard pitches are actually below him.\n",
      "\n",
      "input: 実はここは生命と無縁ではありません ここは私たちが発見した中で最も\n",
      "output: Because contrary to what you might think, this is not devoid of life.\n",
      "answer: Because contrary to what you might think, this is not devoid of life.\n",
      "\n",
      "input: 新世界ザルとも呼ばれるのは 約3500万年前に\n",
      "output: These guys are New World primates, about 35 million years ago.\n",
      "answer: These guys are New World primates, about 35 million years ago.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(train_dataset))\n",
    "    x, _, t = train_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print('input:', x)\n",
    "    print('output:', translate(model, x))\n",
    "    print('answer:', t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一致。\n",
    "\n",
    "訓練データに含まれていないものも試してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: それは現実世界へも拡がるだろうと思います\n",
      "output: Here's the poor: you know, the starts and say, \"I love you.\n",
      "answer: I even think it can go into the real world.\n",
      "\n",
      "input: だから 科学と技術によって 効率が高く 太陽エネルギーを活用し 知的生産に基づく経済へと スムーズに移行できて 2050年には 90億人が豊かに暮らし デジタル生活を送れるという 考えは妄想です\n",
      "output: So if the whole project will be remembered up, then thinking about it  ⁇  And then, then I feel like the first day that we got to understand from what was feeling of a living human up, so he couldn't make any moneying for life.\n",
      "answer: So the idea that we can smoothly transition to a highly-efficient, solar-powered, knowledge-based economy transformed by science and technology so that nine billion people can live in 2050 a life of abundance and digital downloads is a delusion.\n",
      "\n",
      "input: 私たちが気付いた2つ目の問題は 極めて不十分な 退役軍人への社会復帰支援です イラクやアフガンから 退役軍人が復員するに従い 新聞に大きく取り上げられています 彼らは市民生活に復帰することに 大変苦戦しているのです\n",
      "output: In 1947, there was no wind tunnel data And yet, on Tuesday, October 14th, 1947, Chuck Yeager climbed into the cockpit of his Bell  ⁇ -1 and he flew towards an unknown possibility, and in so doing, he became the first pilot to fly faster than the speed of sound. Six of eight Atlas rockets blew up on the pad.\n",
      "answer: The second problem that we became aware of was a very inadequate veteran reintegration, and this is a topic that is front page news right now as veterans are coming home from Iraq and Afghanistan, and they're struggling to reintegrate into civilian life.\n",
      "\n",
      "input: そして被験者にはくすぐったさなどの 様々な質問をしました\n",
      "output: And then I said, to know that it's not a very, very quickly distinct for the poor people who are going to make the future, and follow.\n",
      "answer: And then we're going to ask them to rate a bunch of things including ticklishness.\n",
      "\n",
      "input: そうして効率を倍にし 60%の内部収益率を得ることができるでしょう\n",
      "output: So we need to design for this system, and not so what we do in the future, right.\n",
      "answer: So you can double efficiency with a 60 percent internal rate of return.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "for _ in range(n):\n",
    "    i = random.randint(0, len(test_dataset))\n",
    "    x, _, t = test_dataset[i]\n",
    "    x = sp_ja.decode(x.tolist())\n",
    "    t = sp_en.decode(t.tolist())\n",
    "    print('input:', x)\n",
    "    print('output:', translate(model, x))\n",
    "    print('answer:', t)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 私は猫です。\n",
      "output: So I'm finish.\n",
      "\n",
      "input: 上手く文章が書けるようになりました。\n",
      "output: Well, you know, I think better as the best way around the life.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "sentences = [\n",
    "    '私は猫です。',\n",
    "    '上手く文章が書けるようになりました。'\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print('input:', sentence)\n",
    "    print('output:', translate(model, sentence))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "びみょう。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
