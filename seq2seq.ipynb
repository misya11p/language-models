{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext import transforms\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchvision.transforms import Compose\n",
    "import MeCab\n",
    "# from janome.tokenizer import Tokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('mps')\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data: 5304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>japanese</th>\n",
       "      <th>english</th>\n",
       "      <th>chinese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#0001</td>\n",
       "      <td>Xではないかとつくづく疑問に思う</td>\n",
       "      <td>I often wonder if it might be X.</td>\n",
       "      <td>难道不会是X吗，我实在是感到怀疑。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#0002</td>\n",
       "      <td>Xがいいなといつも思います</td>\n",
       "      <td>I always think X would be nice.</td>\n",
       "      <td>我总觉得X不错。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#0003</td>\n",
       "      <td>それがあるようにいつも思います</td>\n",
       "      <td>It always seems like it is there.</td>\n",
       "      <td>我总觉得那好像是有的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#0004</td>\n",
       "      <td>それが多すぎないかと正直思う</td>\n",
       "      <td>I honestly feel like there is too much.</td>\n",
       "      <td>老实说我觉得那太多了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#0005</td>\n",
       "      <td>山田はみんなに好かれるタイプの人だと思う</td>\n",
       "      <td>I think that Yamada is the type everybody likes.</td>\n",
       "      <td>我想山田是受大家欢迎的那种人。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id              japanese   \n",
       "0  #0001      Xではないかとつくづく疑問に思う  \\\n",
       "1  #0002         Xがいいなといつも思います   \n",
       "2  #0003       それがあるようにいつも思います   \n",
       "3  #0004        それが多すぎないかと正直思う   \n",
       "4  #0005  山田はみんなに好かれるタイプの人だと思う   \n",
       "\n",
       "                                            english            chinese  \n",
       "0                  I often wonder if it might be X.  难道不会是X吗，我实在是感到怀疑。  \n",
       "1                   I always think X would be nice.           我总觉得X不错。  \n",
       "2                 It always seems like it is there.        我总觉得那好像是有的。  \n",
       "3           I honestly feel like there is too much.        老实说我觉得那太多了。  \n",
       "4  I think that Yamada is the type everybody likes.    我想山田是受大家欢迎的那种人。  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('data/JEC_basic_sentence_v1-3.xls', header=None)\n",
    "df.columns = ['id', 'japanese', 'english', 'chinese']\n",
    "print('num of data:', len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger('-Owakati')\n",
    "# tokenizer = Tokenizer()\n",
    "def tokenize(data: List[str], l='en') -> List[List[str]]:\n",
    "    if l == 'ja':\n",
    "        return [tagger.parse(sentence).split() for sentence in data]\n",
    "        # return [[t for t in tokenizer.tokenize(sent, wakati=True)] for sent in data]\n",
    "    elif l == 'en':\n",
    "        return [sent.replace('.', ' .').lower().split() for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ja = tokenize(df['japanese'], l='ja')\n",
    "text_en = tokenize(df['english'], l='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad, bos, eos, unk = '<pad>', '<bos>', '<eos>', '<unk>'\n",
    "specials = [pad, bos, eos, unk]\n",
    "vocab_ja = build_vocab_from_iterator(text_ja, specials=specials)\n",
    "vocab_en = build_vocab_from_iterator(text_en, specials=specials)\n",
    "\n",
    "transform_ja = Compose([\n",
    "    transforms.AddToken(bos, begin=True),\n",
    "    transforms.AddToken(eos, begin=False),\n",
    "    transforms.VocabTransform(vocab_ja),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_en = Compose([\n",
    "    transforms.AddToken(bos, begin=True),\n",
    "    transforms.AddToken(eos, begin=False),\n",
    "    transforms.VocabTransform(vocab_en),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "n_vocab_ja = len(vocab_ja)\n",
    "n_vocab_en = len(vocab_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_enc: torch.Size([32, 17])\n",
      "x_dec: torch.Size([32, 18])\n",
      "label: torch.Size([32, 18])\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, in_text, out_text, in_transform, out_transform):\n",
    "        self.n_samples = len(in_text)\n",
    "        self.in_text = [in_transform(text) for text in in_text]\n",
    "        self.out_text = [out_transform(text) for text in out_text]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        in_text = self.in_text[index]\n",
    "        out_text = self.out_text[index]\n",
    "        return in_text, out_text[:-1], out_text[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "def to_padded_tensor(text_data: List[int], pad_value: int = 0) -> torch.Tensor:\n",
    "    data = pad_sequence(text_data, batch_first=True, padding_value=pad_value)\n",
    "    return data\n",
    "\n",
    "def collate_fn(batch):\n",
    "    enc_in_text, dec_in_text, dec_out_text = zip(*batch)\n",
    "    enc_in_text = to_padded_tensor(enc_in_text)\n",
    "    dec_in_text = to_padded_tensor(dec_in_text)\n",
    "    dec_out_text = to_padded_tensor(dec_out_text)\n",
    "    return enc_in_text, dec_in_text, dec_out_text\n",
    "\n",
    "dataset = TextDataset(text_ja, text_en, transform_ja, transform_en)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "x_enc, x_dec, label = next(iter(dataloader))\n",
    "print('x_enc:', x_enc.shape)\n",
    "print('x_dec:', x_dec.shape)\n",
    "print('label:', label.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            n_vocab,\n",
    "            embed_size,\n",
    "            padding_idx=vocab_ja.get_stoi()[pad]\n",
    "        )\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        # self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h = self.rnn(x)\n",
    "        h = self.fc(h) # (1, batch_size, hidden_size)\n",
    "        # _, h = self.lstm(x)\n",
    "        # h = self.fc(h[0][0])\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        # self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        x = self.embedding(x)\n",
    "        y, h = self.rnn(x, h)\n",
    "        # y, h = self.lstm(x, h)\n",
    "        y = self.fc(y)\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, n_in_vocab, n_out_vocab, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(n_in_vocab, embed_size, hidden_size)\n",
    "        self.decoder = Decoder(n_out_vocab, embed_size, hidden_size)\n",
    "\n",
    "    def forward(self, x_enc, x_dec):\n",
    "        h = self.encoder(x_enc)\n",
    "        y, _ = self.decoder(x_dec, h)\n",
    "        # h0 = h.unsqueeze(0)\n",
    "        # c0 = torch.zeros_like(h0)\n",
    "        # y, _ = self.decoder(x_dec, (h0, c0))\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_en[pad])\n",
    "def train(model, optimizer, n_epochs):\n",
    "    eval_tokens = []\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        for x_enc, x_dec, label in tqdm(dataloader, desc=f'{epoch}/{n_epochs}'):\n",
    "            optimizer.zero_grad()\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            label = label.to(device)\n",
    "            y_pred = model(x_enc, x_dec)\n",
    "            loss = criterion(y_pred.reshape(-1, y_pred.shape[-1]), label.ravel())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        eval_tokens.append((y_pred, label))\n",
    "        print(f'{epoch}/{n_epochs} loss: {epoch_loss/len(dataloader)}', flush=True)\n",
    "    return eval_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(len(vocab_ja), len(vocab_en), 1024, 1024).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval_tokensについては後程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/10: 100%|██████████| 166/166 [00:02<00:00, 72.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 loss: 5.425530226833849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2/10: 100%|██████████| 166/166 [00:02<00:00, 79.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/10 loss: 4.235839473195822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3/10: 100%|██████████| 166/166 [00:02<00:00, 79.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/10 loss: 3.466804277466004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4/10: 100%|██████████| 166/166 [00:02<00:00, 78.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/10 loss: 2.8385165915431747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5/10: 100%|██████████| 166/166 [00:02<00:00, 78.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/10 loss: 2.3676548779728903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6/10: 100%|██████████| 166/166 [00:02<00:00, 79.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/10 loss: 2.0015225862882224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7/10: 100%|██████████| 166/166 [00:02<00:00, 79.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/10 loss: 1.7395591577851628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8/10: 100%|██████████| 166/166 [00:02<00:00, 79.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/10 loss: 1.5688009305172657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9/10: 100%|██████████| 166/166 [00:02<00:00, 79.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/10 loss: 1.4572286483753158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10/10: 100%|██████████| 166/166 [00:02<00:00, 79.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 loss: 1.3897688001035207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_tokens = train(model, optimizer, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 翻訳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def translate(model, text, max_len=100):\n",
    "    model.eval()\n",
    "    tokens = tokenize([text], 'ja')[0]\n",
    "    tokens = transform_ja(tokens).unsqueeze(0).to(device)\n",
    "    h = model.encoder(tokens)\n",
    "    # h0 = h.unsqueeze(0)\n",
    "    # c0 = torch.zeros_like(h0)\n",
    "    # h = (h0, c0)\n",
    "    next_token = vocab_en[bos]\n",
    "\n",
    "    tokens = []\n",
    "    for _ in range(max_len):\n",
    "        next_token = torch.tensor(next_token).reshape(1, 1).to(device)\n",
    "        y, h = model.decoder(next_token, h)\n",
    "        y = F.softmax(y.ravel(), dim=0)\n",
    "        # next_token = random.choices(range(len(y)), weights=y)[0] # 確率的な出力\n",
    "        next_token = y.argmax().item() # 決定的な出力\n",
    "        if next_token == vocab_en[eos]:\n",
    "            break\n",
    "        tokens.append(next_token)\n",
    "    return ' '.join([vocab_en.get_itos()[t] for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the extras dvd contains an over two hour-long animation .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, 'Xではないかとつくづく疑問に思う')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the extras dvd contains an over two hour-long animation .'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, '山田はみんなに好かれるタイプの人だと思う')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lossはいい感じに減っているが、翻訳は上手くいっていない。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the extras dvd contains an over two hour-long animation .'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, 'Xではないかとつくづく疑問に思う')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the extras dvd contains an over two hour-long animation .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, '山田はみんなに好かれるタイプの人だと思う')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成: the it undergoes fusion, it goes through several processes . <eos> . . . . . .\n",
      "正解: before it undergoes fusion, it goes through several processes . <eos> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he have send greater dreams a reality . <eos> . . . . . . . .\n",
      "正解: we will make your thoughts a reality . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he city said education designates it as an important cultural property . <eos> . . . .\n",
      "正解: the minister of education designates it as an important cultural property . <eos> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he will me more human . <eos> . . . . . . . . . .\n",
      "正解: it makes us more human . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he city price includes the 5% consumption tax . <eos> . . . . . . .\n",
      "正解: the list price includes the 5% consumption tax . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he is a into the shopping cart . <eos> . . . . . . . .\n",
      "正解: he put items into the shopping cart . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he is out the cherry blossom viewing spots . <eos> . . . . . . .\n",
      "正解: he goes around to cherry blossom viewing spots . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he city save never touch the keys on the keyboard . <eos> . . . . .\n",
      "正解: the users should never touch the keys on the keyboard . <eos> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: the will to a fresh reminder that all of you . very concerned about that . <eos>\n",
      "正解: it came as a fresh reminder that all of you are very concerned about that . <eos>\n",
      "\n",
      "生成: he will easily fishing different flavors at once . <eos> . . . . . . .\n",
      "正解: you can enjoy two different flavors at once . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "n_data = 10\n",
    "y, t = eval_tokens[n_epochs - 1]\n",
    "y = y.argmax(dim=-1)[:n_data]\n",
    "t = t[:n_data]\n",
    "\n",
    "for yi, ti in zip(y, t):\n",
    "    print('生成:', ' '.join([vocab_en.get_itos()[i] for i in yi]))\n",
    "    print('正解:', ' '.join([vocab_en.get_itos()[i] for i in ti]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(len(vocab_ja), len(vocab_en), 1024, 1024).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, z = next(iter(dataloader))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1024])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = model.encoder(x.to(device))\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.9013e-01,  6.0347e-01,  2.1682e-01,  ...,  3.6929e-01,\n",
       "           2.6787e-04, -2.6888e-01],\n",
       "         [-5.3014e-01,  5.6925e-01,  1.4656e-01,  ...,  4.0848e-01,\n",
       "           2.7478e-02, -2.5881e-01],\n",
       "         [-4.3444e-01,  2.7794e-01,  2.2995e-01,  ...,  2.1237e-01,\n",
       "           6.0023e-03, -2.4894e-01],\n",
       "         ...,\n",
       "         [-1.2835e-01,  1.2345e-01, -3.6252e-01,  ...,  1.2249e-01,\n",
       "          -1.4214e-01, -5.1636e-03],\n",
       "         [-3.8142e-01,  5.4180e-01,  1.1163e-01,  ...,  5.1948e-01,\n",
       "           1.1526e-01, -1.8388e-01],\n",
       "         [-4.6349e-01,  4.7524e-01,  3.2895e-01,  ...,  3.0707e-01,\n",
       "          -7.8735e-02, -2.8814e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## メモ\n",
    "\n",
    "なぜ翻訳が上手くいかないか。\n",
    "\n",
    "### 考察\n",
    "\n",
    "RNN系は学習時と推論時で挙動が異なる。具体的には（decoderへの）入力が異なり、学習時は正解データを入力するが、推論時は推論結果（前の時間でモデルが出力した単語）を入力する。これが原因で、lossは低いが推論は上手くいかないのではないか。\n",
    "\n",
    "とりあえず、調べてみる。学習時にどの様な文章が生成されているのかを調べる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成: he minutes listened earnestly to the reporter . <eos> . . . . . . . . . . . . . . . . . . . .\n",
      "正解: fifty people listened earnestly to the reporter . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he companies from mainland china are listed on the hong kong stock market . <eos> . . . . . . . . . . . . . .\n",
      "正解: 231 companies from mainland china are listed on the hong kong stock market . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he will wonder thirsty . <eos> . . . . . . . to . . . . . . . to . . . . . . .\n",
      "正解: i often get thirsty . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he the 25th, he makes the service worse . <eos> . . . . . . . . . . . . . . . . . . .\n",
      "正解: on the contrary, it makes the service worse . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he will public electric characters for the file name . <eos> . . . . . . . . . . . . . . . . . .\n",
      "正解: he used japanese kana characters for the file name . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he government to the room opened slowly . <eos> . . . . . . . . . . . . . . . . . . . .\n",
      "正解: the door to the room opens slowly . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he springs make the flower of civilization bloom upon the earth . <eos> . . . . . . . . . . . . . . . .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解: saline springs make the flower of civilization bloom upon the earth . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he that point, something felt something i had never felt before . <eos> . . . . . . . . . . . . . . . .\n",
      "正解: at that moment, i felt something i had never felt before . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he explanation using data convenyance is explained below . <eos> . . . . . . . . . . . . . . . . . . .\n",
      "正解: an example using data convenyance is explained below . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "生成: he has me like this . <eos> . . . . . . . . . . . . . . . . . . . . . .\n",
      "正解: someone calls me like this . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "n_data = 10\n",
    "y, t = eval_tokens[n_epochs - 1]\n",
    "y = y.argmax(dim=-1)[:n_data]\n",
    "t = t[:n_data]\n",
    "\n",
    "for yi, ti in zip(y, t):\n",
    "    print('生成:', ' '.join([vocab_en.get_itos()[i] for i in yi]))\n",
    "    print('正解:', ' '.join([vocab_en.get_itos()[i] for i in ti]))\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20epoch時点での最後のバッチの生成結果。  \n",
    "これを見ると、毎回'he'から始まっていることが分かる。ただ、そこからいくつか単語が進むと正解と同じ文章が生成されるようになる。これはモデルが正解の単語を入力として受け取っているからである。推論時は間違った'he'という単語をそのまま受け取るので'he'に続く尤もらしい単語列を生成する。\n",
    "\n",
    "現状、どんな隠れ状態を受け取っても'he'が出力される。これは学習をやり直しても変わらない。なぜ必ず'he'が出力されるかというと、データセット内の初めの単語に'he'が多いから。多分。’\\<bos>'の次の単語は'he'である確率が最も高く、上記の結果は`argmax()`によって確定的に単語を出力しているため、全て'he'となる。\n",
    "\n",
    "対策として、隠れ状態によって初めの単語が変化するようにしなければならない。どうすればいいかな"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配消失"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoderが適切な隠れ状態を出力できていない場合、decoderがencoderへの入力を考慮しないモデルになってしまう気がする。ただの言語モデルと変わらなくなる。だから'\\<bos>'の次に'he'がくる。データセット内ではその確率が最も高いから。\n",
    "\n",
    "encoderがしっかり学習できているかを確かめるため、重みを見てみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(len(vocab_ja), len(vocab_en), 1024, 1024).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.5888e-01, -6.5523e-01,  5.1745e-01,  ...,  5.7562e-01,\n",
       "          1.9208e-01,  1.2221e+00],\n",
       "        [ 1.5327e+00,  7.8946e-02, -2.2005e+00,  ...,  6.6192e-01,\n",
       "         -3.9639e-01, -1.1072e+00],\n",
       "        [-2.0369e-02,  2.2732e-01,  1.1714e+00,  ..., -5.1474e-02,\n",
       "          7.2456e-01,  4.2381e-01],\n",
       "        ...,\n",
       "        [-2.1024e+00, -1.2815e+00,  1.6506e+00,  ...,  9.3447e-01,\n",
       "          1.3707e-01, -1.3592e+00],\n",
       "        [-2.3145e-01, -6.8438e-01, -1.6614e+00,  ...,  3.5744e-01,\n",
       "          2.6786e-01,  8.6400e-01],\n",
       "        [-2.6322e+00, -9.9322e-01,  1.3165e+00,  ...,  6.4995e-01,\n",
       "         -6.0848e-01, -9.7617e-04]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 loss: 5.260552417801087\n",
      "2/5 loss: 4.101400969976402\n",
      "3/5 loss: 3.1145439880440033\n",
      "4/5 loss: 2.226846011288195\n",
      "5/5 loss: 1.6628079163022789\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.6667e-01, -6.6668e-01,  5.2077e-01,  ...,  5.8563e-01,\n",
       "          2.0319e-01,  1.2334e+00],\n",
       "        [ 1.5328e+00,  7.8925e-02, -2.2005e+00,  ...,  6.6193e-01,\n",
       "         -3.9640e-01, -1.1073e+00],\n",
       "        [-1.8841e-02,  2.2387e-01,  1.1681e+00,  ..., -4.8255e-02,\n",
       "          7.2018e-01,  4.2445e-01],\n",
       "        ...,\n",
       "        [-2.1024e+00, -1.2815e+00,  1.6506e+00,  ...,  9.3447e-01,\n",
       "          1.3707e-01, -1.3592e+00],\n",
       "        [-2.3145e-01, -6.8438e-01, -1.6614e+00,  ...,  3.5744e-01,\n",
       "          2.6786e-01,  8.6400e-01],\n",
       "        [-2.6322e+00, -9.9322e-01,  1.3165e+00,  ...,  6.4995e-01,\n",
       "         -6.0848e-01, -9.7620e-04]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.embedding.weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5epoch学習させたが、ほとんど変わっていない。全く変わっていないわけではないので、勾配は行き届いている。ただ勾配が小さすぎるだけ。勾配消失というやつ。\n",
    "\n",
    "勾配を見てみると、こんな感じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4703e-10, -8.5906e-11,  1.6617e-10,  ..., -6.1598e-11,\n",
       "          4.8468e-11,  1.8586e-10],\n",
       "        [ 1.6159e-16,  1.3984e-16, -1.0948e-16,  ..., -1.3816e-16,\n",
       "         -1.2788e-16, -1.4849e-16],\n",
       "        [-1.6270e-17,  5.1073e-17, -6.4859e-17,  ..., -7.4436e-17,\n",
       "          5.5735e-17, -1.3135e-16],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.embedding.weight.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ほぼ0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 考察2\n",
    "\n",
    "勾配消失じゃなくね？\n",
    "\n",
    "LSTM使っても解決しないし、そもそもRNNの場合時間ごとに勾配が足されるのでこんな露骨に0になることもない気がする。勾配消失は終盤に勾配が届かなくなることだよね。ということは序盤は勾配が行き届くのでその分の勾配はあるはず。\n",
    "\n",
    "ということは、encoderが出力した隠れ状態の勾配がそもそも小さすぎる説が有力？。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(len(vocab_ja), len(vocab_en), 1024, 1024).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 30])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, z = next(iter(dataloader))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1024])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = model.encoder(x.to(device))\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0888, -0.0887,  0.3468,  ..., -0.3370,  0.1387, -0.3735],\n",
       "         [ 0.0888, -0.0887,  0.3468,  ..., -0.3370,  0.1387, -0.3735],\n",
       "         [ 0.0888, -0.0887,  0.3468,  ..., -0.3370,  0.1387, -0.3735],\n",
       "         ...,\n",
       "         [ 0.0888, -0.0887,  0.3468,  ..., -0.3370,  0.1387, -0.3735],\n",
       "         [ 0.0888, -0.0887,  0.3468,  ..., -0.3370,  0.1387, -0.3735],\n",
       "         [ 0.0888, -0.0887,  0.3468,  ..., -0.3370,  0.1387, -0.3735]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんだこれ、バッチに沿って全部同じやんけ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08879188, -0.08866556,  0.34679464, ..., -0.33701196,\n",
       "         0.13869706, -0.37350526],\n",
       "       [ 0.0887944 , -0.08866922,  0.34679207, ..., -0.33701053,\n",
       "         0.13869974, -0.37350455]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[0, :2].data.cpu().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "って思ったけど微妙に違うな。プログラムのミスではないということかな。  \n",
    "最後の方に<pad>が並んでいるせいで似た値になりやすい？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
